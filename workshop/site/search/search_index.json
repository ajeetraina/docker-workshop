{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Docker Workshop","text":""},{"location":"#github-sources","title":"GitHub Sources","text":"<p>The source code for this workshop is available here</p> <p>Click here to download the workshop slides.</p>"},{"location":"#authors","title":"Authors","text":"<ul> <li>Ajeet Singh Raina - DevRel @Docker</li> </ul>"},{"location":"#benefits-of-this-docker-workshop","title":"Benefits of this Docker Workshop","text":"<ul> <li>Learn about Inner-loop versus Outer-loop development workflow</li> <li>Learn how to spin multiple Postgres containers</li> <li>Learn how to develop a Product Catalog API from scratch</li> <li>Learn how to perform unit and integration tests</li> <li>Learn how to build your Docker image over the Cloud using Docker Build Cloud</li> <li>Secure your inner-loop with Docker Scout</li> </ul>"},{"location":"lab1/best-practices/","title":"Best practices","text":""},{"location":"lab1/best-practices/#1-use-explicit-base-image-reference-instead-of-the-latest","title":"1. Use explicit base image reference instead of the latest","text":"<p>Developers are often led to believe that specifying the latest tag will provide them with the most recent image in the repository but it has some side effects.</p> <p>Image tags are mutable, meaning a publisher can update a tag to point to a new image. For example, if you specify FROM node:latest in your Dockerfile, it might resolve to the latest patch version for 18.11. However, if you rebuild the image 3 months later, the same tag might point to a different version, such as 18.13. This could result in breaking changes, and it means you also don't have an audit trail of the exact image versions that you're using.. </p> <p></p>"},{"location":"lab1/best-practices/#2-prefer-leaner-docker-images","title":"2. Prefer leaner Docker Images","text":"<p>Using leaner Docker images can help reduce the size of the final image, which can lead to faster build times, smaller storage footprint, and quicker deployment times.</p> <p>For example, try to use Slimmer Images. Select smaller images for your FROM instructions in your Dockerfile. For example, the `node:16.17.0-slim image is a minimal Docker image that provides all of the OS utilities you would expect from a Linux container. There's also the special scratch image, which contains nothing at all and is useful for creating images of statically linked binaries (source).</p> <p></p>"},{"location":"lab1/best-practices/#3-use-multi-stage-builds","title":"3. Use Multi-stage builds","text":"<p>Multi-stage builds let you reduce the size of your final image, by creating a cleaner separation between the building of your image and the final output. Split your Dockerfile instructions into distinct stages to make sure that the resulting output only contains the files that's needed to run the application.</p> <p>Using multiple stages can also let you build more efficiently by executing build steps in parallel.</p> <p></p>"},{"location":"lab1/best-practices/#4-quickly-identify-and-fix-vulnerabilities-during-the-build-time-using-docker-scout","title":"4. Quickly identify and fix vulnerabilities during the Build time using Docker Scout","text":"<p>Container images consist of layers and software packages, which are susceptible to vulnerabilities. These vulnerabilities can compromise the security of containers and applications.</p> <p>Docker Scout is a solution for proactively enhancing your software supply chain security. By analyzing your images, Docker Scout compiles an inventory of components, also known as a Software Bill of Materials (SBOM). The SBOM is matched against a continuously updated vulnerability database to pinpoint security weaknesses.</p> <p>Docker Scout image analysis is available by default for Docker Hub repositories. You can also integrate third-party registries and other services</p> <p></p>"},{"location":"lab1/best-practices/#5-add-healthcheck-in-dockerfile-and-docker-compose","title":"5. Add Healthcheck in Dockerfile and Docker Compose","text":"<p>You can add a healthcheck in both Dockerfile and Docker Compose file. In a Dockerfile, you can use the HEALTHCHECK instruction. Here's an example:</p> <pre><code>HEALTHCHECK --interval=5m --timeout=3s \\\n  CMD curl -f http://localhost/ || exit 1\n</code></pre> <p>In this example, Docker will check every five minutes if a web-server is able to serve the site's main page within three seconds. If the command (curl -f http://localhost/ || exit 1) returns a non-zero code, the container is considered unhealthy (source).</p> <p>In a Docker Compose file, you can use the healthcheck attribute under a service. Here's an example:</p> <pre><code>services:\n  web:\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost\"]\n      interval: 1m30s\n      timeout: 10s\n      retries: 3\n</code></pre> <p>In this example, Docker Compose will run the command curl -f http://localhost to check the health of the web service. It will do this every 1 minute and 30 seconds, and if the command doesn't return within 10 seconds or if it fails 3 times in a row, the service is considered unhealthy (source).</p>"},{"location":"lab1/best-practices/#6-use-dockerignore","title":"6. Use .dockerignore","text":"<p>The .dockerignore file is used to exclude files and directories from the build context when building a Docker image. This can help to improve build speed and avoid sending unwanted files to the Docker builder. The syntax of the .dockerignore file is similar to the .gitignore file, with each line representing a pattern that matches files and directories to be excluded.</p> <p>Here's an example of a .dockerignore file:</p> <pre><code># .dockerignore\nnode_modules\nbar\n</code></pre> <p>In this example, the node_modules directory and any file or directory named bar are excluded from the build context.</p> <p>When you run a build command, Docker looks for a .dockerignore file in the root directory of the context. If this file exists, the files and directories that match patterns in the file are removed from the build context before it's sent to the builder. If you have multiple Dockerfiles, you can use different .dockerignore files for each Dockerfile by using a special naming convention. You should place your .dockerignore file in the same directory as the Dockerfile, and prefix the .dockerignore file with the name of the Dockerfile. For example, for a Dockerfile named myapp.Dockerfile, you would create a .dockerignore file named myapp.Dockerfile.dockerignore.</p> <pre><code>.\n\u251c\u2500\u2500 index.ts\n\u251c\u2500\u2500 src/\n\u251c\u2500\u2500 docker\n\u2502   \u251c\u2500\u2500 build.Dockerfile\n\u2502   \u251c\u2500\u2500 build.Dockerfile.dockerignore\n\u2502   \u251c\u2500\u2500 lint.Dockerfile\n\u2502   \u251c\u2500\u2500 lint.Dockerfile.dockerignore\n\u2502   \u251c\u2500\u2500 test.Dockerfile\n\u2502   \u2514\u2500\u2500 test.Dockerfile.dockerignore\n\u251c\u2500\u2500 package.json\n\u2514\u2500\u2500 package-lock.json\n</code></pre> <p>In this example, each Dockerfile has its own corresponding .dockerignore file. If both a Dockerfile-specific .dockerignore file and a .dockerignore file at the root of the build context exist, the Dockerfile-specific .dockerignore file takes precedence.</p>"},{"location":"lab1/best-practices/#7-run-as-non-root-user-for-security-purpose","title":"7. Run as non-root user for security purpose","text":"<p>Running Docker as a non-root user is a good practice to mitigate potential vulnerabilities in the Docker daemon and the container runtime. Docker provides a feature called \"Rootless mode\" that allows running the Docker daemon and containers as a non-root user.</p> <p></p>"},{"location":"lab1/best-practices/#8-favour-multi-architecture-docker-images","title":"8. Favour Multi-Architecture Docker Images","text":"<p>Using multi-architecture Docker images is beneficial as it allows your Docker images to run on different hardware architectures without any modifications. This means that whether you are using an ARM-based system or an x86 machine, Docker automatically detects and selects the appropriate variant for your host's operating system and architecture.</p> <p>There are three strategies to build multi-platform images depending on your use case: - Using emulation, via QEMU support in the Linux kernel. - Building on a single builder backed by multiple nodes of different architectures. - Using a stage in your Dockerfile to cross-compile to different architectures.</p> <p>To build multi-platform images, you can use the --platform flag with the docker build command to define the target platforms for the build output, such as linux/amd64 and linux/arm64.  For example:</p> <pre><code>$ docker build --platform linux/amd64,linux/arm64 .\n</code></pre> <p>By default, Docker can build for only one platform at a time. To build for multiple platforms concurrently, you can enable the containerd image store or create a custom builder. For example, to enable the containerd image store in Docker Desktop, go to Settings and select Use containerd for pulling and storing images in the General tab. If you're not using Docker Desktop, enable the containerd image store by adding the following feature configuration to your /etc/docker/daemon.json configuration file.</p> <pre><code>{\n  \"features\": {\n    \"containerd-snapshotter\": true\n  }\n}\n</code></pre> <p>Then, restart the daemon after updating the configuration file.</p> <pre><code>$ systemctl restart docker\n</code></pre> <p></p>"},{"location":"lab1/compose-watch/","title":"Compose watch","text":"<p>Compose File Watch is a feature introduced in Docker Compose version 2.22.0. It allows for automatic updates and previews of your running Compose services as you edit and save your code. This can enable a hands-off development workflow once Compose is running, as services automatically update themselves when you save your work.</p> <pre><code>services:\n  web:\n    build: .\n    command: npm start\n    develop:\n      watch:   \n        - actions: sync\n          path: ./web\n          target: /src/web\n          ignore: \n            - node_modules/\n        - action: rebuild\n          path: package.json\n</code></pre> <p>The <code>watch</code> attribute in the Compose file defines a list of rules that control these automatic service updates based on local file changes. Each rule requires a path pattern and an action to take when a modification is detected. The action can be set to rebuild, sync, or sync+restart.</p> <p>Here's a brief explanation of these actions:</p> <ul> <li>rebuild: Compose rebuilds the service image based on the build section and recreates the service with the updated image.</li> <li>sync: Compose keeps the existing service container(s) running, but synchronizes source files with container content according to the target attribute.</li> <li>sync+restart: Compose synchronizes source files with container content according to the target attribute, and then restarts the container.</li> </ul> <p>You can also define a list of patterns for paths to be ignored using the ignore attribute. Any updated file that matches a pattern, or belongs to a folder that matches a pattern, won't trigger services to be re-created. To use Compose Watch, you need to add the watch instructions to your compose.yaml file and then run your application with the docker compose watch command.</p> <p></p>"},{"location":"lab1/compose-watch/#getting-started","title":"Getting Started","text":""},{"location":"lab1/compose-watch/#clone-the-repository","title":"Clone the repository","text":"<pre><code>git clone https://github.com/dockersamples/getting-started-todo-app/\ncd getting-started-todo-app\n</code></pre>"},{"location":"lab1/compose-watch/#switch-to-compose-watch-branch","title":"Switch to compose-watch branch","text":"<pre><code>git checkout compose-watch\n</code></pre>"},{"location":"lab1/compose-watch/#bringing-up-the-app","title":"Bringing up the app","text":"<pre><code>docker compose watch\n</code></pre>"},{"location":"lab1/compose-watch/#make-some-changes","title":"Make some changes","text":"<p>Open a new terminal and modify the following package under frontend/package.json from </p> <pre><code>\"optionalDependencies\": {\n    \"fsevents\": \"^2.1.2\"\n</code></pre> <p>to</p> <pre><code>\"optionalDependencies\": {\n    \"fsevents\": \"^2.1.3\"\n</code></pre> <p>You will find that the frontend service gets rebuild automatcially for you</p> <pre><code>[+] Running 6/6\n \u2714 Network getting-started-todo-app_express-mongo     Created                                                          0.1s \n \u2714 Network getting-started-todo-app_react-express     Created                                                          0.0s \n \u2714 Container mongo                                    Started                                                          1.1s \n \u2714 Container getting-started-todo-app-mongoexpress-1  Started                                                          1.1s \n \u2714 Container backend                                  Started                                                          1.3s \n \u2714 Container frontend                                 Started                                                          1.8s \nWatch enabled\nRebuilding service \"frontend\" after changes were detected...\n[+] Building 52.5s (12/12) FINISHED                                                             docker-container:my-builder\n =&gt; [frontend internal] load build definition from Dockerfile                                                          0.0s\n =&gt; =&gt; transferring dockerfile: 532B                                                                                   0.0s\n =&gt; [frontend internal] load metadata for docker.io/library/node:lts-buster                                            1.1s\n =&gt; [frontend internal] load .dockerignore                                                                             0.0s\n =&gt; =&gt; transferring context: 67B                                                                                       0.0s\n =&gt; [frontend 1/6] FROM docker.io/library/node:lts-buster@sha256:479103df06b40b90f189461b6f824a62906683e26a32c77d7c3e  0.0s\n =&gt; =&gt; resolve docker.io/library/node:lts-buster@sha256:479103df06b40b90f189461b6f824a62906683e26a32c77d7c3e2d855a0e3  0.0s\n =&gt; [frontend internal] load build context                                                                             0.0s\n =&gt; =&gt; transferring context: 1.94kB                                                                                    0.0s\n =&gt; CACHED [frontend 2/6] WORKDIR /usr/src/app                                                                         0.0s\n =&gt; [frontend 3/6] COPY package.json /usr/src/app                                                                      0.0s\n =&gt; [frontend 4/6] COPY package-lock.json /usr/src/app                                                                 0.0s\n =&gt; [frontend 5/6] RUN npm ci                                                                                         24.4s\n =&gt; [frontend 6/6] COPY . /usr/src/app                                                                                 0.3s \n =&gt; [frontend] exporting to docker image format                                                                       26.5s \n =&gt; =&gt; exporting layers                                                                                                8.4s \n =&gt; =&gt; exporting manifest sha256:c3639997f048e5adea7487bafdafaf4ea3f946e071fd273aefb262b72f2c87a8                      0.0s \n =&gt; =&gt; exporting config sha256:1eb3a22fdb9d89316c9bdb1eb1d9f138e0f9545e95af4d812c9db8499309b491                        0.0s\n =&gt; =&gt; sending tarball                                                                                                18.1s\n =&gt; [frontend] importing to docker                                                                                    10.0s\n =&gt; =&gt; loading layer 7544a5e696a4 567B / 567B                                                                         10.0s\n =&gt; =&gt; loading layer 6e1b4449163b 32.77kB / 128.96kB                                                                   9.9s\n =&gt; =&gt; loading layer 874fffdf421f 115.87MB / 121.34MB                                                                  9.9s\n =&gt; =&gt; loading layer b8d4de9612f2 27.14kB / 27.14kB                                                                    0.0s\nservice \"frontend\" successfully built\n</code></pre>"},{"location":"lab1/docker-init/","title":"Docker init","text":"<p>Introduced for the first time in Docker Desktop 4.18, the new docker init CLI generates Docker assets for projects, making it easier to create Docker images and containers. When you run the docker init command in your project directory, it will guide you through the creation of the necessary files for your project with sensible defaults. These files include:</p> <pre><code>.dockerignore\nDockerfile\ndocker-compose.yaml\n</code></pre> <p>The docker init command also allows you to choose the application platform that your project uses and the relative directory of your main package. </p>"},{"location":"lab1/docker-init/#whos-this-for","title":"Who\u2019s this for?","text":"<p>This feature is targeted at developers who want to quickly create and manage Docker assets without having to manually configure everything. </p>"},{"location":"lab1/docker-init/#benefits-of-docker-init","title":"Benefits of Docker Init","text":"<p>The advantages of using the docker init command include:</p> <ul> <li>Simplified Docker asset creation: The command streamlines the creation of necessary Docker files, reducing the chances of errors and ensuring that best practices are followed.</li> <li>Saves time and effort: With the default settings and guided prompts, users can quickly create Docker assets without the need for extensive knowledge of Docker or its syntax.</li> <li>Better project organization: The generated files provide a standardized and organized structure for the project, making it easier for developers to maintain and update the project over time.</li> <li>Enhanced portability: By using Docker assets, projects become more portable across different environments, making it easier to move the project from development to production.</li> </ul> <p></p>"},{"location":"lab1/docker-init/#getting-started","title":"Getting Started","text":"<ul> <li>Install the latest version of Docker Desktop</li> <li>Install Node on your system</li> </ul> <p>Note: You must download and install the Node pre-built installer on your local system to get the <code>npm install</code> command to work seamlessly. Ensure that you select NPM package manager during the custom setup Click here to download</p>"},{"location":"lab1/docker-init/#clone-the-repository","title":"Clone the repository","text":"<pre><code>git clone https://github.com/dockersamples/docker-init-demos\ncd docker-init-demos/node\n</code></pre> <p>The usual way to bring up this node application is to follow the steps:</p> <pre><code>npm install\nnode app.js\n</code></pre> <p>You can verify the output by accessing the URL:</p> <pre><code>curl localhost:8080\n</code></pre> <p>Now let's see how to containerise this project using the <code>docker init</code> CLI.</p>"},{"location":"lab1/docker-init/#run-the-following-command","title":"Run the following command:","text":"<pre><code> docker init\n</code></pre> <p>This utility will walk you through creating the following files with sensible defaults for your project:</p> <ul> <li>Select Node as your application platform</li> <li>Type \"22.2.0\" as Node version, if it doesn't provide you any default option</li> <li>Select npm as package manager</li> <li>Select \"node app.js\" as the command</li> <li>Type \"8080\" as a port that server listens on</li> </ul> <p>The tool creates the following Docker assets for you:</p> <pre><code>  - .dockerignore\n  - Dockerfile\n  - docker-compose.yaml\n</code></pre>"},{"location":"lab1/docker-init/#running-the-container-service","title":"Running the container service","text":"<p><code>docker compose up -d --build</code></p>"},{"location":"lab1/docker-init/#accessing-the-node-app","title":"Accessing the Node app","text":"<pre><code> curl localhost:8080      .\n/\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\\___/ ===\n{                       /  ===-\n\\______ O           __/\n \\    \\         __/\n  \\____\\_______/\n\n\nHello from Docker\n</code></pre>"},{"location":"lab1/docker-workflow/","title":"Docker workflow","text":"<ul> <li> <p>Build: Typically a developer run <code>docker build</code> command to build an image from a Dockerfile, which is a text document that contains instructions for how to build the image.</p> </li> <li> <p>Share: Once a developer builds a Docker image, they can share it with others by pushing it to a container registry. You can use the <code>docker push</code> command to do this.</p> </li> <li> <p>Run: The <code>docker run</code> command lets you create and start a running container based on a Docker image.</p> </li> </ul> <p></p>"},{"location":"lab1/docker-workflow/#docker-inner-loop-development-workflow","title":"Docker Inner-Loop Development workflow","text":"<ul> <li>Development Environment: As you deep-dive into the inner-loop development workflow, developers have their own choice anduse any operating system (OS) they prefer, like Windows, Mac, or Linux. They can also leverage their favorite Integrated Development Environments (IDEs) for coding. This flexibility empowers developers to work in a familiar and comfortable setting.</li> </ul> <ul> <li> <p>Building and storing the application - Once the code is written, the developer uses GitHub Actions. GitHub Actions is a built-in automation tool in GitHub that allows you to automate tasks within your development workflow. In this context, the developer uses GitHub Actions to trigger the building of a Docker image. A Dockerfile, which is a set of instructions that specifies how to build the image, is used in this process.  After the image is built, it's uploaded to a container registry. A container registry acts as a library or repository that stores Docker images.</p> </li> <li> <p>Deployment - Finally, the image is deployed to the cloud. This means the image is uploaded to a cloud platform where it can be run on virtual machines. There are several cloud platforms available, including Google Cloud Platform, Amazon Web Services, and Microsoft Azure. The specific steps for deployment will vary depending on the chosen cloud platform. But generally, it involves using the cloud platform tools to run the Docker image and create a container instance. This container instance then executes the application.</p> </li> </ul>"},{"location":"lab1/overview/","title":"Inner Vs Outer Loop Development workflow","text":"<p>The inner loop is the iterative process of writing, building, and debugging code that a single developer performs before sharing the code, either publicly or with their team. It is typically characterized by frequent changes to the code as the developer learns more about the problem they are trying to solve.</p> <p>The outer loop is everything else that happens leading up to release. This includes code merge, automated code review, test execution, deployment, controlled (canary) release, and observation of results. It is typically characterized by less frequent changes to the code, as the focus is on ensuring that the code is stable and ready for production..</p> <p>The inner loop is often associated with the development phase of the SDLC, while the outer loop is associated with the testing, deployment, and release phases. However, the two loops are not mutually exclusive, and they can overlap in some cases.</p> <p></p>"},{"location":"lab1/postgres/","title":"Running Postgres Containers","text":""},{"location":"lab1/postgres/#running-multiple-postgres-containers","title":"Running Multiple Postgres Containers","text":"<pre><code>docker run -d --name postgres1 -e POSTGRES_PASSWORD=dev -p 5432:5432 postgres:latest\n</code></pre> <pre><code>docker run -d --name postgres2 -e POSTGRES_PASSWORD=dev -p 5433:5432 postgres:13\n</code></pre> <pre><code>docker run -d --name postgres3 -e POSTGRES_PASSWORD=dev -p 5434:5432 postgres:12\n</code></pre> <p>Open Docker Dashboard and run the following commands:</p> <pre><code># psql -d postgres -U postgres -W\nPassword: \npsql (17.2 (Debian 17.2-1.pgdg120+1))\nType \"help\" for help.\n\npostgres=# \n</code></pre> <p>The above command includes three flags:</p> <ul> <li>-d - specifies the name of the database to connect to</li> <li>-U - specifies the name of the user to connect as</li> <li>-W - forces psql to ask for the user password before connecting to the database</li> </ul>"},{"location":"lab1/postgres/#listing-all-the-databases-l","title":"Listing all the databases - \\l","text":"<pre><code>\n                                                    List of databases\n   Name    |  Owner   | Encoding | Locale Provider |  Collate   |   Ctype    | Locale | ICU Rules |   Access privileges   \n-----------+----------+----------+-----------------+------------+------------+--------+-----------+-----------------------\n postgres  | postgres | UTF8     | libc            | en_US.utf8 | en_US.utf8 |        |           | \n template0 | postgres | UTF8     | libc            | en_US.utf8 | en_US.utf8 |        |           | =c/po\nstgres          +\n           |          |          |                 |            |            |        |           | postg\nres=CTc/postgres\n template1 | postgres | UTF8     | libc            | en_US.utf8 | en_US.utf8 |        |           | =c/po\nstgres          +\n           |          |          |                 |            |            |        |           | postg\nres=CTc/postgres\n(3 rows)\n</code></pre>"},{"location":"lab1/postgres/#list-all-schemas","title":"List all schemas","text":"<p>The <code>\\dn</code> psql command lists all the database schemas.</p> <pre><code>postgres=# \\dn\n      List of schemas\n  Name  |       Owner       \n--------+-------------------\n public | pg_database_owner\n(1 row)\n\npostgres=#\n</code></pre>"},{"location":"lab1/postgres/#run-the-following-command-to-show-the-database-activity","title":"Run the following command to show the database activity:","text":"<pre><code>SELECT * from pg_stat_activity;  &lt;--- DONT FORGET \";\"\n</code></pre>"},{"location":"lab1/postgres/#result","title":"Result:","text":"<pre><code>datid | datname  | pid | leader_pid | usesysid | usename  | application_name | client_addr | client_host\nname | client_port |         backend_start         |          xact_start           |          query_start\n          |         state_change          | wait_event_type |     wait_event      | state  | backend_xid \n| backend_xmin | query_id |              query              |         backend_type         \n-------+----------+-----+------------+----------+----------+------------------+-------------+------------\n-----+-------------+-------------------------------+-------------------------------+---------------------\n----------+-------------------------------+-----------------+---------------------+--------+-------------\n+--------------+----------+---------------------------------+------------------------------\n     5 | postgres |  85 |            |       10 | postgres | psql             |             |            \n     |          -1 | 2025-01-08 11:40:22.778949+00 |                               | 2025-01-08 11:40:56.\n590114+00 | 2025-01-08 11:41:26.598462+00 | Client          | ClientRead          | idle   |             \n|              |          | SELECT pg_sleep(30);            | client backend\n     5 | postgres |  92 |            |       10 | postgres | psql             |             |            \n     |          -1 | 2025-01-08 11:41:14.359414+00 | 2025-01-08 11:43:52.615603+00 | 2025-01-08 11:43:52.\n615603+00 | 2025-01-08 11:43:52.61561+00  |                 |                     | active |             \n|          750 |          | SELECT * FROM pg_stat_activity; | client backend\n       |          |  64 |            |          |          |                  |             |            \n     |             | 2025-01-08 11:38:28.74611+00  |                               |                     \n          |                               | Activity        | AutovacuumMain      |        |             \n|              |          |                                 | autovacuum launcher\n       |          |  65 |            |       10 | postgres |                  |             |            \n:\n</code></pre>"},{"location":"lab1/postgres/#method-2-using-ask-gordon","title":"Method 2: Using \"Ask Gordon\"","text":"<pre><code>Run a multiple version of postgres with the standard port and POSTGRES_PASSWORD set to dev\n</code></pre>"},{"location":"lab1/postgres/#remove-the-container","title":"Remove the container","text":"<p>Open Docker Dashboard &gt; Selecting all the running Postgres containsrs and delete them in a single step.</p>"},{"location":"lab1/what-is-a-container/","title":"What is a container","text":"<p>Let\u2019s compare containers to smartphone apps. </p> <p>what is a container</p> <p>Most of us have smartphones that have lots of apps installed on them.</p> <p>When is the last time you thought about how to install the dependencies for one of those apps, how to configure it, and how to set it up? Well, probably never.</p> <p>We typically just open the app store, click the Install button , and then the app is there. <p>And when we open the newly installed red app, we don\u2019t have to worry about how the dependencies and libraries for the green app are going to affect it - they all run in isolated or sandboxed environments.</p> <p>Containers bring this same idea to other types of applications and services, although they are implemented a little differently.</p>"},{"location":"lab2/aws-s3-setup/","title":"Configuring AWS with IAM and S3","text":""},{"location":"lab2/aws-s3-setup/#1-login-to-aws-and-create-a-user","title":"1. Login to AWS and create a user","text":""},{"location":"lab2/aws-s3-setup/#2-add-a-user-called-developer","title":"2. Add a user called developer","text":""},{"location":"lab2/aws-s3-setup/#3-creat-a-group","title":"3. Creat a group","text":""},{"location":"lab2/aws-s3-setup/#4-add-developer-to-admin1-group","title":"4. Add developer to admin1 group","text":""},{"location":"lab2/aws-s3-setup/#5-review-and-save","title":"5. Review and save","text":""},{"location":"lab2/aws-s3-setup/#6-enable-access-keys-and-security-key-for-this-user","title":"6. Enable Access Keys and Security key for this user","text":""},{"location":"lab2/aws-s3-setup/#7-create-a-s3-bucket","title":"7. Create a S3 bucket","text":""},{"location":"lab2/aws-s3-setup/#8-grant-this-user-with-s3-bucket-access","title":"8. Grant this user with S3 bucket access","text":""},{"location":"lab2/getting-started/","title":"Getting started","text":""},{"location":"lab2/getting-started/#a-basic-todo-list-app","title":"A Basic Todo List App","text":"<p>This is a starting point for todo-list app powered with React, Node and Mongo.</p>"},{"location":"lab2/getting-started/#tech-stack","title":"Tech Stack","text":"<ul> <li>Frontend: React</li> <li>Backend: Node.js</li> <li>Database: Mongo DB</li> <li>Database Admin Interface: MongoExpress</li> </ul>"},{"location":"lab2/getting-started/#clone-the-repository","title":"Clone the repository","text":"<pre><code>git clone https://github.com/dockersamples/getting-started-todo-app\ncd getting-started-todo-app\n</code></pre>"},{"location":"lab2/getting-started/#switch-to-basic-branch","title":"Switch to <code>basic</code> branch","text":"<pre><code>git checkout basic\n</code></pre>"},{"location":"lab2/getting-started/#bringing-up-the-service-containers","title":"Bringing up the service containers","text":"<pre><code>docker compose up -d\n</code></pre> <p>After the application starts, navigate to <code>http://localhost:3000</code> in your web browser.</p>"},{"location":"lab2/getting-started/#logging-into-mongo-express","title":"Logging into Mongo Express","text":"<p>Enter admin/pass as credential to login into Mongo Express and view the database and collections/items.</p>"},{"location":"lab2/overview/","title":"Overview","text":"<p>Container-first development takes the concept of containerization a step further. It involves using containers for every aspect of software development, including the application runtime itself. This means developers can ditch traditional local installations and leverage containers for everything needed to run the application.</p>"},{"location":"lab2/overview/#what-it-means","title":"What it means?","text":"<p>In container-first development, developers work within a containerized environment. They:</p> <ul> <li>Clone the project repository.</li> <li>Run a command (often docker-compose up or docker-compose watch) to start the development environment. This command usually pulls pre-built container images containing the application runtime (e.g., Node.js, Python interpreter) and any dependencies.</li> </ul> <p>That's it! The development environment is up and running entirely within containers. No need to install the application runtime or specific libraries on the developer's machine.</p>"},{"location":"lab2/overview/#benefits-for-developers","title":"Benefits for Developers:","text":"<ul> <li>Extreme Portability: Developers only need a container engine and an IDE to work on the project. This ensures identical development environments regardless of the underlying operating system or pre-installed software.</li> <li>Faster Setup: No time wasted installing the application runtime or fiddling with local configurations. Developers can start coding as soon as the containerized environment is up.</li> <li>Improved Isolation: Each project runs within its own isolated container, preventing conflicts between projects or dependencies from interfering with other applications on the developer's machine.</li> <li>Simplified Collaboration: Team members can easily share and reproduce development environments using the same container images.</li> </ul>"},{"location":"lab2/overview/#choosing-container-first","title":"Choosing Container-First:","text":"<ul> <li>Container-first development is ideal for teams seeking:</li> <li>Maximum portability across development environments.</li> <li>Fast and streamlined development setup.</li> <li>Strong isolation between projects to avoid conflicts.</li> </ul> <p>However, it requires a steeper learning curve for containerization technologies and might have higher resource demands.</p> <p>Container-first development offers a powerful approach for building applications entirely within containerized environments. It streamlines development workflows, ensures consistent development environments, and promotes collaboration. But, it's important to weigh the benefits against the increased complexity and potential resource usage before adopting this approach for your development team.</p>"},{"location":"lab2/services/","title":"Services","text":""},{"location":"lab2/services/#1-clone-the-repository","title":"1. Clone the repository:","text":"<pre><code>git clone https://github.com/dockersamples/getting-started-todo-app\ncd getting-started-todo-app\n</code></pre>"},{"location":"lab2/services/#2-switch-to-container-first-branch","title":"2. Switch to container-first branch","text":"<pre><code>git checkout container-first-aws-mongo\n</code></pre>"},{"location":"lab2/services/#3-add-the-environment-variables","title":"3. Add the Environment Variables","text":"<p>Copy .env.sample to .env file and Ensure that you have the right environmental variable added as shown:</p> <pre><code>MONGODB_URI=mongodb://mongodb:27017/todo-app\nJWT_SECRET=603b31XXXXXXX90d3b8cb62f0a585fd70a5ee0b4d\nAWS_ACCESS_KEY_ID=AKIAXXXXXDDDX\nAWS_SECRET_ACCESS_KEY=hSYXtvXXXXXXXO/k39FGt3u078pYWsh\nAWS_REGION=us-east-1\nS3_BUCKET_NAME=localbuckett\n</code></pre> <p>You can leverage this link to generate JWT token.</p>"},{"location":"lab2/services/#4-bring-up-the-services","title":"4. Bring up the services:","text":"<pre><code>docker compose watch\n</code></pre>"},{"location":"lab2/services/#5-access-the-app","title":"5. Access the app","text":"<p>Open http://localhost:3000 to access the todo-list app. Try adding a task and uploading the image.</p>"},{"location":"lab2/services/#verify-mongo","title":"Verify Mongo","text":"<p>You can verify if task gets added by selecting the container and clicking on \"Exec\" option on the Docker dashboard. Now you should be able to run the following command to verify the tasks.</p> <pre><code># mongosh\nCurrent Mongosh Log ID: 66879e864955d6e7b2f3f54d\nConnecting to:          mongodb://127.0.0.1:27017/?directConnection=true&amp;serverSelectionTimeoutMS=2000&amp;appName=mongosh+2.2.10\nUsing MongoDB:          7.0.12\nUsing Mongosh:          2.2.10\n\nFor mongosh info see: https://docs.mongodb.com/mongodb-shell/\n\n\nTo help improve our products, anonymous usage data is collected and sent to MongoDB periodically (https://www.mongodb.com/legal/privacy-policy).\nYou can opt-out by running the disableTelemetry() command.\n\n------\n   The server generated these startup warnings when booting\n   2024-07-05T07:18:03.008+00:00: Using the XFS filesystem is strongly recommended with the WiredTiger storage engine. See http://dochub.mongodb.org/core/prodnotes-filesystem\n   2024-07-05T07:18:03.737+00:00: Access control is not enabled for the database. Read and write access to data and configuration is unrestricted\n   2024-07-05T07:18:03.738+00:00: /sys/kernel/mm/transparent_hugepage/enabled is 'always'. We suggest setting it to 'never' in this binary version\n   2024-07-05T07:18:03.738+00:00: vm.max_map_count is too low\n------\n\ntest&gt; show dbs\nadmin     40.00 KiB\nconfig    12.00 KiB\nlocal     40.00 KiB\ntodo-app  68.00 KiB\ntest&gt; use todo-app\nswitched to db todo-app\ntodo-app&gt; show collections\ntodos\nusers\ntodo-app&gt; db.todos.showDocuments()\nTypeError: db.todos.showDocuments is not a function\ntodo-app&gt; db.todos.countDocuments()\n1\ntodo-app&gt; db.todos.countDocuments()\n2\ntodo-app&gt; db.todos.countDocuments()\n3\ntodo-app&gt;\n</code></pre>"},{"location":"lab2/services/#verify-the-images-added-to-aws-s3","title":"Verify the images added to AWS S3","text":"<p>Open AWS Dashboard &gt; S3 service to see the list of images uploaded.</p> <p></p>"},{"location":"lab2/tech-stack/","title":"Tech stack","text":"<ul> <li>Frontend: React, Material UI.</li> <li>Backend: Node.js, Express</li> <li>Database: Mongo(Atlas or running locally for storing tasks)</li> <li>Object Storage: AWS S3(for storing images)</li> </ul>"},{"location":"lab3/overview/","title":"Overview","text":"<p>Container-supported development is the idea of using containers to support and enhance development without touching the main application runtime itself. </p>"},{"location":"lab3/overview/#what-it-means","title":"What it means?","text":"<p>The developer will run their application using a runtime installed natively on their machine (such as a JVM, Node engine, or Python interpreter). But, the external dependencies will run in containers. </p>"},{"location":"lab3/overview/#what-benefits-does-it-provide-to-the-developers","title":"What benefits does it provide to the developers?","text":"<p>Even though the developers didn\u2019t go \u201call-in\u201d, Docker still provided significant value by running dependent services out of containers, making it quick and easy to get started and ensure version consistency across the entire team.</p> <p>With Docker, teams can do things that might otherwise have been impossible. They can run local instances of cloud services, run real services in their tests, and more. There is no \u201cone right path\u201d for teams to leverage Docker. You see teams using wrapper scripts that run docker run commands, others using IDE plugins to launch declarative Compose stacks, or programmatic interactions using Testcontainers.In many of these cases, teams can leverage off-the-shelf (or very slightly customized versions of) images from our DOI/DVP catalog.</p>"},{"location":"lab3/overview/#choosing-the-container-supported-approach","title":"Choosing the container-supported approach:","text":"<ul> <li> <p>Separation of Concerns: Developers focus on the core application logic using their familiar runtime (JVM, Node, Python etc.), while external dependencies are isolated in containers.</p> </li> <li> <p>Improved Efficiency:</p> </li> <li> <p>Easier setup: Containers simplify dependency management, reducing time spent configuring environments. Version consistency: All developers use the same container image, ensuring consistent dependencies across the team.</p> </li> <li> <p>Enhanced Capabilities: Docker allows running local simulations of cloud services and real services within tests, providing a more realistic development environment.</p> </li> <li> <p>Flexibility in Implementation: There's no single approach. Teams can use:</p> </li> <li> <p>Wrapper scripts for simple container execution.</p> </li> <li>IDE plugins for launching development environments defined in Docker Compose files.</li> <li> <p>Programming interactions with libraries like Testcontainers.</p> </li> <li> <p>Leveraging Shared Resources: Teams can benefit from pre-built container images from public repositories like Docker Official Images (DOI) and Docker Verified Publishers (DVP).</p> </li> </ul> <p>Overall, container-supported development offers a way to streamline the development process by managing dependencies and enhancing development environments without completely switching to a containerized application runtime.</p>"},{"location":"lab3/services/","title":"Bringing up the services","text":""},{"location":"lab3/services/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker Desktop</li> <li>JWT Secret</li> </ul> <p>Note: If you're a Windows user, you need to download the pre-built binary using this link </p>"},{"location":"lab3/services/#getting-started","title":"Getting Started","text":""},{"location":"lab3/services/#1-clone-the-repository","title":"1. Clone the repository:","text":"<pre><code>git clone https://github.com/dockersamples/getting-started-todo-app\ncd getting-started-todo-app\n</code></pre>"},{"location":"lab3/services/#2-switch-to-the-right-branch","title":"2. Switch to the right branch","text":"<p>Switch to container-supported branch before you run the following command:</p> <pre><code>git checkout container-supported\n</code></pre>"},{"location":"lab3/services/#3-bring-up-the-services","title":"3. Bring up the services","text":"<p>Use Docker Compose to set up MongoDB and LocalStack:</p> <pre><code>docker compose -f compose-native.yml up -d --build\n</code></pre> <p> </p>"},{"location":"lab3/services/#4-create-s3-bucket-manually","title":"4. Create S3 bucket manually","text":"<p>Select the LocalStack container, select EXEC, and run the following command to create the S3 bucket:</p> <pre><code>awslocal s3 mb s3://sample-bucket\nmake_bucket: sample-bucket\n</code></pre>"},{"location":"lab3/services/#5-check-the-localstack-logs","title":"5. Check the LocalStack logs","text":"<pre><code>2024-07-03 19:27:32 LocalStack version: 3.5.1.dev\n2024-07-03 19:27:32 LocalStack build date: 2024-06-24\n2024-07-03 19:27:32 LocalStack build git hash: 9a3d238ac\n2024-07-03 19:27:32 Ready.\n2024-07-03 19:28:13 2024-07-03T13:58:13.804  INFO --- [et.reactor-0] localstack.request.aws     \n2024-07-03 19:28:32 AWS s3.CreateBucket =&gt; 200: AWS s3.CreateBucket =&gt; 200\n</code></pre>"},{"location":"lab3/services/#6-access-the-app-and-try-uploading-the-image","title":"6. Access the app and try uploading the image","text":"<p>Open http://localhost:3000 and try to create a new task as well as upload the image.</p>"},{"location":"lab3/services/#7-listing-the-items-in-the-s3-bucket","title":"7. Listing the items in the S3 bucket","text":"<pre><code>awslocal s3api list-objects --bucket sample-bucket\n{\n    \"Contents\": [\n        {\n            \"Key\": \"1720015203095-Screenshot 2024-07-03 at 9.24.34 AM.png\",\n            \"LastModified\": \"2024-07-03T14:00:03.000Z\",\n            \"ETag\": \"\\\"cd4396baa401efb22797472599faff87\\\"\",\n            \"Size\": 735617,\n            \"StorageClass\": \"STANDARD\",\n            \"Owner\": {\n                \"DisplayName\": \"webfile\",\n                \"ID\": \"75aa57f09aa0c8caeab4f8c24e99d10f8e7faeebf76c078efc7c6caea54ba06a\"\n            }\n        }\n    ],\n    \"RequestCharged\": null\n}\n</code></pre>"},{"location":"lab3/services/#8-verify-if-item-gets-into-mongo-database","title":"8. Verify if item gets into Mongo database","text":"<pre><code>Connecting to:          mongodb://127.0.0.1:27017/?directConnection=true&amp;serverSelectionTimeoutMS=2000&amp;appName=mongosh+2.2.10\nUsing MongoDB:          7.0.12\nUsing Mongosh:          2.2.10\n\nFor mongosh info see: https://docs.mongodb.com/mongodb-shell/\n\n------\n   The server generated these startup warnings when booting\n   2024-07-03T13:57:31.418+00:00: Using the XFS filesystem is strongly recommended with the WiredTiger storage engine. See http://dochub.mongodb.org/core/prodnotes-filesystem\n   2024-07-03T13:57:32.732+00:00: Access control is not enabled for the database. Read and write access to data and configuration is unrestricted\n   2024-07-03T13:57:32.733+00:00: /sys/kernel/mm/transparent_hugepage/enabled is 'always'. We suggest setting it to 'never' in this binary version\n   2024-07-03T13:57:32.733+00:00: vm.max_map_count is too low\n------\n\ntest&gt; show dbs\nadmin      40.00 KiB\nconfig     72.00 KiB\nlocal      80.00 KiB\ntodo-app  180.00 KiB\ntest&gt; use todo-app\nswitched to db todo-app\ntodo-app&gt; db.todos.countDocuments()\n5\ntodo-app&gt; db.todos.countDocuments()\n6\ntodo-app&gt;\n</code></pre>"},{"location":"lab3/tech-stack/","title":"Tech stack","text":"<ul> <li>Frontend: React, Material UI.</li> <li>Backend: Node.js, Express</li> <li>Database: Mongo(running locally for storing tasks)</li> <li>Object Storage: Localstack (for emulating S3 and storing images locally for testing purpose)</li> </ul>"},{"location":"prereq/prereq/","title":"Prerequisites","text":""},{"location":"prereq/prereq/#1-docker-desktop","title":"1. Docker Desktop","text":"<p>Download and Install Docker Desktop on your system. Make sure you are using Docker Desktop v4.27.2 and above.</p> <ul> <li>Apple Chip</li> <li>Intel Chip</li> <li>Windows</li> <li>Linux</li> </ul>"},{"location":"prereq/prereq/#enabling-wsl-2-based-engine-on-docker-desktop-for-windows","title":"Enabling WSL 2 based engine on Docker Desktop for Windows","text":"<p>In case you're using Windows 11, you will need to enable WSL 2 by opening Docker Desktop &gt; Settings &gt; Resources &gt; WSL Integration</p> <p></p>"},{"location":"prereq/prereq/#2-install-nodejs","title":"2. Install Nodejs","text":"<p>To demonstrate the Product Catalog sample app, you will require Node 22+ version installed on your system.</p> <p>Note: You must download and install the Node pre-built installer on your local system to get the npm install command to work seamlessly. Click here to download</p>"},{"location":"prereq/prereq/#3-access-to-the-repositories","title":"3. Access to the repositories","text":"<ul> <li>https://github.com/dockersamples/docker-init-demos</li> <li>https://github.com/dockersamples/avatars</li> <li>https://github.com/dockersamples/catalog-service-node </li> </ul>"},{"location":"prereq/prereq/#4-access-to-the-list-of-packages","title":"4. Access to the list of Packages","text":"<p>If you're behind the firewall, these are the list of packages required for this workshop:</p>"},{"location":"prereq/prereq/#docker-init-demo","title":"Docker Init Demo","text":"<pre><code>alpine-baselayout-3.6.5-r0\nalpine-baselayout-data-3.6.5-r0\nalpine-keys-2.4-r1\napk-tools-2.14.4-r0\nbusybox-1.36.1-r29\nbusybox-binsh-1.36.1-r29\nca-certificates-bundle-20240705-r0\nlibcrypto3-3.3.2-r0\nlibgcc-13.2.1_git20240309-r0\nlibssl3-3.3.2-r0\nlibstdc++-13.2.1_git20240309-r0\nmusl-1.2.5-r0\nmusl-utils-1.2.5-r0\nscanelf-1.3.7-r2\nssl_client-1.36.1-r29\nzlib-1.3.1-r1\n</code></pre>"},{"location":"prereq/prereq/#install-testcontainers-desktop-app","title":"Install Testcontainers Desktop App","text":"<p>Click here to download Testcontainers Desktop app and install it on your machine.</p>"},{"location":"product-catalog/build/","title":"Build","text":""},{"location":"product-catalog/build/#using-docker-build-cloud","title":"Using Docker Build Cloud","text":"<p>Method: 1 : Running it locally</p> <pre><code>docker buildx create --driver cloud dockerdevrel/demo-builder\ndocker buildx build --builder cloud-dockerdevrel-demo-builder .\n</code></pre> <p></p> <p>Method:2 =- Running it using GitHub Workflow</p> <p>Open dockersamples/ repo and se the workflow.</p>"},{"location":"product-catalog/develop/","title":"Develop","text":""},{"location":"product-catalog/develop/#clone-the-repo","title":"Clone the repo","text":"<pre><code>git clone https://github.com/dockersamples/catalog-service-node\n</code></pre>"},{"location":"product-catalog/develop/#initial-setup","title":"Initial Setup","text":"<pre><code>cd demo/sdlc-e2e\n./setup.sh\n</code></pre> <p>The setup.sh script performs several important setup tasks to prepare the development environment for the Docker workshop. Let me explain what it does in detail: The script performs the following tasks:</p> <ol> <li>Creates a demo branch:</li> </ol> <p>It determines the repository root using git rev-parse --show-toplevel It creates a new git branch with a unique name combining \"demo\", the current date, and your username (e.g., demo-20250304-ajeet) This ensures each participant has their own isolated branch to work with</p> <ol> <li>Cleans the environment:</li> </ol> <p>Runs git clean -f to remove untracked files Deletes any existing branches named 'temp' or with the same demo branch name Creates a temporary branch, deletes the main branch locally, then recreates it This ensures everyone starts with a clean state</p> <ol> <li>Updates to latest code:</li> </ol> <p>Pulls the latest changes from the remote repository</p> <p>Applies the demo patch:</p> <p>Applies the demo.patch file using git apply --whitespace=fix This patch includes specific modifications to the codebase for the workshop In particular, it removes the upc: product.upc, line from src/services/ProductService.js (line 52) It also modifies the Dockerfile to use an older Node.js version and changes some package versions</p> <ol> <li>Creates a commit:</li> </ol> <p>Commits the changes with the message \"Demo prep\"</p> <ol> <li>Installs dependencies:</li> </ol> <p>Runs npm install to install all required Node.js dependencies</p> <ol> <li>Downloads container images:</li> </ol> <p>Runs docker compose pull to download all the required Docker images in advance This saves time during the workshop</p> <ol> <li>Prepares for Docker Build Cloud:</li> </ol> <p>Removes postgres:17.2 container image (if it exists) Creates and configures a cloud buildx builder for Docker Build Cloud This enables faster builds using Docker's cloud-based build infrastructure</p> <ol> <li>Configures Docker Scout:</li> </ol> <p>Sets up Docker Scout with the dockerdevrel organization This enables security scanning capabilities for the workshop</p> <p>In summary, the setup.sh script prepares a consistent, clean environment with all necessary dependencies, tools, and configurations so that workshop participants can immediately start working on the exercises without spending time on setup. It also makes deliberate modifications to the codebase (like removing the UPC field from Kafka messages) that will be \"fixed\" during the workshop exercises.</p>"},{"location":"product-catalog/develop/#run-the-compose","title":"Run the Compose","text":"<pre><code>docker compose up -d\n</code></pre>"},{"location":"product-catalog/develop/#setting-up-the-demo","title":"Setting up the Demo","text":"<p>Bring up the API service </p> <pre><code>npm install\nnpm run dev\n</code></pre>"},{"location":"product-catalog/develop/#accessing-the-web-client","title":"Accessing the Web Client","text":"<p>Open the web client (http://localhost:5173) and create a few products.</p>"},{"location":"product-catalog/develop/#accessing-the-database-visualizer","title":"Accessing the database visualizer","text":"<p>Open http://localhost:5050 and validate the products exist in the database.  \"Good! We see the UPCs are persisted in the database\"</p> <p>Use the following Postgres CLI to check if the products are added or not.</p> <pre><code># psql -U postgres\npsql (17.1 (Debian 17.1-1.pgdg120+1))\nType \"help\" for help\npostgres=# \\c catalog\nYou are now connected to database \"catalog\" as user \"postgres\".\ncatalog=# SELECT * FROM products;\n  1 | New Product | 100000000001 | 100.00 | f\n  2 | New Product | 100000000002 | 100.00 | f\n  3 | New Product | 100000000003 | 100.00 | f\n</code></pre>"},{"location":"product-catalog/develop/#access-the-kafka-visualizer","title":"Access the Kafka Visualizer","text":"<p>Before we access visualizer, let's apply the patch:</p> <p>Open the Kafka visualizer http://localhost:8080 and look at the published messages.  \"Ah! We see the messages don't have the UPC\"</p> <p></p>"},{"location":"product-catalog/develop/#lets-fix-it","title":"Let's fix it...","text":""},{"location":"product-catalog/develop/#configuring","title":"Configuring","text":"<p>In VS Code, open the <code>src/services/ProductService.js</code> file and add the following to the publishEvent on line ~52:</p> <pre><code>upc: product.upc,\n</code></pre> <p>Save the file and create a new product using the web UI. </p> <p></p> <p>Validate the message has the expected contents.</p>"},{"location":"product-catalog/overview/","title":"Catalog Service - Node","text":"<p>This repo is a demo project that demonstrates all of Docker's services in a single project. Specifically, it includes the following:</p> <ul> <li>A containerized development environment (in a few varieties of setup)</li> <li>Integration testing with Testcontainers</li> <li>Building in GitHub Actions with Docker Build Cloud</li> </ul> <p>This project is also setup to be used for various demos. </p>"},{"location":"product-catalog/secure/","title":"Secure","text":""},{"location":"product-catalog/secure/#secure-scout","title":"Secure - Scout","text":"<pre><code>docker build -t newcatalog .\n</code></pre>"},{"location":"product-catalog/secure/#apply-the-patches","title":"Apply the patches","text":"<pre><code>patch -p1 &lt; /demo/scout.patch\n</code></pre>"},{"location":"product-catalog/tech-stack/","title":"Tech stack","text":""},{"location":"product-catalog/tech-stack/#application-architecture","title":"Application architecture","text":"<p>This sample app provides an API that utilizes the following setup:</p> <ul> <li>Data is stored in a PostgreSQL database</li> <li>Product images are stored in a AWS S3 bucket</li> <li>Inventory data comes from an external inventory service</li> <li>Updates to products are published to a Kafka cluster</li> </ul> <p></p> <p>During development, containers provide the following services:</p> <ul> <li>PostgreSQL and Kafka runs directly in a container</li> <li>LocalStack is used to run S3 locally</li> <li>WireMock is used to mock the external inventory service</li> <li>pgAdmin and kafbat are added to visualize the PostgreSQL database and Kafka cluster</li> </ul> <p></p>"},{"location":"product-catalog/test/","title":"Test","text":"<pre><code>npm run unit-test\nnpm run integration-test\n</code></pre>"},{"location":"product-catalog/test/#how-does-this-work","title":"How does this work?","text":"<p>In your <code>package.json</code> file, several scripts are being defined related to testing your application:</p> <ul> <li>test: This command runs all tests using Jest, with the --detectOpenHandles flag to detect and report any open handles.</li> <li>unit-test: This command runs unit tests using Jest, with the --detectOpenHandles flag and a specific testPathIgnorePatterns to exclude integration tests.</li> <li>integration-test: This command runs integration tests using Jest, with the --detectOpenHandles flag and a specific test pattern to include only integration tests.</li> <li>test-watch: This command runs Jest in watch mode, allowing you to run tests as you make changes to your code.</li> <li>prepare: This command runs the husky install script, which sets up Git hooks for your project.</li> </ul> <p>To determine which tests are executed when you run <code>npm run integration-test</code>, you need to look at the command itself.  The command is:</p> <pre><code>env-cmd --file .env.node -- jest \"test/integration/.*\\\\.spec\\\\.js\" --detectOpenHandles\n</code></pre> <p>In this command, Jest is executed with the <code>--testPathPattern</code> option set to \"test/integration/.*\\.spec\\.js\". This pattern matches all files in the test/integration directory that end with .spec.js. Therefore, only the integration tests will be executed when you run npm run integration-test.</p> <p>The other scripts, such as <code>test</code>, <code>unit-test</code>, and <code>test-watch</code>, are configured to run different types of tests based on the specified options. The unit-test script excludes integration tests by using the <code>--testPathIgnorePatterns</code> option. </p> <p>The <code>test-watch</code> script runs Jest in watch mode, allowing you to run tests as you make changes to your code.</p> <p>By running <code>npm run integration-test</code>, you can ensure that only the integration tests are executed, providing a focused and targeted testing experience.</p>"},{"location":"product-catalog/test/#whats-inside-integrationspecjs-file","title":"What's inside integration/.*\\.spec\\.js file?","text":"<p>The file integration/.\\.spec\\.js is a JavaScript file containing integration tests for your application. The file name pattern .\\.spec\\.js is a regular expression that matches any file ending with .spec.js in the integration directory.</p> <p>Inside the integration/.*\\.spec\\.js file, you'll find the following components:</p>"},{"location":"product-catalog/test/#1importing-required-modules","title":"1.Importing required modules:","text":"<ul> <li>fs for reading image files.</li> <li>containerSupport for creating and managing Docker containers for testing.</li> <li>kafkaSupport for consuming messages from Kafka.</li> </ul>"},{"location":"product-catalog/test/#2setting-up-and-tearing-down-test-environment","title":"2.Setting up and tearing down test environment:","text":"<ul> <li>Starting Docker containers for PostgreSQL, Kafka, and Localstack using the containerSupport module.</li> <li>Creating an instance of the KafkaConsumer for consuming messages from Kafka.</li> <li>Importing the ProductService and PublisherService for testing the product creation feature.</li> <li>Disconnecting from Kafka and tearing down the test environment after all tests have completed.</li> </ul>"},{"location":"product-catalog/test/#3defining-unit-and-integration-tests","title":"3.Defining unit and integration tests:","text":"<ul> <li>\"Product creation\" test suite:</li> <li>\"should publish and return a product when creating a product\": Tests the creation of a product, verification of the product's properties, and retrieval of the product from the product service.</li> <li>\"should publish a Kafka message when creating a product\": Tests the creation of a product and verification of the corresponding Kafka message.</li> <li>\"should upload a file correctly\": Tests the upload of an image file, verification of the corresponding Kafka message, and retrieval of the image file from the product service.</li> <li>\"doesn't allow duplicate UPCs\": Tests the prevention of creating products with duplicate UPCs.</li> </ul> <p>The script also includes a timeout value for starting the Docker containers and waiting for messages from Kafka, ensuring that the tests have enough time to complete.</p> <p>Overall, the script provides a comprehensive set of unit and integration tests for the \"Product creation\" feature, ensuring that the functionality is working as expected and providing valuable feedback on the overall system.</p>"},{"location":"product-catalog/test/#accessing-testcontainers-desktop-app","title":"Accessing Testcontainers Desktop App","text":"<p>Open Testcontainer Desktop app and you'll notice that 3 containers appear and clean up once the test gets completed.</p> <p></p>"},{"location":"product-catalog/test/#how-about-the-unit-tests","title":"How about the unit-tests?","text":"<p>Unit tests are tests that focus on individual components or units of your application, validating the functionality and behavior of specific parts of your code. They are essential for catching and identifying issues at a granular level, ensuring the reliability and correctness of your application.</p> <p>For example, the \"Product creation\" test suite includes tests such as: - \"should publish and return a product when creating a product\" - \"should publish a Kafka message when creating a product\" - \"should upload a file correctly\" - \"doesn't allow duplicate UPCs\"</p> <p>When you run <code>npm run unit-test</code>, the following steps are executed based on the configuration in your package.json file:</p> <ol> <li> <p>The <code>env-cmd</code> command is executed with the <code>--file .env.node option</code>, which loads environment variables from the <code>.env.node</code> file.</p> </li> <li> <p>The jest command is executed with the specified options:</p> </li> <li> <p><code>--detectOpenHandles</code>: This flag detects and reports any open handles.</p> </li> <li><code>--testPathIgnorePatterns \"test/integration/.*\\\\.spec\\\\.js\"</code>: This option excludes integration tests by using a regular expression pattern that matches the file names of integration test files. The jest command runs the unit tests defined in your project. The  <code>--testPathIgnorePatterns</code> option ensures that only unit tests are executed, excluding integration tests.</li> </ol> <p>In summary, when you run <code>npm run unit-test</code>, the unit tests are executed using Jest, with the specified options to exclude integration tests and detect open handles.</p>"}]}