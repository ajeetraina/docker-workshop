{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Docker GenAI Workshop","text":""},{"location":"#github-sources","title":"GitHub Sources","text":"<p>The source code for this workshop is available here</p> <ul> <li>Ajeet Singh Raina - DevRel @Docker</li> </ul>"},{"location":"#benefits-of-this-docker-workshop","title":"Benefits of this Docker Workshop","text":"<ul> <li>Learn about Docker Model Runner</li> <li>Learn how to get started with Model Runner CLI</li> <li>Learn how to build GenAI Chatbot using Docker Model Runner</li> <li>Learn how to use Docker MCP Catalog and Toolkit</li> </ul>"},{"location":"lab1/best-practices/","title":"Best practices","text":""},{"location":"lab1/best-practices/#1-use-explicit-base-image-reference-instead-of-the-latest","title":"1. Use explicit base image reference instead of the latest","text":"<p>Developers are often led to believe that specifying the latest tag will provide them with the most recent image in the repository but it has some side effects.</p> <p>Image tags are mutable, meaning a publisher can update a tag to point to a new image. For example, if you specify FROM node:latest in your Dockerfile, it might resolve to the latest patch version for 18.11. However, if you rebuild the image 3 months later, the same tag might point to a different version, such as 18.13. This could result in breaking changes, and it means you also don't have an audit trail of the exact image versions that you're using.. </p> <p></p>"},{"location":"lab1/best-practices/#2-prefer-leaner-docker-images","title":"2. Prefer leaner Docker Images","text":"<p>Using leaner Docker images can help reduce the size of the final image, which can lead to faster build times, smaller storage footprint, and quicker deployment times.</p> <p>For example, try to use Slimmer Images. Select smaller images for your FROM instructions in your Dockerfile. For example, the `node:16.17.0-slim image is a minimal Docker image that provides all of the OS utilities you would expect from a Linux container. There's also the special scratch image, which contains nothing at all and is useful for creating images of statically linked binaries (source).</p> <p></p>"},{"location":"lab1/best-practices/#3-use-multi-stage-builds","title":"3. Use Multi-stage builds","text":"<p>Multi-stage builds let you reduce the size of your final image, by creating a cleaner separation between the building of your image and the final output. Split your Dockerfile instructions into distinct stages to make sure that the resulting output only contains the files that's needed to run the application.</p> <p>Using multiple stages can also let you build more efficiently by executing build steps in parallel.</p> <p></p>"},{"location":"lab1/best-practices/#4-quickly-identify-and-fix-vulnerabilities-during-the-build-time-using-docker-scout","title":"4. Quickly identify and fix vulnerabilities during the Build time using Docker Scout","text":"<p>Container images consist of layers and software packages, which are susceptible to vulnerabilities. These vulnerabilities can compromise the security of containers and applications.</p> <p>Docker Scout is a solution for proactively enhancing your software supply chain security. By analyzing your images, Docker Scout compiles an inventory of components, also known as a Software Bill of Materials (SBOM). The SBOM is matched against a continuously updated vulnerability database to pinpoint security weaknesses.</p> <p>Docker Scout image analysis is available by default for Docker Hub repositories. You can also integrate third-party registries and other services</p> <p></p>"},{"location":"lab1/best-practices/#5-add-healthcheck-in-dockerfile-and-docker-compose","title":"5. Add Healthcheck in Dockerfile and Docker Compose","text":"<p>You can add a healthcheck in both Dockerfile and Docker Compose file. In a Dockerfile, you can use the HEALTHCHECK instruction. Here's an example:</p> <pre><code>HEALTHCHECK --interval=5m --timeout=3s \\\n  CMD curl -f http://localhost/ || exit 1\n</code></pre> <p>In this example, Docker will check every five minutes if a web-server is able to serve the site's main page within three seconds. If the command (curl -f http://localhost/ || exit 1) returns a non-zero code, the container is considered unhealthy (source).</p> <p>In a Docker Compose file, you can use the healthcheck attribute under a service. Here's an example:</p> <pre><code>services:\n  web:\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost\"]\n      interval: 1m30s\n      timeout: 10s\n      retries: 3\n</code></pre> <p>In this example, Docker Compose will run the command curl -f http://localhost to check the health of the web service. It will do this every 1 minute and 30 seconds, and if the command doesn't return within 10 seconds or if it fails 3 times in a row, the service is considered unhealthy (source).</p>"},{"location":"lab1/best-practices/#6-use-dockerignore","title":"6. Use .dockerignore","text":"<p>The .dockerignore file is used to exclude files and directories from the build context when building a Docker image. This can help to improve build speed and avoid sending unwanted files to the Docker builder. The syntax of the .dockerignore file is similar to the .gitignore file, with each line representing a pattern that matches files and directories to be excluded.</p> <p>Here's an example of a .dockerignore file:</p> <pre><code># .dockerignore\nnode_modules\nbar\n</code></pre> <p>In this example, the node_modules directory and any file or directory named bar are excluded from the build context.</p> <p>When you run a build command, Docker looks for a .dockerignore file in the root directory of the context. If this file exists, the files and directories that match patterns in the file are removed from the build context before it's sent to the builder. If you have multiple Dockerfiles, you can use different .dockerignore files for each Dockerfile by using a special naming convention. You should place your .dockerignore file in the same directory as the Dockerfile, and prefix the .dockerignore file with the name of the Dockerfile. For example, for a Dockerfile named myapp.Dockerfile, you would create a .dockerignore file named myapp.Dockerfile.dockerignore.</p> <pre><code>.\n\u251c\u2500\u2500 index.ts\n\u251c\u2500\u2500 src/\n\u251c\u2500\u2500 docker\n\u2502   \u251c\u2500\u2500 build.Dockerfile\n\u2502   \u251c\u2500\u2500 build.Dockerfile.dockerignore\n\u2502   \u251c\u2500\u2500 lint.Dockerfile\n\u2502   \u251c\u2500\u2500 lint.Dockerfile.dockerignore\n\u2502   \u251c\u2500\u2500 test.Dockerfile\n\u2502   \u2514\u2500\u2500 test.Dockerfile.dockerignore\n\u251c\u2500\u2500 package.json\n\u2514\u2500\u2500 package-lock.json\n</code></pre> <p>In this example, each Dockerfile has its own corresponding .dockerignore file. If both a Dockerfile-specific .dockerignore file and a .dockerignore file at the root of the build context exist, the Dockerfile-specific .dockerignore file takes precedence.</p>"},{"location":"lab1/best-practices/#7-run-as-non-root-user-for-security-purpose","title":"7. Run as non-root user for security purpose","text":"<p>Running Docker as a non-root user is a good practice to mitigate potential vulnerabilities in the Docker daemon and the container runtime. Docker provides a feature called \"Rootless mode\" that allows running the Docker daemon and containers as a non-root user.</p> <p></p>"},{"location":"lab1/best-practices/#8-favour-multi-architecture-docker-images","title":"8. Favour Multi-Architecture Docker Images","text":"<p>Using multi-architecture Docker images is beneficial as it allows your Docker images to run on different hardware architectures without any modifications. This means that whether you are using an ARM-based system or an x86 machine, Docker automatically detects and selects the appropriate variant for your host's operating system and architecture.</p> <p>There are three strategies to build multi-platform images depending on your use case: - Using emulation, via QEMU support in the Linux kernel. - Building on a single builder backed by multiple nodes of different architectures. - Using a stage in your Dockerfile to cross-compile to different architectures.</p> <p>To build multi-platform images, you can use the --platform flag with the docker build command to define the target platforms for the build output, such as linux/amd64 and linux/arm64.  For example:</p> <pre><code>$ docker build --platform linux/amd64,linux/arm64 .\n</code></pre> <p>By default, Docker can build for only one platform at a time. To build for multiple platforms concurrently, you can enable the containerd image store or create a custom builder. For example, to enable the containerd image store in Docker Desktop, go to Settings and select Use containerd for pulling and storing images in the General tab. If you're not using Docker Desktop, enable the containerd image store by adding the following feature configuration to your /etc/docker/daemon.json configuration file.</p> <pre><code>{\n  \"features\": {\n    \"containerd-snapshotter\": true\n  }\n}\n</code></pre> <p>Then, restart the daemon after updating the configuration file.</p> <pre><code>$ systemctl restart docker\n</code></pre> <p></p>"},{"location":"lab1/compose-watch/","title":"Compose watch","text":"<p>Compose File Watch is a feature introduced in Docker Compose version 2.22.0. It allows for automatic updates and previews of your running Compose services as you edit and save your code. This can enable a hands-off development workflow once Compose is running, as services automatically update themselves when you save your work.</p> <pre><code>services:\n  web:\n    build: .\n    command: npm start\n    develop:\n      watch:   \n        - actions: sync\n          path: ./web\n          target: /src/web\n          ignore: \n            - node_modules/\n        - action: rebuild\n          path: package.json\n</code></pre> <p>The <code>watch</code> attribute in the Compose file defines a list of rules that control these automatic service updates based on local file changes. Each rule requires a path pattern and an action to take when a modification is detected. The action can be set to rebuild, sync, or sync+restart.</p> <p>Here's a brief explanation of these actions:</p> <ul> <li>rebuild: Compose rebuilds the service image based on the build section and recreates the service with the updated image.</li> <li>sync: Compose keeps the existing service container(s) running, but synchronizes source files with container content according to the target attribute.</li> <li>sync+restart: Compose synchronizes source files with container content according to the target attribute, and then restarts the container.</li> </ul> <p>You can also define a list of patterns for paths to be ignored using the ignore attribute. Any updated file that matches a pattern, or belongs to a folder that matches a pattern, won't trigger services to be re-created. To use Compose Watch, you need to add the watch instructions to your compose.yaml file and then run your application with the docker compose watch command.</p> <p></p>"},{"location":"lab1/compose-watch/#getting-started","title":"Getting Started","text":""},{"location":"lab1/compose-watch/#clone-the-repository","title":"Clone the repository","text":"<pre><code>git clone https://github.com/dockersamples/getting-started-todo-app/\ncd getting-started-todo-app\n</code></pre>"},{"location":"lab1/compose-watch/#switch-to-compose-watch-branch","title":"Switch to compose-watch branch","text":"<pre><code>git checkout compose-watch\n</code></pre>"},{"location":"lab1/compose-watch/#bringing-up-the-app","title":"Bringing up the app","text":"<pre><code>docker compose watch\n</code></pre>"},{"location":"lab1/compose-watch/#make-some-changes","title":"Make some changes","text":"<p>Open a new terminal and modify the following package under frontend/package.json from </p> <pre><code>\"optionalDependencies\": {\n    \"fsevents\": \"^2.1.2\"\n</code></pre> <p>to</p> <pre><code>\"optionalDependencies\": {\n    \"fsevents\": \"^2.1.3\"\n</code></pre> <p>You will find that the frontend service gets rebuild automatcially for you</p> <pre><code>[+] Running 6/6\n \u2714 Network getting-started-todo-app_express-mongo     Created                                                          0.1s \n \u2714 Network getting-started-todo-app_react-express     Created                                                          0.0s \n \u2714 Container mongo                                    Started                                                          1.1s \n \u2714 Container getting-started-todo-app-mongoexpress-1  Started                                                          1.1s \n \u2714 Container backend                                  Started                                                          1.3s \n \u2714 Container frontend                                 Started                                                          1.8s \nWatch enabled\nRebuilding service \"frontend\" after changes were detected...\n[+] Building 52.5s (12/12) FINISHED                                                             docker-container:my-builder\n =&gt; [frontend internal] load build definition from Dockerfile                                                          0.0s\n =&gt; =&gt; transferring dockerfile: 532B                                                                                   0.0s\n =&gt; [frontend internal] load metadata for docker.io/library/node:lts-buster                                            1.1s\n =&gt; [frontend internal] load .dockerignore                                                                             0.0s\n =&gt; =&gt; transferring context: 67B                                                                                       0.0s\n =&gt; [frontend 1/6] FROM docker.io/library/node:lts-buster@sha256:479103df06b40b90f189461b6f824a62906683e26a32c77d7c3e  0.0s\n =&gt; =&gt; resolve docker.io/library/node:lts-buster@sha256:479103df06b40b90f189461b6f824a62906683e26a32c77d7c3e2d855a0e3  0.0s\n =&gt; [frontend internal] load build context                                                                             0.0s\n =&gt; =&gt; transferring context: 1.94kB                                                                                    0.0s\n =&gt; CACHED [frontend 2/6] WORKDIR /usr/src/app                                                                         0.0s\n =&gt; [frontend 3/6] COPY package.json /usr/src/app                                                                      0.0s\n =&gt; [frontend 4/6] COPY package-lock.json /usr/src/app                                                                 0.0s\n =&gt; [frontend 5/6] RUN npm ci                                                                                         24.4s\n =&gt; [frontend 6/6] COPY . /usr/src/app                                                                                 0.3s \n =&gt; [frontend] exporting to docker image format                                                                       26.5s \n =&gt; =&gt; exporting layers                                                                                                8.4s \n =&gt; =&gt; exporting manifest sha256:c3639997f048e5adea7487bafdafaf4ea3f946e071fd273aefb262b72f2c87a8                      0.0s \n =&gt; =&gt; exporting config sha256:1eb3a22fdb9d89316c9bdb1eb1d9f138e0f9545e95af4d812c9db8499309b491                        0.0s\n =&gt; =&gt; sending tarball                                                                                                18.1s\n =&gt; [frontend] importing to docker                                                                                    10.0s\n =&gt; =&gt; loading layer 7544a5e696a4 567B / 567B                                                                         10.0s\n =&gt; =&gt; loading layer 6e1b4449163b 32.77kB / 128.96kB                                                                   9.9s\n =&gt; =&gt; loading layer 874fffdf421f 115.87MB / 121.34MB                                                                  9.9s\n =&gt; =&gt; loading layer b8d4de9612f2 27.14kB / 27.14kB                                                                    0.0s\nservice \"frontend\" successfully built\n</code></pre>"},{"location":"lab1/docker-init/","title":"Docker init","text":"<p>Introduced for the first time in Docker Desktop 4.18, the new docker init CLI generates Docker assets for projects, making it easier to create Docker images and containers. When you run the docker init command in your project directory, it will guide you through the creation of the necessary files for your project with sensible defaults. These files include:</p> <pre><code>.dockerignore\nDockerfile\ndocker-compose.yaml\n</code></pre> <p>The docker init command also allows you to choose the application platform that your project uses and the relative directory of your main package. </p>"},{"location":"lab1/docker-init/#whos-this-for","title":"Who\u2019s this for?","text":"<p>This feature is targeted at developers who want to quickly create and manage Docker assets without having to manually configure everything. </p>"},{"location":"lab1/docker-init/#benefits-of-docker-init","title":"Benefits of Docker Init","text":"<p>The advantages of using the docker init command include:</p> <ul> <li>Simplified Docker asset creation: The command streamlines the creation of necessary Docker files, reducing the chances of errors and ensuring that best practices are followed.</li> <li>Saves time and effort: With the default settings and guided prompts, users can quickly create Docker assets without the need for extensive knowledge of Docker or its syntax.</li> <li>Better project organization: The generated files provide a standardized and organized structure for the project, making it easier for developers to maintain and update the project over time.</li> <li>Enhanced portability: By using Docker assets, projects become more portable across different environments, making it easier to move the project from development to production.</li> </ul> <p></p>"},{"location":"lab1/docker-init/#getting-started","title":"Getting Started","text":"<ul> <li>Install the latest version of Docker Desktop</li> <li>Install Nodejs on your local system</li> </ul> <p>Note: You must download and install the Node pre-built installer on your local system to get the <code>npm install</code> command to work seamlessly. Click here to download</p>"},{"location":"lab1/docker-init/#clone-the-repository","title":"Clone the repository","text":"<pre><code>git clone https://github.com/dockersamples/docker-init-demos\ncd docker-init-demos/node\n</code></pre> <p>The usual way to bring up this node application is to follow the steps:</p> <pre><code>npm install\nnode app.js\n</code></pre> <p>You can verify the output by accessing the URL:</p> <pre><code>curl localhost:8080\n</code></pre> <p>Now let's see how to containerise this project using the <code>docker init</code> CLI.</p>"},{"location":"lab1/docker-init/#run-the-following-command","title":"Run the following command:","text":"<pre><code> docker init\n</code></pre> <p>This utility will walk you through creating the following files with sensible defaults for your project:</p> <ul> <li>Select Node as your application platform</li> <li>Type \"22.2.0\" as Node version, if it doesn't provide you any default option</li> <li>Select npm as package manager</li> <li>Select \"node app.js\" as the command</li> <li>Type \"8080\" as a port that server listens on</li> </ul> <p>The tool creates the following Docker assets for you:</p> <pre><code>  - .dockerignore\n  - Dockerfile\n  - docker-compose.yaml\n</code></pre>"},{"location":"lab1/docker-init/#running-the-container-service","title":"Running the container service","text":"<p><code>docker compose up -d --build</code></p>"},{"location":"lab1/docker-init/#accessing-the-node-app","title":"Accessing the Node app","text":"<pre><code> curl localhost:8080      .\n/\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\\___/ ===\n{                       /  ===-\n\\______ O           __/\n \\    \\         __/\n  \\____\\_______/\n\n\nHello from Docker\n</code></pre>"},{"location":"lab1/docker-workflow/","title":"Docker workflow","text":"<ul> <li> <p>Build: Typically a developer run <code>docker build</code> command to build an image from a Dockerfile, which is a text document that contains instructions for how to build the image.</p> </li> <li> <p>Share: Once a developer builds a Docker image, they can share it with others by pushing it to a container registry. You can use the <code>docker push</code> command to do this.</p> </li> <li> <p>Run: The <code>docker run</code> command lets you create and start a running container based on a Docker image.</p> </li> </ul> <p></p>"},{"location":"lab1/docker-workflow/#docker-inner-loop-development-workflow","title":"Docker Inner-Loop Development workflow","text":"<ul> <li>Development Environment: As you deep-dive into the inner-loop development workflow, developers have their own choice anduse any operating system (OS) they prefer, like Windows, Mac, or Linux. They can also leverage their favorite Integrated Development Environments (IDEs) for coding. This flexibility empowers developers to work in a familiar and comfortable setting.</li> </ul> <ul> <li> <p>Building and storing the application - Once the code is written, the developer uses GitHub Actions. GitHub Actions is a built-in automation tool in GitHub that allows you to automate tasks within your development workflow. In this context, the developer uses GitHub Actions to trigger the building of a Docker image. A Dockerfile, which is a set of instructions that specifies how to build the image, is used in this process.  After the image is built, it's uploaded to a container registry. A container registry acts as a library or repository that stores Docker images.</p> </li> <li> <p>Deployment - Finally, the image is deployed to the cloud. This means the image is uploaded to a cloud platform where it can be run on virtual machines. There are several cloud platforms available, including Google Cloud Platform, Amazon Web Services, and Microsoft Azure. The specific steps for deployment will vary depending on the chosen cloud platform. But generally, it involves using the cloud platform tools to run the Docker image and create a container instance. This container instance then executes the application.</p> </li> </ul>"},{"location":"lab1/overview/","title":"Inner Vs Outer Loop Development workflow","text":"<p>The inner loop is the iterative process of writing, building, and debugging code that a single developer performs before sharing the code, either publicly or with their team. It is typically characterized by frequent changes to the code as the developer learns more about the problem they are trying to solve.</p> <p>The outer loop is everything else that happens leading up to release. This includes code merge, automated code review, test execution, deployment, controlled (canary) release, and observation of results. It is typically characterized by less frequent changes to the code, as the focus is on ensuring that the code is stable and ready for production..</p> <p>The inner loop is often associated with the development phase of the SDLC, while the outer loop is associated with the testing, deployment, and release phases. However, the two loops are not mutually exclusive, and they can overlap in some cases.</p> <p></p>"},{"location":"lab1/postgres/","title":"Running Postgres Containers","text":""},{"location":"lab1/postgres/#running-multiple-postgres-containers","title":"Running Multiple Postgres Containers","text":"<pre><code>docker run -d --name postgres1 -e POSTGRES_PASSWORD=dev -p 5432:5432 postgres:latest\n</code></pre> <pre><code>docker run -d --name postgres2 -e POSTGRES_PASSWORD=dev -p 5433:5432 postgres:13\n</code></pre> <pre><code>docker run -d --name postgres3 -e POSTGRES_PASSWORD=dev -p 5434:5432 postgres:12\n</code></pre> <p>Open Docker Dashboard and run the following commands:</p> <pre><code># psql -d postgres -U postgres -W\nPassword: \npsql (17.2 (Debian 17.2-1.pgdg120+1))\nType \"help\" for help.\n\npostgres=# \n</code></pre> <p>The above command includes three flags:</p> <ul> <li>-d - specifies the name of the database to connect to</li> <li>-U - specifies the name of the user to connect as</li> <li>-W - forces psql to ask for the user password before connecting to the database</li> </ul>"},{"location":"lab1/postgres/#listing-all-the-databases-l","title":"Listing all the databases - \\l","text":"<pre><code>\n                                                    List of databases\n   Name    |  Owner   | Encoding | Locale Provider |  Collate   |   Ctype    | Locale | ICU Rules |   Access privileges   \n-----------+----------+----------+-----------------+------------+------------+--------+-----------+-----------------------\n postgres  | postgres | UTF8     | libc            | en_US.utf8 | en_US.utf8 |        |           | \n template0 | postgres | UTF8     | libc            | en_US.utf8 | en_US.utf8 |        |           | =c/po\nstgres          +\n           |          |          |                 |            |            |        |           | postg\nres=CTc/postgres\n template1 | postgres | UTF8     | libc            | en_US.utf8 | en_US.utf8 |        |           | =c/po\nstgres          +\n           |          |          |                 |            |            |        |           | postg\nres=CTc/postgres\n(3 rows)\n</code></pre>"},{"location":"lab1/postgres/#list-all-schemas","title":"List all schemas","text":"<p>The <code>\\dn</code> psql command lists all the database schemas.</p> <pre><code>postgres=# \\dn\n      List of schemas\n  Name  |       Owner       \n--------+-------------------\n public | pg_database_owner\n(1 row)\n\npostgres=#\n</code></pre>"},{"location":"lab1/postgres/#run-the-following-command-to-show-the-database-activity","title":"Run the following command to show the database activity:","text":"<pre><code>SELECT * from pg_stat_activity;  &lt;--- DONT FORGET \";\"\n</code></pre>"},{"location":"lab1/postgres/#result","title":"Result:","text":"<pre><code>datid | datname  | pid | leader_pid | usesysid | usename  | application_name | client_addr | client_host\nname | client_port |         backend_start         |          xact_start           |          query_start\n          |         state_change          | wait_event_type |     wait_event      | state  | backend_xid \n| backend_xmin | query_id |              query              |         backend_type         \n-------+----------+-----+------------+----------+----------+------------------+-------------+------------\n-----+-------------+-------------------------------+-------------------------------+---------------------\n----------+-------------------------------+-----------------+---------------------+--------+-------------\n+--------------+----------+---------------------------------+------------------------------\n     5 | postgres |  85 |            |       10 | postgres | psql             |             |            \n     |          -1 | 2025-01-08 11:40:22.778949+00 |                               | 2025-01-08 11:40:56.\n590114+00 | 2025-01-08 11:41:26.598462+00 | Client          | ClientRead          | idle   |             \n|              |          | SELECT pg_sleep(30);            | client backend\n     5 | postgres |  92 |            |       10 | postgres | psql             |             |            \n     |          -1 | 2025-01-08 11:41:14.359414+00 | 2025-01-08 11:43:52.615603+00 | 2025-01-08 11:43:52.\n615603+00 | 2025-01-08 11:43:52.61561+00  |                 |                     | active |             \n|          750 |          | SELECT * FROM pg_stat_activity; | client backend\n       |          |  64 |            |          |          |                  |             |            \n     |             | 2025-01-08 11:38:28.74611+00  |                               |                     \n          |                               | Activity        | AutovacuumMain      |        |             \n|              |          |                                 | autovacuum launcher\n       |          |  65 |            |       10 | postgres |                  |             |            \n:\n</code></pre>"},{"location":"lab1/postgres/#remove-the-container","title":"Remove the container","text":"<p>Open Docker Dashboard &gt; Selecting all the running Postgres containsrs and delete them in a single step.</p>"},{"location":"lab1/what-is-a-container/","title":"What is a container","text":"<p>Let\u2019s compare containers to smartphone apps. </p> <p></p> <p>Most of us have smartphones that have lots of apps installed on them.</p> <p>When is the last time you thought about how to install the dependencies for one of those apps, how to configure it, and how to set it up? Well, probably never.</p> <p>We typically just open the app store, click the Install button , and then the app is there. <p>And when we open the newly installed red app, we don\u2019t have to worry about how the dependencies and libraries for the green app are going to affect it - they all run in isolated or sandboxed environments.</p> <p>Containers bring this same idea to other types of applications and services, although they are implemented a little differently.</p>"},{"location":"lab10/overview/","title":"Overview","text":""},{"location":"lab10/overview/#what-is-cagent","title":"What is cagent?","text":"<p>Docker cagent is an open-source, multi-agent runtime designed to simplify the development and deployment of autonomous AI systems. Unlike traditional chatbots, this tool allows developers to orchestrate specialized teams of agents that can plan, reason, and execute complex tasks through a declarative YAML configuration. </p>"},{"location":"lab10/overview/#what-problem-it-solves","title":"What problem it solves?","text":"<p>AI agents today work alone. They can't collaborate or specialize. This creates several challenges:</p> <ul> <li>Single Point of Failure: One agent handles everything. No specialization means lower quality results.</li> <li>No Task Delegation: Complex projects need different expertise. Current agents can't hand off work to specialists.</li> <li>Tool Access Chaos: Each agent manages its own tools separately. No unified tool ecosystem across agents.</li> <li>Configuration Complexity: Setting up multiple AI systems requires separate configurations. Each client needs different setup.</li> <li>Security Gaps: Running AI tools with full system access. No container isolation or proper secret management.</li> <li>Scaling Problems: Can't distribute workload across multiple specialized agents. Everything bottlenecks through one system.</li> </ul>"},{"location":"lab10/overview/#the-solution","title":"The Solution","text":"<p>Docker introduces cagent - a multi-agent orchestration platform that solves these problems through: Hierarchical Agent Teams: Root agents coordinate with specialized sub-agents. Each agent has specific expertise and tools. Unified Tool Ecosystem: All agents share the same secure tool infrastructure. Built-in tools plus MCP server integration. Enterprise Security: Container isolation for tool execution. Proper secret management and multi-tenant support.</p>"},{"location":"lab10/overview/#introducing-cagent","title":"Introducing cagent","text":"<p>cagent enables you to create intelligent agent teams where each agent has specialized knowledge, tools, and capabilities. Think of it as building a virtual team of AI experts that collaborate to solve complex problems.</p> <p>Built in Go by Docker, cagent brings enterprise-grade security and scalability to multi-agent AI systems.</p>"},{"location":"lab10/overview/#key-features","title":"Key Features","text":"<ul> <li>Multi-Agent Architecture: Create specialized agents for different domains. Root agents delegate tasks to expert sub-agents automatically.</li> <li>Rich Tool Ecosystem: Built-in tools (think, todo, memory, filesystem, shell). Plus MCP protocol integration for external tools.</li> <li>Multiple AI Providers: Support for OpenAI, Anthropic, Gemini, and Docker Model Runner (DMR). Mix different models in one conversation with \"alloy models\".</li> <li>Flexible Interfaces: CLI, Web UI, TUI, and MCP Server modes. Same agent configs work across all interfaces.</li> <li>YAML Configuration: Simple, declarative setup for agents, models, and tools. Version control friendly configurations.</li> <li>Docker Integration: Push and pull agent configs like container images. Share agents through Docker Hub.</li> </ul> <p>Built-in Security: Container isolation, secret management, and multi-tenant support. Production-ready from day one.</p>"},{"location":"lab10/overview/#whos-this-for","title":"Who's This For","text":"<ul> <li>AI Application Developers: Build sophisticated AI applications with specialized agent teams. Focus on business logic while cagent handles coordination.</li> <li>Enterprise Development Teams: Deploy secure, scalable AI agent systems. Centralized management with proper access controls.</li> <li>DevOps Engineers: Container-native AI agent platform. Integrates with existing Docker workflows and Kubernetes deployments.</li> <li>AI Researchers: Experiment with multi-agent architectures. Easy configuration and tool integration for research projects.</li> </ul>"},{"location":"lab2/aws-s3-setup/","title":"Configuring AWS with IAM and S3","text":""},{"location":"lab2/aws-s3-setup/#1-login-to-aws-and-create-a-user","title":"1. Login to AWS and create a user","text":""},{"location":"lab2/aws-s3-setup/#2-add-a-user-called-developer","title":"2. Add a user called developer","text":""},{"location":"lab2/aws-s3-setup/#3-creat-a-group","title":"3. Creat a group","text":""},{"location":"lab2/aws-s3-setup/#4-add-developer-to-admin1-group","title":"4. Add developer to admin1 group","text":""},{"location":"lab2/aws-s3-setup/#5-review-and-save","title":"5. Review and save","text":""},{"location":"lab2/aws-s3-setup/#6-enable-access-keys-and-security-key-for-this-user","title":"6. Enable Access Keys and Security key for this user","text":""},{"location":"lab2/aws-s3-setup/#7-create-a-s3-bucket","title":"7. Create a S3 bucket","text":""},{"location":"lab2/aws-s3-setup/#8-grant-this-user-with-s3-bucket-access","title":"8. Grant this user with S3 bucket access","text":""},{"location":"lab2/getting-started/","title":"Getting started","text":""},{"location":"lab2/getting-started/#a-basic-todo-list-app","title":"A Basic Todo List App","text":"<p>This is a starting point for todo-list app powered with React, Node, Mongo and Mongo Express.</p>"},{"location":"lab2/getting-started/#tech-stack","title":"Tech Stack","text":"<ul> <li>Frontend: React</li> <li>Backend: Node.js</li> <li>Database: Mongo DB</li> <li>Database Admin Interface: MongoExpress</li> </ul>"},{"location":"lab2/getting-started/#clone-the-repository","title":"Clone the repository","text":"<pre><code>git clone https://github.com/dockersamples/getting-started-todo-app\ncd getting-started-todo-app\n</code></pre>"},{"location":"lab2/getting-started/#switch-to-basic-branch","title":"Switch to <code>basic</code> branch","text":"<pre><code>git checkout basic\n</code></pre>"},{"location":"lab2/getting-started/#bringing-up-the-service-containers","title":"Bringing up the service containers","text":"<pre><code>docker compose up -d\n</code></pre> <p>After the application starts, navigate to <code>http://localhost:3000</code> in your web browser.</p>"},{"location":"lab2/getting-started/#logging-into-mongo-express","title":"Logging into Mongo Express","text":"<p>Enter <code>admin/pass</code> as credential to login into Mongo Express and view the database and collections/items.</p>"},{"location":"lab2/overview/","title":"Overview","text":"<p>Container-first development takes the concept of containerization a step further. It involves using containers for every aspect of software development, including the application runtime itself. This means developers can ditch traditional local installations and leverage containers for everything needed to run the application.</p>"},{"location":"lab2/overview/#what-it-means","title":"What it means?","text":"<p>In container-first development, developers work within a containerized environment. They:</p> <ul> <li>Clone the project repository.</li> <li>Run a command (often docker-compose up or docker-compose watch) to start the development environment. This command usually pulls pre-built container images containing the application runtime (e.g., Node.js, Python interpreter) and any dependencies.</li> </ul> <p>That's it! The development environment is up and running entirely within containers. No need to install the application runtime or specific libraries on the developer's machine.</p>"},{"location":"lab2/overview/#benefits-for-developers","title":"Benefits for Developers:","text":"<ul> <li>Extreme Portability: Developers only need a container engine and an IDE to work on the project. This ensures identical development environments regardless of the underlying operating system or pre-installed software.</li> <li>Faster Setup: No time wasted installing the application runtime or fiddling with local configurations. Developers can start coding as soon as the containerized environment is up.</li> <li>Improved Isolation: Each project runs within its own isolated container, preventing conflicts between projects or dependencies from interfering with other applications on the developer's machine.</li> <li>Simplified Collaboration: Team members can easily share and reproduce development environments using the same container images.</li> </ul>"},{"location":"lab2/overview/#choosing-container-first","title":"Choosing Container-First:","text":"<ul> <li>Container-first development is ideal for teams seeking:</li> <li>Maximum portability across development environments.</li> <li>Fast and streamlined development setup.</li> <li>Strong isolation between projects to avoid conflicts.</li> </ul> <p>However, it requires a steeper learning curve for containerization technologies and might have higher resource demands.</p> <p>Container-first development offers a powerful approach for building applications entirely within containerized environments. It streamlines development workflows, ensures consistent development environments, and promotes collaboration. But, it's important to weigh the benefits against the increased complexity and potential resource usage before adopting this approach for your development team.</p>"},{"location":"lab2/services/","title":"Services","text":""},{"location":"lab2/services/#1-clone-the-repository","title":"1. Clone the repository:","text":"<pre><code>git clone https://github.com/dockersamples/getting-started-todo-app\ncd getting-started-todo-app\n</code></pre>"},{"location":"lab2/services/#2-switch-to-container-first-branch","title":"2. Switch to container-first branch","text":"<pre><code>git checkout container-first-aws-mongo\n</code></pre>"},{"location":"lab2/services/#3-add-the-environment-variables","title":"3. Add the Environment Variables","text":"<p>Copy .env.sample to .env file and Ensure that you have the right environmental variable added as shown:</p> <pre><code>MONGODB_URI=mongodb://mongodb:27017/todo-app\nJWT_SECRET=603b31XXXXXXX90d3b8cb62f0a585fd70a5ee0b4d\nAWS_ACCESS_KEY_ID=AKIAXXXXXDDDX\nAWS_SECRET_ACCESS_KEY=hSYXtvXXXXXXXO/k39FGt3u078pYWsh\nAWS_REGION=us-east-1\nS3_BUCKET_NAME=localbuckett\n</code></pre> <p>You can leverage this link to generate JWT token.</p>"},{"location":"lab2/services/#4-bring-up-the-services","title":"4. Bring up the services:","text":"<pre><code>docker compose watch\n</code></pre>"},{"location":"lab2/services/#5-access-the-app","title":"5. Access the app","text":"<p>Open http://localhost:3000 to access the todo-list app. Try adding a task and uploading the image.</p>"},{"location":"lab2/services/#verify-mongo","title":"Verify Mongo","text":"<p>You can verify if task gets added by selecting the container and clicking on \"Exec\" option on the Docker dashboard. Now you should be able to run the following command to verify the tasks.</p> <pre><code># mongosh\nCurrent Mongosh Log ID: 66879e864955d6e7b2f3f54d\nConnecting to:          mongodb://127.0.0.1:27017/?directConnection=true&amp;serverSelectionTimeoutMS=2000&amp;appName=mongosh+2.2.10\nUsing MongoDB:          7.0.12\nUsing Mongosh:          2.2.10\n\nFor mongosh info see: https://docs.mongodb.com/mongodb-shell/\n\n\nTo help improve our products, anonymous usage data is collected and sent to MongoDB periodically (https://www.mongodb.com/legal/privacy-policy).\nYou can opt-out by running the disableTelemetry() command.\n\n------\n   The server generated these startup warnings when booting\n   2024-07-05T07:18:03.008+00:00: Using the XFS filesystem is strongly recommended with the WiredTiger storage engine. See http://dochub.mongodb.org/core/prodnotes-filesystem\n   2024-07-05T07:18:03.737+00:00: Access control is not enabled for the database. Read and write access to data and configuration is unrestricted\n   2024-07-05T07:18:03.738+00:00: /sys/kernel/mm/transparent_hugepage/enabled is 'always'. We suggest setting it to 'never' in this binary version\n   2024-07-05T07:18:03.738+00:00: vm.max_map_count is too low\n------\n\ntest&gt; show dbs\nadmin     40.00 KiB\nconfig    12.00 KiB\nlocal     40.00 KiB\ntodo-app  68.00 KiB\ntest&gt; use todo-app\nswitched to db todo-app\ntodo-app&gt; show collections\ntodos\nusers\ntodo-app&gt; db.todos.showDocuments()\nTypeError: db.todos.showDocuments is not a function\ntodo-app&gt; db.todos.countDocuments()\n1\ntodo-app&gt; db.todos.countDocuments()\n2\ntodo-app&gt; db.todos.countDocuments()\n3\ntodo-app&gt;\n</code></pre>"},{"location":"lab2/services/#verify-the-images-added-to-aws-s3","title":"Verify the images added to AWS S3","text":"<p>Open AWS Dashboard &gt; S3 service to see the list of images uploaded.</p> <p></p>"},{"location":"lab2/tech-stack/","title":"Tech stack","text":"<ul> <li>Frontend: React, Material UI.</li> <li>Backend: Node.js, Express</li> <li>Database: Mongo(Atlas or running locally for storing tasks)</li> <li>Object Storage: AWS S3(for storing images)</li> </ul>"},{"location":"lab3/overview/","title":"Overview","text":"<p>Container-supported development is the idea of using containers to support and enhance development without touching the main application runtime itself. </p>"},{"location":"lab3/overview/#what-it-means","title":"What it means?","text":"<p>The developer will run their application using a runtime installed natively on their machine (such as a JVM, Node engine, or Python interpreter). But, the external dependencies will run in containers. </p>"},{"location":"lab3/overview/#what-benefits-does-it-provide-to-the-developers","title":"What benefits does it provide to the developers?","text":"<p>Even though the developers didn\u2019t go \u201call-in\u201d, Docker still provided significant value by running dependent services out of containers, making it quick and easy to get started and ensure version consistency across the entire team.</p> <p>With Docker, teams can do things that might otherwise have been impossible. They can run local instances of cloud services, run real services in their tests, and more. There is no \u201cone right path\u201d for teams to leverage Docker. You see teams using wrapper scripts that run docker run commands, others using IDE plugins to launch declarative Compose stacks, or programmatic interactions using Testcontainers.In many of these cases, teams can leverage off-the-shelf (or very slightly customized versions of) images from our DOI/DVP catalog.</p>"},{"location":"lab3/overview/#choosing-the-container-supported-approach","title":"Choosing the container-supported approach:","text":"<ul> <li> <p>Separation of Concerns: Developers focus on the core application logic using their familiar runtime (JVM, Node, Python etc.), while external dependencies are isolated in containers.</p> </li> <li> <p>Improved Efficiency:</p> </li> <li> <p>Easier setup: Containers simplify dependency management, reducing time spent configuring environments. Version consistency: All developers use the same container image, ensuring consistent dependencies across the team.</p> </li> <li> <p>Enhanced Capabilities: Docker allows running local simulations of cloud services and real services within tests, providing a more realistic development environment.</p> </li> <li> <p>Flexibility in Implementation: There's no single approach. Teams can use:</p> </li> <li> <p>Wrapper scripts for simple container execution.</p> </li> <li>IDE plugins for launching development environments defined in Docker Compose files.</li> <li> <p>Programming interactions with libraries like Testcontainers.</p> </li> <li> <p>Leveraging Shared Resources: Teams can benefit from pre-built container images from public repositories like Docker Official Images (DOI) and Docker Verified Publishers (DVP).</p> </li> </ul> <p>Overall, container-supported development offers a way to streamline the development process by managing dependencies and enhancing development environments without completely switching to a containerized application runtime.</p>"},{"location":"lab3/services/","title":"How to build and test AWS Cloud applications with LocalStack and Docker","text":"<p>This repo contains the sample application for Building and testing Cloud applications with LocalStack and Docker guide on Docker Docs. This simple to-do List application allows developers to upload images to S3-emulated LocalStack.</p> <p>Notice: This sample repo is intended to support the guide mentioned above. As such, the application code is purposely kept simple to keep the focus on the guide's content and should not be considered production ready.</p>"},{"location":"lab3/services/#tech-stack","title":"Tech Stack","text":"<ul> <li>Frontend: React, Material UI.</li> <li>Backend: Node.js, Express</li> <li>Database: Mongo(running locally for storing tasks)</li> <li>Object Storage: LocalStack (for emulating S3 and storing images locally for testing purposes)</li> </ul>"},{"location":"lab3/services/#project-structure","title":"Project Structure","text":"<p>This project contains the following components:</p> <ul> <li>/backend - This directory contains the Node.js application that handles the server-side logic and interacts with the database. This directory contains configuration settings for uploading images to LocalStack (emulated AWS S3). The uploadConfig.js file is responsible for configuring the S3 client to connect to the LocalStack S3 endpoint. This allows the backend application to store and retrieve images associated with the Todo List items.</li> <li>/frontend - The frontend directory contains the React application that handles the user interface and interacts with the backend. </li> </ul>"},{"location":"lab3/services/#development","title":"Development","text":"<ol> <li>Install awscli-local tool</li> </ol> <pre><code>pip install awscli-local\n</code></pre> <p>In case, it throws error related to python. Follow the below steps:</p> <pre><code>python3 -m venv venv\nsource venv/bin/activate\npip install awscli-local\n</code></pre> <ol> <li>Clone the repository</li> </ol> <pre><code>git clone https://github.com/dockersamples/todo-list-localstack-docker\n</code></pre> <ol> <li>Navigate into the project.</li> </ol> <pre><code>cd todo-list-localstack-docker\n</code></pre>"},{"location":"lab3/services/#run-the-app-natively","title":"Run the app natively","text":""},{"location":"lab3/services/#bring-up-localstack","title":"Bring up LocalStack","text":"<p>To run the app natively, you will need to run LocalStack and Mongo using Docker Compose while running frontend and backend locally.</p> <pre><code>docker compose -f compose-native.yml up -d --build\n</code></pre> <p></p>"},{"location":"lab3/services/#verify-if-localstack-is-up-and-running","title":"Verify if LocalStack is up and running","text":""},{"location":"lab3/services/#add-a-sample-s3-bucket","title":"Add a Sample S3 Bucket","text":"<p>By using the AWS CLI with LocalStack, you can interact with the emulated services exactly as you would with real AWS services. This helps ensure that your application behaves the same way in a local environment as it would in a production environment on AWS.</p> <p>Let\u2019s create a new S3 bucket within the LocalStack environment:</p> <pre><code>awslocal s3 mb s3://mysamplebucket\n</code></pre> <p>The command <code>s3 mb s3://mysamplebucket</code> tells the AWS CLI to create a new S3 bucket (mb stands for \"make bucket\"). The bucket is named <code>mysamplebucket</code> It should show the following result:</p> <pre><code>make_bucket: mysamplebucket\n</code></pre>"},{"location":"lab3/services/#connecting-to-localstack-from-a-non-containerised-node-app","title":"Connecting to LocalStack from a non-containerised Node app","text":"<p>Now it\u2019s time to connect your app to LocalStack. The index.js file, located in the backend/ directory, serves as the main entry point for the backend application.</p> <p>The code interacts with LocalStack\u2019s S3 service, which is accessed via the endpoint defined by the <code>S3_ENDPOINT_URL</code> environment variable, typically set to <code>http://localhost:4556</code> for local development.  </p> <p>The <code>S3Client</code> from the AWS SDK is configured to use this LocalStack endpoint, along with test credentials (<code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code>) that are also sourced from environment variables. This setup allows the application to perform operations on the locally simulated S3 service as if it were interacting with the real AWS S3, making the code flexible for different environments.</p> <p>The code uses multer and multer-s3 to handle file uploads. When a user uploads an image through the <code>/upload</code> route, the file is stored directly in the S3 bucket simulated by LocalStack. The bucket name is retrieved from the environment variable <code>S3_BUCKET_NAME</code>. Each uploaded file is given a unique name by appending the current timestamp to the original file name. The route then returns the URL of the uploaded file within the local S3 service, making it accessible just as it would be if hosted on a real AWS S3 bucket</p>"},{"location":"lab3/services/#bring-up-backend","title":"Bring up Backend","text":"<pre><code>cd backend/\nnpm install\n\n</code></pre> <p>Please note that these are placeholders that LocalStack uses to simulate AWS credentials and not the real values. Hence, no changes are needed.</p> <pre><code>AWS_ACCESS_KEY_ID=test\nAWS_SECRET_ACCESS_KEY=test\nS3_BUCKET_NAME=mysamplebucket\nS3_ENDPOINT_URL=http://localhost:4566\nMONGODB_URI=mongodb://mongodb:27017/todos\nAWS_REGION=us-east-1\n</code></pre> <p>Start the backend server:</p> <pre><code>node index.js\n</code></pre> <p>You will see the message that the backend service has successfully started at port 5000.</p>"},{"location":"lab3/services/#start-the-frontend","title":"Start the frontend","text":"<p>Open a new terminal and run the following command:</p> <pre><code>cd frontend/\nnpm run dev\n</code></pre> <p>By now, you should see the following message</p> <pre><code>VITE v5.4.2  ready in 110 ms\n\n  \u279c  Local:   http://localhost:5173/\n  \u279c  Network: use --host to expose\n  \u279c  press h + enter to show help\n\n</code></pre>"},{"location":"lab3/services/#try-adding-a-task-and-uploading-the-image","title":"Try adding a task and uploading the image","text":"<p>It shows the image is successfully uploaded.</p>"},{"location":"lab3/services/#check-the-localstack-container-logs","title":"Check the LocalStack container logs","text":""},{"location":"lab3/services/#check-the-mongo-container-logs","title":"Check the Mongo container logs","text":"<pre><code># mongosh\nCurrent Mongosh Log ID: 66cb1093118d7d4cc1c76a8a\nConnecting to:          mongodb://127.0.0.1:27017/?directConnection=true&amp;serverSelectionTimeoutMS=2000&amp;appName=mongosh+2.3.0\nUsing MongoDB:          7.0.12\nUsing Mongosh:          2.3.0\n\nFor mongosh info see: https://www.mongodb.com/docs/mongodb-shell/\n\n\nTo help improve our products, anonymous usage data is collected and sent to MongoDB periodically (https://www.mongodb.com/legal/privacy-policy).\nYou can opt-out by running the disableTelemetry() command.\n\n------\n   The server generated these startup warnings when booting\n   2024-08-25T10:58:46.918+00:00: Using the XFS filesystem is strongly recommended with the WiredTiger storage engine. See http://dochub.mongodb.org/core/prodnotes-filesystem\n   2024-08-25T10:58:47.668+00:00: Access control is not enabled for the database. Read and write access to data and configuration is unrestricted\n   2024-08-25T10:58:47.668+00:00: /sys/kernel/mm/transparent_hugepage/enabled is 'always'. We suggest setting it to 'never' in this binary version\n   2024-08-25T10:58:47.668+00:00: vm.max_map_count is too low\n------\n\ntest&gt; show dbs\nadmin   40.00 KiB\nconfig  60.00 KiB\nlocal   40.00 KiB\ntodos    8.00 KiB\ntest&gt; use todos\nswitched to db todos\ntodos&gt; db.todos.countDocuments()\n2\ntodos&gt; db.todos.countDocuments()\n3\ntodos&gt; \n</code></pre>"},{"location":"lab3/services/#stop-the-container-services","title":"Stop the container services","text":"<pre><code>docker compose -f compose-native.yml down\n</code></pre>"},{"location":"lab3/services/#connecting-to-containerised-localstack-from-a-containerised-node-app","title":"Connecting to containerised LocalStack from a containerised Node app","text":"<pre><code>docker compose -f compose.yml up -d --build\n</code></pre>"},{"location":"lab3/services/#add-a-sample-s3-bucket_1","title":"Add a Sample S3 Bucket","text":"<p>Using the AWS CLI with LocalStack lets you interact with the emulated services exactly as you would with real AWS services. This helps ensure that your application behaves the same way in a local environment as it would in a production environment on AWS.</p> <p>Let\u2019s create a new S3 bucket within the LocalStack environment:</p> <pre><code>awslocal s3 mb s3://mysamplebucket\n</code></pre> <p>The command <code>s3 mb s3://mysamplebucket</code> tells the AWS CLI to create a new S3 bucket (mb stands for \"make bucket\"). The bucket is named <code>mysamplebucket</code> It should show the following result:</p> <pre><code>make_bucket: mysamplebucket\n</code></pre>"},{"location":"lab3/services/#try-adding-a-task-and-uploading-the-image_1","title":"Try adding a task and uploading the image","text":"<p>It shows the image is successfully uploaded.</p>"},{"location":"lab3/services/#check-the-localstack-container-logs_1","title":"Check the LocalStack container logs","text":""},{"location":"lab3/services/#check-the-mongo-container-logs_1","title":"Check the Mongo container logs","text":"<pre><code># mongosh\nCurrent Mongosh Log ID: 66cb1093118d7d4cc1c76a8a\nConnecting to:          mongodb://127.0.0.1:27017/?directConnection=true&amp;serverSelectionTimeoutMS=2000&amp;appName=mongosh+2.3.0\nUsing MongoDB:          7.0.12\nUsing Mongosh:          2.3.0\n\nFor mongosh info see: https://www.mongodb.com/docs/mongodb-shell/\n\n\nTo help improve our products, anonymous usage data is collected and sent to MongoDB periodically (https://www.mongodb.com/legal/privacy-policy).\nYou can opt-out by running the disableTelemetry() command.\n\n------\n   The server generated these startup warnings when booting\n   2024-08-25T10:58:46.918+00:00: Using the XFS filesystem is strongly recommended with the WiredTiger storage engine. See http://dochub.mongodb.org/core/prodnotes-filesystem\n   2024-08-25T10:58:47.668+00:00: Access control is not enabled for the database. Read and write access to data and configuration is unrestricted\n   2024-08-25T10:58:47.668+00:00: /sys/kernel/mm/transparent_hugepage/enabled is 'always'. We suggest setting it to 'never' in this binary version\n   2024-08-25T10:58:47.668+00:00: vm.max_map_count is too low\n------\n\ntest&gt; show dbs\nadmin   40.00 KiB\nconfig  60.00 KiB\nlocal   40.00 KiB\ntodos    8.00 KiB\ntest&gt; use todos\nswitched to db todos\ntodos&gt; db.todos.countDocuments()\n2\ntodos&gt; db.todos.countDocuments()\n3\ntodos&gt; \n</code></pre>"},{"location":"lab3/tech-stack/","title":"Tech stack","text":"<ul> <li>Frontend: React, Material UI.</li> <li>Backend: Node.js, Express</li> <li>Database: Mongo(running locally for storing tasks)</li> <li>Object Storage: Localstack (for emulating S3 and storing images locally for testing purpose)</li> </ul>"},{"location":"lab4/getting-started/","title":"Getting Started","text":""},{"location":"lab4/getting-started/#prereq","title":"Prereq","text":"<ul> <li>Install the latest version of Docker Desktop 4.42+</li> <li>Ensure that \u201cDocker Model Runner\u201d is enabled.</li> </ul> <p>There are two ways to enable Model Runner - either using CLI or using Docker Dashboard.</p>"},{"location":"lab4/getting-started/#using-cli","title":"Using CLI","text":"<pre><code>docker desktop enable model-runner\n</code></pre>"},{"location":"lab4/getting-started/#using-docker-dashboard","title":"Using Docker Dashboard","text":"<p>The \"Enable host-side TCP support\" feature allows Docker Model Runner to  additionally accept connections on the host OS on the specified TCP port (default: 12434) rather than only through the host Docker socket (/var/run/docker.sock). You can change this to another port if needed, particularly if 12434 is already in use by another application. We will see its usage later in the docs.</p> <p>Once you enable the option, select \u201cApply &amp; Restart\u201d.</p> <p>If you\u2019re not seeing the \u201cEnable Model Runner\u201d option, it is recommended to enable \u201cUse nightly builds\u201d option under Software Updates and try to see if the option is available. If still facing issue, reach out to Eva.</p> <p>Open up the terminal and you should be able to see docker model as the new CLI.</p> <pre><code>docker model --help\nUsage:  docker model COMMAND\n\nDocker Model Runner\n\nCommands:\n  inspect     Display detailed information on one model\n  list        List the available models that can be run with the Docker Model Runner\n  pull        Download a model\n  rm          Remove a model downloaded from Docker Hub\n  run         Run a model with the Docker Model Runner\n  status      Check if the Docker Model Runner is running\n  version     Show the Docker Model Runner version\n\nRun 'docker model COMMAND --help' for more information on a command.\n</code></pre>"},{"location":"lab4/getting-started/#check-if-the-model-runner-is-running-or-not","title":"Check if the Model Runner is running or not","text":"<pre><code>docker model status\nDocker Model Runner is running\n</code></pre>"},{"location":"lab4/getting-started/#list-the-available-models","title":"List the available models","text":"<pre><code>docker model ls\nMODEL  PARAMETERS  QUANTIZATION  ARCHITECTURE  FORMAT  MODEL ID  CREATED  SIZE\n</code></pre> <p>The response shows an empty list. Let\u2019s go ahead and download the model from the Docker Hub.</p>"},{"location":"lab4/getting-started/#download-a-model","title":"Download a model","text":"<pre><code>docker model pull ai/llama3.2:1B-Q8_0\n</code></pre> <p>All these models are hosted on https://hub.docker.com/u/ai: </p> <pre><code>ai/gemma3\nai/llama3.2\nai/qwq\nai/mistral-nemo\nai/mistral\nai/phi4\nai/qwen2.5\nai/deepseek-r1-distill-llama (distill means it\u2019s not the actual RL-ed deepseek, it\u2019s a llama trained on DeepSeek-R1 inputs/outputs) \n</code></pre> <p>More models will be coming in the future, we plan to add more popular ones first. </p>"},{"location":"lab4/getting-started/#list-the-model","title":"List the Model","text":"<pre><code>docker model ls\nMODEL                PARAMETERS  QUANTIZATION  ARCHITECTURE  MODEL ID      CREATED       SIZE\nai/llama3.2:1B-Q8_0  1.24 B      Q8_0          llama         a15c3117eeeb  20 hours ago  1.22 GiB\n</code></pre>"},{"location":"lab4/getting-started/#use-docker-model-run-to-send-a-single-message","title":"Use docker model run to send a single message","text":"<pre><code>docker model run ai/llama3.2:1B-Q8_0 \"Hi\"\nHello! How can I help you today?\n</code></pre>"},{"location":"lab4/getting-started/#run-the-model-in-interactive-mode","title":"Run the Model in interactive mode","text":"<pre><code>docker model run ai/llama3.2:1B-Q8_0\nInteractive chat mode started. Type '/bye' to exit.\n&gt; why is water blue?\nWater appears blue because ...\n</code></pre>"},{"location":"lab4/getting-started/#remove-the-model","title":"Remove the model","text":"<pre><code>docker model rm ai/llama3.2:1B-Q8_0\n</code></pre>"},{"location":"lab4/getting-started/#packaging-your-own-ai-model","title":"Packaging your own AI Model","text":""},{"location":"lab4/getting-started/#step-1-pull-the-model-from-hugging-face","title":"Step 1. Pull the model from Hugging Face","text":"<pre><code>docker model pull hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF\nDownloaded: 769.73 MB\nModel pulled successfully\n</code></pre>"},{"location":"lab4/getting-started/#step-2-setup-a-local-docker-registry","title":"Step 2. Setup a local Docker registry","text":"<pre><code>docker run -d -p 5000:5000 --name registry registry:2\n</code></pre>"},{"location":"lab4/getting-started/#step-3-tag-the-model-for-your-local-registry","title":"Step 3. Tag the model for your local registry","text":"<pre><code>docker model tag hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF localhost:5000/foobar\nModel \"hf.co/bartowski/llama-3.2-1b-instruct-gguf\" tagged successfully with \"localhost:5000/foobar:latest\"\n</code></pre>"},{"location":"lab4/getting-started/#step-4-push-the-model-to-your-local-registry","title":"Step 4. Push the model to your local registry","text":"<pre><code>docker model push localhost:5000/foobar\nUploaded: 770.00 MB\nModel pushed successfully\n</code></pre>"},{"location":"lab4/getting-started/#step-5-push-the-model-to-your-local-registry","title":"Step 5. Push the model to your local registry","text":"<pre><code>docker model push localhost:5000/foobar\nUploaded: 770.00 MB\nModel pushed successfully\n</code></pre>"},{"location":"lab4/getting-started/#step-6-list-the-available-models","title":"Step 6. List the available models","text":"<pre><code>docker model ls\nMODEL NAME                                  PARAMETERS  QUANTIZATION  ARCHITECTURE  MODEL ID      CREATED       SIZE\nai/llama3.2:1B-Q8_0                         1.24 B      Q8_0          llama         a15c3117eeeb  2 months ago  1.22 GiB\nhf.co/bartowski/llama-3.2-1b-instruct-gguf  1.24B                     llama         7ca6390d8288  8 months ago  808M\nlocalhost:5000/foobar:latest  \n</code></pre>"},{"location":"lab4/overview/","title":"Overview","text":""},{"location":"lab4/overview/#docker-model-runner","title":"Docker Model Runner","text":"<ul> <li>Docker Model Runner is a new experimental feature introduced in Docker Desktop for Mac 4.40+. </li> <li>It provides a Docker-native experience for running Large Language Models (LLMs) locally, seamlessly integrating with existing container tooling and workflows. </li> <li>The feature is specifically optimized to utilize Apple Silicon Mac's GPU resources for efficient model inference as of now. </li> </ul>"},{"location":"lab4/overview/#what-problem-does-it-solve","title":"What problem does it solve?","text":"<p>With the Model Runner feature, Docker provides inference capabilities to developers on their laptop, and in the future in CI, allowing them to run LLM models locally. This is an important feature to help developing GenAI applications. The runner essentially provides GPU-accelerated inference engines that are accessible both through the Docker socket (<code>/var/run/docker.sock</code>) and via a TCP connection at <code>model-runner.docker.internal:80</code>.</p>"},{"location":"lab4/overview/#new-docker-model-cli","title":"New <code>docker model</code> CLI","text":"<p>Docker Desktop 4.40+ introduces <code>docker model</code> CLI as the first class-citizen. This means AI models are now treated as fundamental, well-supported objects within the Docker CLI, similar to how Docker already treats containers, images, and volumes. </p> <pre><code>docker model --help\nUsage:  docker model COMMAND\n\nDocker Model Runner\n\nCommands:\n  inspect     Display detailed information on one model\n  list        List the available models that can be run with the Docker Model Runner\n  pull        Download a model\n  rm          Remove a model downloaded from Docker Hub\n  run         Run a model with the Docker Model Runner\n  status      Check if the Docker Model Runner is running\n  version     Show the Docker Model Runner version\n\nRun 'docker model COMMAND --help' for more information on a command.\n</code></pre> <p>By using this new CLI, developers can:</p> <ul> <li>Pull models from registries (e,g Docker Hub)</li> <li>Run models locally with GPU acceleration</li> <li>Integrate models into their development workflows</li> <li>Test GenAI applications during development without relying on external APIs</li> </ul> <p>This capability is particularly valuable for developing and testing GenAI applications locally before deployment, allowing for faster iteration cycles and reduced dependency on cloud services during development.</p>"},{"location":"lab4/overview/#architecture-of-docker-model-runner","title":"Architecture of Docker Model Runner","text":"<p>With Docker Model Runner, the AI model DOES NOT run in a container. Instead, Docker Model Runner uses a host-installed inference server (llama.cpp for now) that runs natively on your Mac rather than containerizing the model. We do plan to support additional inference (i.e MLX) in future releases.</p>"},{"location":"lab4/overview/#1-host-level-process","title":"1. Host-level process:","text":"<p>Docker Desktop runs llama.cpp directly on your host machine This allows direct access to the hardware GPU acceleration on Apple Silicon</p>"},{"location":"lab4/overview/#2-gpu-acceleration","title":"2. GPU acceleration:","text":"<p>By running directly on the host, the inference server can access Apple's Metal API This provides direct GPU acceleration without the overhead of containerization You can see the GPU usage in Activity Monitor when queries are being processed</p>"},{"location":"lab4/overview/#3-model-loading-process","title":"3. Model Loading Process:","text":"<p>When you run docker model pull, the model files are downloaded from the Docker Hub These models are cached locally on the host machine's storage  Models are dynamically loaded into memory by llama.cpp when needed</p> <p>Note: Unlike traditional Docker containers that package large AI models with &gt; the model runtime (which results in slow deployment), Model Runner separates &gt; the model from the runtime, allowing for faster deployment. </p> <p>Model Runner enables local LLM execution. It runs large language models  (LLMs) directly on your machine rather than sending data to external API  services. This means your data never leaves your infrastructure. All you need &gt; to do is pull the model from Docker Hub and start integrating it with your  application.</p>"},{"location":"lab4/projects/catalog-chatbot/","title":"Product Catalog Chatbot with AI-Enhanced Management System","text":"<p>A product catalog management platform powered by Docker Model Runner. This system combines conversational AI and real-time processing for comprehensive catalog management.</p>"},{"location":"lab4/projects/catalog-chatbot/#system-overview","title":"\ud83c\udfaf System Overview","text":"<p>This is a complete AI-enhanced catalog management system featuring:</p>"},{"location":"lab4/projects/catalog-chatbot/#core-ai-components","title":"\ud83e\udd16 Core AI Components","text":"<ul> <li>Chatbot Interface - Natural language product queries and conversations</li> <li>Model Runner Integration - Local AI model execution with Llama 3.2</li> </ul>"},{"location":"lab4/projects/catalog-chatbot/#complete-architecture","title":"\ud83c\udfd7\ufe0f Complete Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Frontend      \u2502  \u2502  Agent Portal   \u2502  \u2502  Chatbot UI     \u2502\n\u2502   Port: 5173    \u2502  \u2502   Port: 3001    \u2502  \u2502   Port: 5174    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        \u2502                     \u2502                     \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                   \u2502                    \u2502\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502   Backend API   \u2502  \u2502 Agent Service   \u2502  \u2502 Chatbot API     \u2502\n        \u2502   Port: 3000    \u2502  \u2502  Port: 7777     \u2502  \u2502  Port: 8082     \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                   \u2502                    \u2502                    \u2502\n                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502                   \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502  MCP Gateway    \u2502  \u2502  Model Runner   \u2502\n                    \u2502  Port: 8811     \u2502  \u2502  (Local AI)     \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n              \u2502               \u2502               \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502   PostgreSQL    \u2502 \u2502     MongoDB     \u2502 \u2502     Kafka       \u2502\n    \u2502   Port: 5432    \u2502 \u2502   Port: 27017   \u2502 \u2502  Port: 9092     \u2502\n    \u2502 (Products DB)   \u2502 \u2502 (Agent History) \u2502 \u2502 (Event Stream)  \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"lab4/projects/catalog-chatbot/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"lab4/projects/catalog-chatbot/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker Desktop with Model Runner enabled</li> <li>At least 8GB RAM (4GB+ for AI models)</li> <li>The latest version of Docker Compose</li> </ul>"},{"location":"lab4/projects/catalog-chatbot/#1-pull-required-models","title":"1. Pull Required Models","text":"<pre><code># Pull the AI model for chatbot and agents\ndocker model pull ai/llama3.2:1B-Q8_0\n</code></pre>"},{"location":"lab4/projects/catalog-chatbot/#2-start-all-services","title":"2. Start All Services","text":"<pre><code># Clone the repository\ngit clone https://github.com/ajeetraina/catalog-service-node-chatbot.git\ncd catalog-service-node-chatbot\n\n# Start the complete system\ndocker compose up -d --build\n</code></pre>"},{"location":"lab4/projects/catalog-chatbot/#3-access-the-applications","title":"3. Access the Applications","text":"Service URL Description \ud83e\udd16 Chatbot Interface http://localhost:5174 Main chatbot for product queries \ud83c\udfe0 Main Frontend http://localhost:5173 Product catalog management \ud83d\udcca Kafka UI http://localhost:8080 Event streaming monitoring \ud83d\uddc4\ufe0f pgAdmin http://localhost:5050 Database administration"},{"location":"lab4/projects/catalog-chatbot/#chatbot-capabilities","title":"\ud83d\udcac Chatbot Capabilities","text":"<p>Note: After running <code>sh add-products.sh</code>, your chatbot will have access to 50+ sample products for testing these capabilities.</p>"},{"location":"lab4/projects/catalog-chatbot/#natural-language-product-search","title":"Natural Language Product Search","text":"<pre><code>User: \"Show me all electronics under $500\"\nChatbot: \"I found 8 electronics products under $500:\n\n\ud83d\udcf1 iPhone SE - $399.00\n   Compact smartphone with A15 Bionic chip\n\n\ud83c\udfa7 Sony WH-1000XM5 - $399.00  \n   Industry-leading noise canceling headphones\n\n\u231a Apple Watch - $249.00\n   Advanced health and fitness tracking\n\n[Additional products...]\n\nWould you like details about any specific product?\"\n</code></pre>"},{"location":"lab4/projects/catalog-chatbot/#intelligent-category-browsing","title":"Intelligent Category Browsing","text":"<pre><code>User: \"What's popular in home electronics?\"\nChatbot: \"Based on our catalog data:\n\n\ud83c\udfe0 Smart Home Category (12 products):\n\u2022 Smart speakers: 4 products ($49-199)\n\u2022 Security cameras: 3 products ($89-299) \n\u2022 Smart thermostats: 2 products ($149-249)\n\u2022 Home automation hubs: 3 products ($99-349)\n\nThe Amazon Echo Dot and Ring Doorbell are our top sellers.\nWhich type of smart home device interests you?\"\n</code></pre>"},{"location":"lab4/projects/catalog-chatbot/#catalog-analytics","title":"Catalog Analytics","text":"<pre><code>User: \"Give me a summary of the catalog\"\nChatbot: \"Here's your catalog overview:\n\n\ud83d\udcca **Catalog Statistics:**\n\u2022 Total Products: 127\n\u2022 Active Categories: 8\n\u2022 Price Range: $9.99 - $2,499.00\n\u2022 Average Price: $186.45\n\u2022 New This Week: 5 products\n\n\ud83d\udd25 **Top Categories:**\n1. Electronics (34 products)\n2. Clothing (28 products)  \n3. Home &amp; Garden (21 products)\n\nIs there a specific category you'd like to explore?\"\n</code></pre>"},{"location":"lab4/projects/catalog-chatbot/#testing-the-chatbot","title":"Testing the Chatbot","text":"<pre><code># Test natural language queries\ncurl -X POST http://localhost:8082/api/chat \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"message\": \"What are your most expensive electronics?\"}'\n\n\n# Test category lookup\ncurl http://localhost:8082/api/categories\n</code></pre>"},{"location":"lab4/projects/catalog-chatbot/#adding-products-data","title":"\ud83d\ude80 Adding Products &amp; Data","text":""},{"location":"lab4/projects/catalog-chatbot/#automated-product-import","title":"Automated Product Import","text":"<pre><code># Use the provided import script\n./add-products.sh\n\n# Or use the Node.js automation script\nnpm run import-products\n</code></pre>"},{"location":"lab4/projects/catalog-chatbot/#testing-with-sample-data","title":"Testing with Sample Data","text":"<p>The system includes comprehensive sample data:</p> <ul> <li>50+ Products across multiple categories</li> <li>Vendor Information with AI evaluations</li> <li>Mock Market Data for agent testing</li> <li>Customer Interaction Patterns for recommendation testing</li> </ul>"},{"location":"lab4/projects/genai-chatbot/","title":"GenAI Chatbot","text":"<p>A modern, full-stack chat application demonstrating how to integrate React frontend with a Go backend and run local Large Language Models (LLMs) using Docker's Model Runner.</p> <p>This repo also integrates the GenAI app with the Observability stack that includes Prometheus, Grafana and Jaeger.</p>"},{"location":"lab4/projects/genai-chatbot/#overview","title":"Overview","text":"<p>This project showcases a complete Generative AI interface that includes:</p> <ul> <li>React/TypeScript frontend with a responsive chat UI</li> <li>Go backend server for API handling</li> <li>Integration with Docker's Model Runner to run Llama 3.2 locally</li> <li>Comprehensive observability with metrics, logging, and tracing</li> <li>llama.cpp metrics integration directly in the UI</li> </ul>"},{"location":"lab4/projects/genai-chatbot/#architecture","title":"Architecture","text":"<p>The application consists of these main components:</p> <p></p>"},{"location":"lab4/projects/genai-chatbot/#prerequisites","title":"Prerequisites","text":"<p>Before we begin, make sure you have:</p> <ul> <li>Docker Desktop (version 4.40 or newer) </li> <li>Docker Model Runner enabled</li> <li>At least 16GB of RAM for running AI models efficiently</li> <li>Familiarity with Go (for backend development)</li> <li>Familiarity with React and TypeScript (for frontend development)</li> </ul> <pre><code>services:\n  chat-app:\n    image: my-chat-app\n    models:\n      - llm\n\nmodels:\n  llm:\n    model: ai/smollm2\n</code></pre> <p>Docker Model Runner can be integrated with Docker Compose to run AI models as part of your multi-container applications. This lets you define and run AI-powered applications alongside your other services.</p> <p>Compose introduces a new service type called provider that allows you to declare platform capabilities required by your application. For AI models, you can use the model type to declare model dependencies. Here's an example of how to define a model provider:</p> <pre><code>services:\n  chat:\n    image: my-chat-app\n    depends_on:\n      - ai_runner\n\n  ai_runner:\n    provider:\n      type: model\n      options:\n        model: ai/smollm2\n</code></pre> <p>Notice the dedicated provider attribute in the ai_runner service. This attribute specifies that the service is a model provider and lets you define options such as the name of the model to be used. There is also a depends_on attribute in the chat service. This attribute specifies that the chat service depends on the ai_runner service. This means that the ai_runner service will be started before the chat service to allow injection of model information to the chat service.</p>"},{"location":"lab4/projects/genai-chatbot/#docker-compose-support-for-model-runner","title":"Docker Compose Support for Model Runner","text":"<p>Compose allows you to use <code>models</code> as a top-level element in the following way:</p>"},{"location":"lab4/projects/genai-chatbot/#clone-the-repository","title":"Clone the repository","text":"<pre><code>git clone https://github.com/dockersamples/genai-model-runner-metrics\ncd genai-model-runner-metrics\n</code></pre>"},{"location":"lab4/projects/genai-chatbot/#enable-docker-model-runner-in-docker-desktop","title":"Enable Docker Model Runner in Docker Desktop","text":"<p>Go to Settings &gt; Features in Development &gt; Beta tab Enable \"Docker Model Runner\" Select \u201cApply and restart\u201d</p> <p></p>"},{"location":"lab4/projects/genai-chatbot/#download-the-model","title":"Download the model","text":"<pre><code>docker model pull ai/llama3.2:1B-Q8_0\n</code></pre>"},{"location":"lab4/projects/genai-chatbot/#verify-the-backendenv","title":"Verify the backend.env","text":"<pre><code>BASE_URL=http://localhost:12434/engines/llama.cpp/v1/\nMODEL=ai/llama3.2:1B-Q8_0\nAPI_KEY=${API_KEY:-dockermodelrunner}\n\n# Observability configuration\nLOG_LEVEL=info\nLOG_PRETTY=true\nTRACING_ENABLED=true\nOTLP_ENDPOINT=jaeger:4318\n</code></pre>"},{"location":"lab4/projects/genai-chatbot/#start-the-application-using-docker-compose","title":"Start the application using Docker Compose","text":"<pre><code>docker compose up -d --build\n</code></pre> <p>You can access the frontend at http://localhost:3000</p> <p></p> <p>To access Grafana, use the following address: http://localhost:3001 (admin/admin)</p> <p></p> <p>Ensure that you provide <code>http://prometheus:9090</code> instead of <code>localhost:9090</code> to see the metrics on the Grafana dashboard.</p> <ul> <li>Jaeger UI: http://localhost:16686</li> <li>Prometheus: http://localhost:9091</li> </ul> <p></p>"},{"location":"lab4/projects/genai-chatbot/#how-it-works","title":"How It Works","text":"<ol> <li>The frontend sends chat messages to the backend API</li> <li>The backend formats the messages and sends them to the Model Runner</li> <li>The LLM processes the input and generates a response</li> <li>The backend streams the tokens back to the frontend as they're generated</li> <li>The frontend displays the incoming tokens in real-time</li> <li>Observability components collect metrics, logs, and traces throughout the process</li> </ol>"},{"location":"lab5/getting-started/","title":"Getting Started","text":""},{"location":"lab5/getting-started/#prereq","title":"Prereq","text":"<ul> <li>Ensure that you have Docker Desktop 4.42+ installed on your system.</li> <li>Enable MCP Toolkit in Docker Desktop</li> </ul>"},{"location":"lab5/overview/","title":"Overview","text":"<p>MCP refers to Model Context Protocol. It is an open protocol that standardizes how applications provide context to LLMs. </p> <p>Think of MCP like a USB-C port for AI applications. Just as USB-C provides a standardized way to connect your devices to various peripherals and accessories, MCP provides a standardized way to connect AI models to different data sources and tools.</p>"},{"location":"lab5/overview/#a-typical-mcp-workflow","title":"A Typical MCP Workflow","text":""},{"location":"lab5/overview/#user-interaction","title":"User Interaction","text":"<ul> <li>User submits a prompt (e.g., \"Fetch me list of users in the Slack\")</li> <li>Users interact through various interfaces/applications connected to MCP</li> </ul>"},{"location":"lab5/overview/#mcp-hosts","title":"MCP Hosts","text":"<ul> <li>Acts as central orchestration layer for the collaboration platform</li> <li>Contains MCP Clients with different applications (Claude, Cursor, etc.)</li> <li>Handles user interactions and connects to backend services</li> </ul>"},{"location":"lab5/overview/#client-server-communication-layer","title":"Client-Server Communication Layer","text":"<ul> <li>Initial Request: Client sends user's prompt to appropriate servers</li> <li>Initial Response: Servers provide initial processing results</li> <li>Notification: Ongoing updates as processing continues (enables feedback loops)</li> </ul>"},{"location":"lab5/overview/#tool-selection","title":"Tool Selection","text":"<ul> <li>Determines which tools are appropriate for fulfilling user requests</li> <li>Bridges client and server sides by matching user needs with available capabilities</li> </ul>"},{"location":"lab5/overview/#mcp-servers","title":"MCP Servers","text":"<ul> <li>Backend services that process requests, including:</li> <li>Kubernetes (container orchestration)</li> <li>Slack (communication)</li> <li>GitHub (code and version control)</li> <li>PostgreSQL (database operations)</li> <li>Grafana (monitoring and analytics)</li> </ul>"},{"location":"lab5/overview/#capabilities-branches","title":"Capabilities Branches","text":"<ul> <li>Tools: Specific utilities that servers employ to fulfill requests</li> <li>Resources: Assets and data available to the system</li> <li>Prompts: Template instructions for guiding LLM interactions</li> </ul>"},{"location":"lab5/overview/#data-sources","title":"Data Sources","text":"<ul> <li>Database: Structured data storage</li> <li>Web Server: Internet-accessible resources</li> <li>Local Files: Filesystem resources</li> </ul>"},{"location":"lab5/overview/#additional-components","title":"Additional Components","text":"<ul> <li>Notification: Alert system that ensures prompt delivery of results when operations complete</li> <li>Sampling: Process for selecting representative data from large datasets to balance accuracy with computational efficiency</li> </ul>"},{"location":"lab5/overview/#challenges-with-the-mcp-landscape","title":"Challenges with the MCP Landscape","text":"<p>However, the current MCP experience faces several challenges:</p> <ul> <li>Fragmented Discovery: Developers struggle to find MCP servers across registries, community lists, and blog posts\u2014with no way to verify which are official and trustworthy.</li> <li>Complex Setup: Getting started requires cloning repositories, managing conflicting dependencies, and self-hosting services that often aren't containerized, hindering portability.</li> <li>Security Concerns: Many MCP tools run with full host access (via npx or uvx) without isolation or sandboxing. Credentials are commonly passed as plain text environment variables, exposing sensitive data.</li> <li>Lack of Enterprise Readiness: Current tools often miss critical features like policy enforcement, audit logs, and standardized security practices.</li> </ul>"},{"location":"lab5/overview/#how-docker-solves-these-challenges","title":"How Docker solves these challenges","text":"<p>Docker is addressing these challenges with two complementary offerings:</p>"},{"location":"lab5/overview/#1-docker-mcp-catalog","title":"1. Docker MCP Catalog:","text":"<p>A trusted hub for discovering and accessing verified MCP servers, seamlessly integrated into Docker Hub with over 100 verified tools at launch.</p> <p>URL: https://hub.docker.com/catalogs/mcp </p> <p></p>"},{"location":"lab5/overview/#2-docker-mcp-toolkit","title":"2. Docker MCP Toolkit:","text":"<p>A suite of tools and services that make MCP servers secure, seamless, and instantly usable on your local machine or anywhere Docker runs.</p> <p>URL: https://open.docker.com/extensions/marketplace?extensionId=docker/labs-ai-tools-for-devs</p>"},{"location":"lab5/overview/#docker-as-a-mcp-runtime","title":"Docker as a MCP Runtime","text":"<p>Every time a new MCP server is added, a config file needs to be updated and the MCP client needs to be updated. The current workaround is to develop MCP servers which configure new MCP servers, but even this requires reloading. </p> <p></p> <p>A much better approach is to simply use one MCP server: Docker. This MCP server acts as a gateway into a dynamic set of containerized tools.</p> <p></p>"},{"location":"lab5/projects/Docker-CLI-With-Gordon/","title":"Docker MCP Server and Gordon","text":""},{"location":"lab5/projects/Docker-CLI-With-Gordon/#prerequisites","title":"Prerequisites","text":"<p>Before we start, make sure you have:</p> <ul> <li>Docker Desktop 4.42.0+ with the MCP Toolkit Extension installed</li> <li>Ensure that Docker AI is enabled under Docker Dashboard</li> </ul>"},{"location":"lab5/projects/Docker-CLI-With-Gordon/#step-1-select-docker-cli-under-mcp-server","title":"Step 1. Select Docker CLI under MCP Server","text":""},{"location":"lab5/projects/Docker-CLI-With-Gordon/#step-2-select-gordon-under-mcp-client","title":"Step 2. Select Gordon under MCP Client","text":""},{"location":"lab5/projects/Docker-CLI-With-Gordon/#step-3-ensure-that-docker-cli-is-enabled-under-ask-gordon","title":"Step 3. Ensure that Docker CLI is enabled under Ask Gordon","text":""},{"location":"lab5/projects/Docker-CLI-With-Gordon/#prompt-1","title":"Prompt 1:","text":"<p>\"List out all the containers running on my system\"</p> <p></p>"},{"location":"lab5/projects/Docker-CLI-With-Gordon/#prompt-2","title":"Prompt 2:","text":"<p>\"Create a new redis container with the name 'myredis' and run it on port 6379\"</p> <p></p>"},{"location":"lab5/projects/Docker-CLI-With-VSCode/","title":"Docker MCP Server and VS Code","text":""},{"location":"lab5/projects/Docker-CLI-With-VSCode/#prerequisites","title":"Prerequisites","text":"<p>Before we start, make sure you have:</p> <ul> <li>Docker Desktop 4.41.0+ with the MCP Toolkit Extension installed</li> <li>Node.js (v18 or later) for running the frontend</li> <li>VS Code (or any IDE of your choice)</li> </ul>"},{"location":"lab5/projects/Docker-CLI-With-VSCode/#setting-up-the-sample-database","title":"Setting Up the Sample Database","text":"<p>Instead of using an empty Postgres database, let's use a real example with actual data. We'll use a sample product catalog service:</p>"},{"location":"lab5/projects/Docker-CLI-With-VSCode/#step-1-clone-the-sample-catalog-service","title":"Step 1. Clone the sample catalog service","text":"<pre><code>git clone https://github.com/ajeetraina/catalog-service-node\ncd catalog-service-node\n</code></pre>"},{"location":"lab5/projects/Docker-CLI-With-VSCode/#step-2-start-the-backend-services-includes-postgres-with-sample-data","title":"Step 2. Start the backend services (includes Postgres with sample data)","text":"<pre><code>docker compose up -d --build\n</code></pre> <p>This will spin up:</p> <ul> <li>A Postgres database on port 5432 with sample catalog data</li> <li>A Node.js backend service</li> <li>Sample data including products, categories, and inventory</li> </ul> <p>Now let's bring up the frontend to see what data we're working with:</p>"},{"location":"lab5/projects/Docker-CLI-With-VSCode/#step-3-install-frontend-dependencies","title":"Step 3. Install frontend dependencies","text":"<pre><code>npm install\n</code></pre>"},{"location":"lab5/projects/Docker-CLI-With-VSCode/#step-4-start-the-development-server","title":"Step 4. Start the development server","text":"<pre><code>npm run dev\n</code></pre> <p>Open your browser to `http://localhost:5173 to see the catalog application. This gives you a visual understanding of the data structure we'll be querying with Claude.</p> <p>Hit \"Create Product\" button and start adding the new items to your Product catalog system.</p> <p>Perfect! Now we have a realistic database to work with instead of an empty one.</p>"},{"location":"lab5/projects/Docker-CLI-With-VSCode/#step-5-setting-up-mcp-toolkit","title":"Step 5. Setting up MCP Toolkit","text":"<p>Open Docker Desktop and navigate to the MCP Toolkit section in the sidebar.</p> <p>Enable Docker MCP Server</p> <p></p>"},{"location":"lab5/projects/Docker-CLI-With-VSCode/#step-6-configuring-the-vs-code","title":"Step 6. Configuring the VS Code","text":"<p>Open your VS Code and install the MCP Toolkit extension if you haven't already.</p>"},{"location":"lab5/projects/Docker-CLI-With-VSCode/#step-7-add-mcp-server","title":"Step 7. Add MCP Server","text":""},{"location":"lab5/projects/Docker-CLI-With-VSCode/#step-8-using-github-co-pilot","title":"Step 8. Using GitHub Co-Pilot","text":"<p>It's time to use GitHub Co-Pilot to interact with the Docker CLI MCP server. Select Agent under Co-Pilot and select tools that are available to chat.</p> <p></p>"},{"location":"lab5/projects/Docker-CLI-With-VSCode/#step-8-chatting-with-github-co-pilot","title":"Step 8. Chatting with GitHub Co-Pilot","text":"<p>Prompt: \"list out all the containers running on my Docker Desktop\"</p> <p></p>"},{"location":"lab5/projects/Docker-CLI-With-VSCode/#troubleshooting","title":"Troubleshooting:","text":"<p>In case you face the following issuse:</p> <pre><code>Reason: You may not include more than 128 tools in your request.\n</code></pre> <p>The fix is to reduce the number of tools in your request to 128 or fewer.  You can do this by selecting only the necessary tools you want to use with GitHub Co-Pilot.</p>"},{"location":"lab5/projects/GitHub-Claude/","title":"GitHub MCP Server and Claude Desktop","text":"<p>Let\u2019s see how to use GitHub MCP Server using Claude Desktop in the following steps:</p>"},{"location":"lab5/projects/GitHub-Claude/#prerequisite","title":"Prerequisite","text":"<ul> <li>Docker Desktop</li> <li>MCP Toolkit Enabled</li> <li>GitHub Account</li> <li>Claude Desktop installed on your system</li> </ul>"},{"location":"lab5/projects/GitHub-Claude/#step-1-create-a-github-personal-access-token-pat","title":"Step 1: Create a GitHub Personal Access Token (PAT)","text":"<ul> <li>Go to GitHub.com and sign in to your account</li> <li>Click your profile picture in the top-right corner</li> <li>Select \"Settings\"</li> <li>Scroll down to \"Developer settings\" in the left sidebar</li> <li>Click on \"Personal access tokens\" \u2192 \"Tokens (classic)\"</li> <li>Click \"Generate new token\" \u2192 \"Generate new token (classic)\"</li> <li>Give your token a descriptive name like \"Docker MCP GitHub Access\"</li> <li>Select the following scopes (permissions):</li> <li>repo (Full control of private repositories)</li> <li>workflow (if you need workflow actions)</li> <li>read:org (if you need organization access)</li> <li>Click \"Generate token\"</li> </ul>"},{"location":"lab5/projects/GitHub-Claude/#step-2-configure-the-github-mcp-server-in-docker","title":"Step 2: Configure the GitHub MCP Server in Docker","text":"<ul> <li>Open Docker Desktop</li> <li>Navigate to the MCP Server</li> <li>Find the GitHub (official) card and click on it to expand details.</li> </ul> <p>Alternatively, you can even set it up using <code>docker mcp</code> CLI as shown below: </p> <p>In your terminal, set up the GitHub token as a secret:</p> <pre><code>docker mcp secret set GITHUB.PERSONAL_ACCESS_TOKEN=github_pat_YOUR_TOKEN_HERE\n</code></pre> <p>For example:</p> <pre><code>docker mcp secret set GITHUB.PERSONAL_ACCESS_TOKEN=github_pat_XXXXMRCAXXXXXXxEp_QRZW43Wo1k6KYWwDXXXXXXXXGPXLZ7EGEnse82YM\nInfo: No policy specified, using default policy\n</code></pre>"},{"location":"lab5/projects/GitHub-Claude/#step-3-configure-claude-desktop","title":"Step 3. Configure Claude Desktop","text":"<p>Open MCP Client &gt; Click \"Connect\"</p> <p></p>"},{"location":"lab5/projects/GitHub-Claude/#step-4-restart-the-claude-desktop","title":"Step 4. Restart the Claude Desktop","text":"<pre><code>{\n  \"mcpServers\":{\n       \"MCP_DOCKER\":\n          {\n            \"command\":\n                \"docker\",\n                     \"args\":[\"mcp\",\"gateway\",\"run\"]\n       }\n   }\n}\n</code></pre>"},{"location":"lab5/projects/GitHub-Claude/#step-5-verify-if-mcp_docker-gets-added-to-the-claude-desktop","title":"Step 5. Verify if MCP_DOCKER gets added to the Claude Desktop","text":""},{"location":"lab5/projects/GitHub-Claude/#step-6-start-chatting-with-your-github-repository","title":"Step 6. Start chatting with your GitHub repository","text":""},{"location":"lab5/projects/GitHub-MCP-Gordon/","title":"GitHub MCP Server and Gordon","text":"<p>Let\u2019s see how to use GitHub MCP Server using Gordon as well as Claude Desktop in the following steps:</p>"},{"location":"lab5/projects/GitHub-MCP-Gordon/#step-1-create-a-github-personal-access-token-pat","title":"Step 1: Create a GitHub Personal Access Token (PAT)","text":"<ul> <li>Go to GitHub.com and sign in to your account</li> <li>Click your profile picture in the top-right corner</li> <li>Select \"Settings\"</li> <li>Scroll down to \"Developer settings\" in the left sidebar</li> <li>Click on \"Personal access tokens\" \u2192 \"Tokens (classic)\"</li> <li>Click \"Generate new token\" \u2192 \"Generate new token (classic)\"</li> <li>Give your token a descriptive name like \"Docker MCP GitHub Access\"</li> <li>Select the following scopes (permissions):</li> <li>repo (Full control of private repositories)</li> <li>workflow (if you need workflow actions)</li> <li>read:org (if you need organization access)</li> <li>Click \"Generate token\"</li> </ul>"},{"location":"lab5/projects/GitHub-MCP-Gordon/#step-2-configure-the-github-mcp-server-in-docker","title":"Step 2: Configure the GitHub MCP Server in Docker","text":"<ul> <li>Open Docker Desktop</li> <li>Navigate to the MCP Server</li> <li>Find the GitHub tool (official) card and click on it to expand details.</li> </ul> <p>In your terminal, set up the GitHub token as a secret:</p> <pre><code>docker mcp secret set GITHUB.PERSONAL_ACCESS_TOKEN=github_pat_YOUR_TOKEN_HERE\n</code></pre> <p>For example:</p> <pre><code>docker mcp secret set GITHUB.PERSONAL_ACCESS_TOKEN=github_pat_11AACMRCAXXXXXXxEp_QRZW43Wo1k6KYWwDXXXXXXXXGPXLZ7EGEnse82YM\nInfo: No policy specified, using default policy\n</code></pre> <p>If you have enabled Ask Gordon and enabled MCP Catalog (as shown in the following screenshot), then you can use docker ai command to play around with your GitHub repository.</p> <p></p> <p>Run the following docker ai command to create a new repository on your GitHub repo directly.</p> <pre><code>docker ai \"create a repo called modelorbital on my github repo\"\n\n    \u2022 Calling create_repository...\n\nThe repository \"modelorbital\" has been created on your GitHub account. You can access it here: https://github.com/username/modelorbital\n\nLet me know if you need help with anything else!\n</code></pre> <p>This looks great! We've successfully used Docker AI to create a new GitHub repository called \"modelorbital\" on your GitHub account.</p>"},{"location":"lab5/projects/Kubernetes-MCP/","title":"Kubernetes MCP Server and Claude","text":"<p>Imagine managing the Kubernetes clusters using simple natural language commands instead of memorizing dozens of kubectl incantations.  The Docker MCP (Model Context Protocol) Toolkit represents Docker's vision for a more integrated and accessible developer experience. It provides a comprehensive solution for managing Kubernetes through AI assistants with its Kubernetes MCP Server, which can be deployed in just 5 minutes.</p> <p>Before we begin, make sure you have the following requirements in place: Docker Desktop installed and running (the latest version is recommended)</p>"},{"location":"lab5/projects/Kubernetes-MCP/#step-1-enable-kubernetes-in-docker-desktop","title":"Step 1. Enable Kubernetes in Docker Desktop","text":""},{"location":"lab5/projects/Kubernetes-MCP/#step-2-setup-a-3-node-kind-cluster","title":"Step 2. Setup a 3-node Kind cluster","text":"<p>Select \u201cKind\u201d to set up a 3-node Kind cluster on the Docker Desktop.</p> <p></p>"},{"location":"lab5/projects/Kubernetes-MCP/#step-3-enable-kubernetes-mcp-server","title":"Step 3. Enable Kubernetes MCP Server","text":""},{"location":"lab5/projects/Kubernetes-MCP/#step-4-view-the-kubernetes-mcp-tools","title":"Step 4. View the Kubernetes MCP Tools","text":""},{"location":"lab5/projects/Kubernetes-MCP/#step-5-configure-the-claude-desktop","title":"Step 5. Configure the Claude Desktop","text":""},{"location":"lab5/projects/Kubernetes-MCP/#step-6-configure-mcp_docker-in-claude-desktop","title":"Step 6. Configure MCP_DOCKER in Claude Desktop","text":"<p>Open Claude Desktop and go to the \"Settings\" tab. You will see the following entry:</p> <p></p>"},{"location":"lab5/projects/Kubernetes-MCP/#step-7-verify-the-kubernetes-tools-under-claude-desktop","title":"Step 7. Verify the Kubernetes Tools under Claude Desktop","text":""},{"location":"lab5/projects/Kubernetes-MCP/#step-8-start-chatting-with-your-kubernetes-mcp-server","title":"Step 8. Start chatting with your Kubernetes MCP Server","text":""},{"location":"lab5/projects/Kubernetes-MCP/#prompt-1","title":"Prompt 1:","text":"<p>\"Create an Ngnix Pod in my Kubernetes cluster and list them in the tabular format\"</p> <p></p>"},{"location":"lab5/projects/Kubernetes-MCP/#available-kubernetes-management-tools","title":"Available Kubernetes Management Tools","text":"<p>The Kubernetes MCP server provides a comprehensive set of 40 tools for managing your Kubernetes resources. Here's a breakdown of the key tools available:</p>"},{"location":"lab5/projects/Kubernetes-MCP/#cluster-management-tools","title":"Cluster Management Tools","text":"<ul> <li>get_current_context: Get the current Kubernetes context</li> <li>list_contexts: List all available Kubernetes contexts</li> <li>set_current_context: Set the current Kubernetes context</li> <li>list_api_resources: List the API resources available in the cluster</li> </ul>"},{"location":"lab5/projects/Kubernetes-MCP/#resource-creation-tools","title":"Resource Creation Tools","text":"<ul> <li>create_namespace: Create a new Kubernetes namespace</li> <li>create_pod: Create a new Kubernetes pod</li> <li>create_deployment: Create a new Kubernetes deployment</li> <li>create_service: Create a new Kubernetes service</li> <li>create_configmap: Create a new Kubernetes ConfigMap</li> <li>create_cronjob: Create a new Kubernetes CronJob</li> </ul>"},{"location":"lab5/projects/Kubernetes-MCP/#resource-management-tools","title":"Resource Management Tools","text":"<ul> <li>list_namespaces: List all namespaces</li> <li>list_pods: List pods in a namespace</li> <li>list_deployments: List deployments in a namespace</li> <li>list_services: List services in a namespace</li> <li>list_nodes: List all nodes in the cluster</li> <li>list_cronjobs: List CronJobs in a namespace</li> <li>list_jobs: List Jobs in a namespace, optionally filtered by a CronJob parent</li> </ul>"},{"location":"lab5/projects/Kubernetes-MCP/#detailed-information-tools","title":"Detailed Information Tools","text":"<ul> <li>describe_pod: Describe a Kubernetes pod (read details like status, containers, etc.)</li> <li>describe_deployment: Get details about a Kubernetes deployment</li> <li>describe_service: Describe a Kubernetes service (read details like status, ports, selectors, etc.)</li> <li>describe_node: Describe a Kubernetes node (read details like status, capacity, conditions, etc.)</li> <li>describe_cronjob: Get detailed information about a Kubernetes CronJob including recent job history</li> <li>explain_resource: Get documentation for a Kubernetes resource or field</li> <li>get_events: Get Kubernetes events from the cluster</li> </ul>"},{"location":"lab5/projects/Kubernetes-MCP/#update-and-scale-tools","title":"Update and Scale Tools","text":"<ul> <li>scale_deployment: Scale a Kubernetes deployment</li> <li>update_deployment: Update an existing kubernetes deployment in cluster</li> <li>update_service: Update an existing kubernetes service in cluster</li> </ul>"},{"location":"lab5/projects/Kubernetes-MCP/#deletion-tools","title":"Deletion Tools","text":"<ul> <li>delete_pod: Delete a Kubernetes pod</li> <li>delete_deployment: Delete a Kubernetes deployment</li> <li>delete_service: Delete a Kubernetes service</li> <li>delete_namespace: Delete a Kubernetes namespace</li> <li>delete_cronjob: Delete a Kubernetes CronJob</li> <li>cleanup: Cleanup all managed resources</li> </ul>"},{"location":"lab5/projects/Kubernetes-MCP/#helm-chart-management","title":"Helm Chart Management","text":"<ul> <li>install_helm_chart: Install a Helm chart</li> <li>upgrade_helm_chart: Upgrade a Helm release</li> <li>uninstall_helm_chart: Uninstall a Helm release</li> </ul>"},{"location":"lab5/projects/Kubernetes-MCP/#debugging-tools","title":"Debugging Tools","text":"<ul> <li>get_logs: Get logs from pods, deployments, jobs, or resources matching a label selector</li> <li>get_job_logs: Get logs from Pods created by a specific Job</li> <li>port_forward: Forward a local port to a port on a Kubernetes resource</li> <li>stop_port_forward: Stop a port-forward process</li> </ul>"},{"location":"lab5/projects/Slack-MCP-With-Claude/","title":"Slack MCP Server and Claude","text":""},{"location":"lab5/projects/Slack-MCP-With-Claude/#step-1-create-a-slack-app-and-get-required-credentials","title":"Step 1. Create a Slack App and Get Required Credentials","text":"<p>First, you need to create a Slack app and get the necessary credentials:</p> <ul> <li>Go to the Slack API website and sign in to your Slack account</li> <li>Click \"Create New App\" and choose \"From scratch\"</li> <li>Name your app (e.g., \"Claude MCP Integration\") and select your workspace</li> <li>Once created, navigate to \"OAuth &amp; Permissions\" in the sidebar</li> <li>Under \"Bot Token Scopes\", add the following permissions:</li> </ul> <pre><code>channels:history\nchannels:read\nchat:write\nreactions:write\nusers:read\n</code></pre> <ul> <li>Scroll up and click \"Install to Workspace\"</li> <li>After installation, you'll receive a Bot User OAuth Token starting with \"xoxb-\"</li> </ul>"},{"location":"lab5/projects/Slack-MCP-With-Claude/#step-2-get-your-team-id-and-channel-id","title":"Step 2. Get Your Team ID and Channel ID","text":"<ul> <li>In your Slack workspace, right-click on your workspace name in the top-left and select \"Copy link\"</li> <li>The link will look like https://app.slack.com/client/T01ABCDEFG/... - the T01ABCDEFG part is your Team ID</li> <li>To get a Channel ID, right-click on a channel and select \"Copy link\"</li> <li>The link will look like https://your-workspace.slack.com/archives/C01ABCDEFG - the C01ABCDEFG part is your Channel ID</li> </ul> <p>By now, you must have the following details with you.</p> <pre><code>provider: slack\nteam_id: XXX\nchannel_id: XXX\nSlack token: xoxb-1XXX\n</code></pre>"},{"location":"lab5/projects/Slack-MCP-With-Claude/#step-3-configure-slack-mcp-server","title":"Step 3. Configure Slack MCP Server","text":"<p>Ensure that the MCP Toolkit is enabled in Docker Desktop.  If not, enable it from the Docker Desktop settings.</p> <p></p>"},{"location":"lab5/projects/Slack-MCP-With-Claude/#step-4-configure-the-slack-mcp-server","title":"Step 4. Configure the Slack MCP Server","text":""},{"location":"lab5/projects/Slack-MCP-With-Claude/#step-5-configure-claude-desktop-as-mcp-client","title":"Step 5. Configure Claude Desktop as MCP Client","text":""},{"location":"lab5/projects/Slack-MCP-With-Claude/#step-6-verify-if-mcp_docker-appear-under-claude-desktop","title":"Step 6. Verify if MCP_DOCKER appear under Claude Desktop","text":""},{"location":"lab5/projects/Slack-MCP-With-Claude/#step-7-pick-up-your-preferred-mcp-tools","title":"Step 7. Pick up your preferred MCP tools","text":""},{"location":"lab5/projects/Slack-MCP-With-Claude/#step-8-start-chatting-with-your-slack-mcp-server","title":"Step 8. Start chatting with your Slack MCP Server","text":""},{"location":"lab5/projects/postgres-mcp/","title":"Postgres mcp","text":""},{"location":"lab5/projects/postgres-mcp/#prerequisites","title":"Prerequisites","text":"<p>Before we start, make sure you have:</p> <ul> <li>Docker Desktop 4.41.0+ with the MCP Toolkit Extension installed</li> <li>Node.js (v18 or later) for running the frontend</li> <li>Claude Desktop installed</li> <li>Basic familiarity with Docker and JavaScript/TypeScript</li> <li>Basic SQL knowledge</li> </ul>"},{"location":"lab5/projects/postgres-mcp/#setting-up-the-sample-database","title":"Setting Up the Sample Database","text":"<p>Instead of using an empty Postgres database, let's use a real example with actual data.  We'll use a sample product catalog service:</p>"},{"location":"lab5/projects/postgres-mcp/#step-1-clone-the-sample-catalog-service","title":"Step 1. Clone the sample catalog service","text":"<pre><code>git clone https://github.com/ajeetraina/catalog-service-node\ncd catalog-service-node\n</code></pre>"},{"location":"lab5/projects/postgres-mcp/#step-2-start-the-backend-services-includes-postgres-with-sample-data","title":"Step 2. Start the backend services (includes Postgres with sample data)","text":"<pre><code>docker compose up -d --build\n</code></pre> <p>This will spin up:</p> <ul> <li>A Postgres database on port 5432 with sample catalog data</li> <li>A Node.js backend service</li> <li>Sample data including products, categories, and inventory</li> </ul> <p>Now let's bring up the frontend to see what data we're working with:</p>"},{"location":"lab5/projects/postgres-mcp/#step-3-install-frontend-dependencies","title":"Step 3. Install frontend dependencies","text":"<pre><code>npm install\n</code></pre>"},{"location":"lab5/projects/postgres-mcp/#step-4-start-the-development-server","title":"Step 4. Start the development server","text":"<pre><code>npm run dev\n</code></pre> <p>Open your browser to `http://localhost:5173 to see the catalog application.  This gives you a visual understanding of the data structure we'll be querying with Claude.</p> <p>Hit \"Create Product\" button and start adding the new items to your Product catalog system.</p> <p>Perfect! Now we have a realistic database to work with instead of an empty one.</p>"},{"location":"lab5/projects/postgres-mcp/#step-5-setting-up-mcp-toolkit","title":"Step 5. Setting up MCP Toolkit","text":"<p>Open Docker Desktop and navigate to the MCP Toolkit extension. Under \"MCP Server\", search for \"Postgres\" and select the Postgres MCP Server</p> <p></p>"},{"location":"lab5/projects/postgres-mcp/#step-6-configure-the-postgres-mcp-server","title":"Step 6. Configure the Postgres MCP Server","text":"<p>Click \"Configuration\" and add the right URL for your Postgres database:</p> <pre><code>postgresql://postgres:postgres@host.docker.internal:5432/catalog\n</code></pre> <p></p>"},{"location":"lab5/projects/postgres-mcp/#step-7-activate-the-postgres-mcp-server","title":"Step 7. Activate the Postgres MCP Server","text":"<p>Toggle the \"Enable\" switch to activate the Postgres MCP Server.</p>"},{"location":"lab5/projects/postgres-mcp/#step-8-configure-the-claude-desktop","title":"Step 8. Configure the Claude Desktop","text":""},{"location":"lab5/projects/postgres-mcp/#step-9-configure-mcp_docker-in-claude-desktop","title":"Step 9. Configure MCP_DOCKER in Claude Desktop","text":"<p>Open Claude Desktop and go to the \"Settings\" tab. You will see the following entry:</p> <ul> <li>Select Claude Settings</li> <li>Click on the Developer tab</li> <li>Click on the Edit Config button</li> <li>Add MCP_DOCKER to mcpServers section:</li> </ul> <pre><code>{\n  \"mcpServers\": {\n    \"MCP_DOCKER\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"alpine/socat\",\n        \"STDIO\",\n        \"TCP:host.docker.internal:8811\"\n      ]\n    }\n  }\n}\n</code></pre>"},{"location":"lab5/projects/postgres-mcp/#step-10-restart-the-claude-desktop","title":"Step 10. Restart the Claude Desktop","text":""},{"location":"lab5/projects/postgres-mcp/#step-11-start-chatting-with-the-postgres-database","title":"Step 11. Start chatting with the Postgres Database","text":"<p>Prompt: </p> <pre><code>List out all the products in the catalog\n</code></pre> <p></p>"},{"location":"lab5/projects/postgres-vscode/","title":"Postgres vscode","text":""},{"location":"lab5/projects/postgres-vscode/#prerequisites","title":"Prerequisites","text":"<p>Before we start, make sure you have:</p> <ul> <li>Docker Desktop 4.41.0+ with the MCP Toolkit Extension installed</li> <li>Node.js (v18 or later) for running the frontend</li> <li>Claude Desktop installed</li> <li>Basic familiarity with Docker and JavaScript/TypeScript</li> <li>Basic SQL knowledge</li> </ul>"},{"location":"lab5/projects/postgres-vscode/#setting-up-the-sample-database","title":"Setting Up the Sample Database","text":"<p>Instead of using an empty Postgres database, let's use a real example with actual data.  We'll use a sample product catalog service:</p>"},{"location":"lab5/projects/postgres-vscode/#step-1-clone-the-sample-catalog-service","title":"Step 1. Clone the sample catalog service","text":"<pre><code>git clone https://github.com/ajeetraina/catalog-service-node\ncd catalog-service-node\n</code></pre>"},{"location":"lab5/projects/postgres-vscode/#step-2-start-the-backend-services-includes-postgres-with-sample-data","title":"Step 2. Start the backend services (includes Postgres with sample data)","text":"<pre><code>docker compose up -d --build\n</code></pre> <p>This will spin up:</p> <ul> <li>A Postgres database on port 5432 with sample catalog data</li> <li>A Node.js backend service</li> <li>Sample data including products, categories, and inventory</li> </ul> <p>Now let's bring up the frontend to see what data we're working with:</p>"},{"location":"lab5/projects/postgres-vscode/#step-3-install-frontend-dependencies","title":"Step 3. Install frontend dependencies","text":"<pre><code>npm install\n</code></pre>"},{"location":"lab5/projects/postgres-vscode/#step-4-start-the-development-server","title":"Step 4. Start the development server","text":"<pre><code>npm run dev\n</code></pre> <p>Open your browser to `http://localhost:5173 to see the catalog application.  This gives you a visual understanding of the data structure we'll be querying with Claude.</p> <p>Hit \"Create Product\" button and start adding the new items to your Product catalog system.</p> <p>Perfect! Now we have a realistic database to work with instead of an empty one.</p>"},{"location":"lab5/projects/postgres-vscode/#step-5-setting-up-mcp-toolkit","title":"Step 5. Setting up MCP Toolkit","text":"<p>Open Docker Desktop and navigate to the MCP Toolkit extension. Under \"MCP Server\", search for \"Postgres\" and select the Postgres MCP Server</p> <p></p>"},{"location":"lab5/projects/postgres-vscode/#step-6-configure-the-postgres-mcp-server","title":"Step 6. Configure the Postgres MCP Server","text":"<p>Click \"Configuration\" and add the right URL for your Postgres database:</p> <pre><code>postgresql://postgres:postgres@host.docker.internal:5432/catalog\n</code></pre> <p></p>"},{"location":"lab5/projects/postgres-vscode/#step-7-activate-the-postgres-mcp-server","title":"Step 7. Activate the Postgres MCP Server","text":"<p>Toggle the \"Enable\" switch to activate the Postgres MCP Server.</p>"},{"location":"lab5/projects/postgres-vscode/#step-8-configure-the-claude-desktop","title":"Step 8. Configure the Claude Desktop","text":""},{"location":"lab5/projects/postgres-vscode/#step-9-configure-mcp_docker-in-claude-desktop","title":"Step 9. Configure MCP_DOCKER in Claude Desktop","text":"<p>Open Claude Desktop and go to the \"Settings\" tab. You will see the following entry:</p> <ul> <li>Select Claude Settings</li> <li>Click on the Developer tab</li> <li>Click on the Edit Config button</li> <li>Add MCP_DOCKER to mcpServers section:</li> </ul> <pre><code>{\n  \"mcpServers\": {\n    \"MCP_DOCKER\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"alpine/socat\",\n        \"STDIO\",\n        \"TCP:host.docker.internal:8811\"\n      ]\n    }\n  }\n}\n</code></pre>"},{"location":"lab5/projects/postgres-vscode/#step-10-restart-the-claude-desktop","title":"Step 10. Restart the Claude Desktop","text":""},{"location":"lab5/projects/postgres-vscode/#step-11-start-chatting-with-the-postgres-database","title":"Step 11. Start chatting with the Postgres Database","text":"<p>Prompt: </p> <pre><code>List out all the products in the catalog\n</code></pre> <p></p>"},{"location":"lab5/projects/visual-chatbot/","title":"Visual chatbot","text":""},{"location":"lab5/projects/visual-chatbot/#prerequisites","title":"Prerequisites","text":"<ul> <li>Install Docker Desktop 4.42.0 or later</li> <li>Enable Docker Model Runner in Docker Dashboard</li> <li>Enable the TCP host socket for DMR (use the default port of 12434)</li> <li>Download a model</li> </ul> <pre><code>doocker model pull ai/llama3.2:1B-Q8_0 \n</code></pre>"},{"location":"lab5/projects/visual-chatbot/#step-1-start-the-visual-chatbot-container","title":"Step 1. Start the visual chatbot container","text":"<pre><code>docker run -dp 3002:3000 -v /var/run/docker.sock:/var/run/docker.sock mikesir87/visual-chatbot\n</code></pre> <p>Did you notice that we're running the chatbot on port 3002?</p>"},{"location":"lab5/projects/visual-chatbot/#step-2-access-the-chatbot","title":"Step 2. Access the chatbot","text":"<p>Open your web browser and go to http://localhost:3002.</p>"},{"location":"lab5/projects/visual-chatbot/#step-3-choose-the-right-model-and-llm-backend","title":"Step 3. Choose the right model and LLM Backend","text":"<p>For this demo, select the following options:</p> <ul> <li>Docker Model Runner</li> <li>Model: <code>ai/llama3.2:1B-Q8_0</code></li> </ul> <p>Click \"Save\" to apply the settings.</p> <p>The system prompt currently indicates the LLM should act as a whimsical guide.  This suggests it will be fun and probably use lots of emojis!</p>"},{"location":"lab5/projects/visual-chatbot/#step-4-ask-what-it-can-do","title":"Step 4. Ask what it can do","text":"<p>Enter a prompt asking \"Hello! What can you do for me today?\"</p> <p></p> <p>Note that the app does not stream the responses, so it may take a little while to get the response.  So, try to stall for time a bit (without revealing it's all running locally).</p> <p>Hooray! We have a message!</p>"},{"location":"lab5/projects/visual-chatbot/#step-5-click-the-response-to-see-the-message","title":"Step 5. Click the response to see the message","text":"<p>Clicking on a message will provide the details of that message, which came directly from the LLM and will go back in the next API request.</p> <p></p>"},{"location":"lab5/projects/visual-chatbot/#step-6-change-the-system-prompt","title":"Step 6. Change the system prompt","text":"<p>Go to <code>Settings -&gt; System prompt</code> and change the prompt to be the <code>\"Grumpy old man\"</code>.</p> <p></p> <p>Before submitting, enable the <code>\"replay messages\"</code>. Then, press <code>save</code>.</p>"},{"location":"lab5/projects/visual-chatbot/#step-7-notice-the-very-different-persona","title":"Step 7. Notice the very different persona","text":"<p>Highlight the much shorter and direct response from the LLM now. At the end of the day, it's the same model, but with a very different set of instructions.</p> <p>This starts to introduce prompt engineering and how it's important to set the rules and the persona for the LLM. Again, normally, this is done by the GenAI application and not something an end user can change.</p>"},{"location":"lab5/projects/visual-chatbot/#interact-with-the-llm-for-the-real-time-information","title":"Interact with the LLM for the real-time information","text":"<p>Reset all the messages by clicking on the \"Reset messages\" button in the top right corner of the chat window.</p> <p>Let's ask the LLM what time it is in New York.</p> <p>Enter a prompt asking \"What time is it in New York now?\"</p> <p></p>"},{"location":"lab5/projects/visual-chatbot/#step-8-adding-time-tool","title":"Step 8. Adding Time Tool","text":"<p>Click \"Add Time tool\" to add a tool that will provide the current time in New York.</p> <p></p>"},{"location":"lab5/projects/visual-chatbot/#step-9-ask-the-llm-again","title":"Step 9. Ask the LLM again","text":"<p>Now, ask the LLM again \"What time is it in New York now?\".</p> <p></p>"},{"location":"lab5/projects/visual-chatbot/mcp/","title":"Running your First MCP Server","text":""},{"location":"lab5/projects/visual-chatbot/mcp/#prerequisites","title":"Prerequisites","text":"<ul> <li>Node.js installed (version 18 or higher recommended)</li> <li>npm or yarn package manager</li> </ul>"},{"location":"lab5/projects/visual-chatbot/mcp/#create-a-directory","title":"Create a directory","text":"<p>Create a new directory called mcptooling</p> <pre><code>mkdir mcptooling\n</code></pre>"},{"location":"lab5/projects/visual-chatbot/mcp/#package-depdendencies","title":"Package Depdendencies","text":"<p>Install the required packages:</p> <pre><code>npm install @modelcontextprotocol/sdk zod\n</code></pre>"},{"location":"lab5/projects/visual-chatbot/mcp/#execution-steps","title":"Execution Steps","text":"<p>Save the code to a file (e.g., time-server.js)</p> <pre><code>import { McpServer } from \"@modelcontextprotocol/sdk/server/mcp.js\";\nimport { StdioServerTransport } from \"@modelcontextprotocol/sdk/server/stdio.js\";\nimport { z } from \"zod\";\n\nconst server = new McpServer({ name: \"Time\", version: \"1.0.0\" });\n\nserver.tool(\n  \"get-current-time\",\n  \"Get the current time for a requested timezone\",\n  { \n    timezone: z.string().describe(\"The requested timezone in IANA format\"),\n  },\n  async ({ timezone }) =&gt; {\n    const time = new Date().toLocaleString(\"en-US\", { timeZone: timezone });\n\n    return {\n      content: [{\n        type: \"text\",\n        text: time,\n      }]\n    };\n  }\n);\n\nconst transport = new StdioServerTransport();\nawait server.connect(transport);\n</code></pre> <p>Save the code above to a file named <code>time-server.js</code> in the <code>mcptooling</code> directory.</p>"},{"location":"lab5/projects/visual-chatbot/mcp/#run-the-server","title":"Run the server:","text":"<p>Then, run the server using Node.js:</p> <pre><code>node time-server.js\n</code></pre>"},{"location":"lab5/projects/visual-chatbot/mcp/#what-this-server-does","title":"What this server does","text":"<p>This MCP server:</p> <ul> <li>Creates a tool called \"get-current-time\"</li> <li>Accepts a timezone parameter (in IANA format like \"America/New_York\")</li> <li>Returns the current time in that timezone</li> <li>Uses stdio transport (communicates via standard input/output)</li> </ul>"},{"location":"lab5/projects/visual-chatbot/mcp/#testing-the-server","title":"Testing the server","text":"<p>Since this uses stdio transport, you can test it by:</p> <ul> <li>Running it and sending JSON-RPC messages via stdin</li> <li>Or connecting it to an MCP client that supports stdio transport</li> <li>Or using it with applications that integrate MCP servers (like Claude Desktop, if configured)</li> </ul> <pre><code>node time-server.js\n</code></pre> <p>Great! Your MCP server is now running successfully. Since there's no output shown, that means it's working correctly and waiting for MCP protocol messages on stdin.</p>"},{"location":"lab5/projects/visual-chatbot/mcp/#method-1-test-with-a-simple-message","title":"Method 1: Test with a simple message","text":"<p>You can type this JSON-RPC message (press Enter after pasting):</p> <pre><code>{\"jsonrpc\": \"2.0\", \"id\": 1, \"method\": \"tools/list\"}\n</code></pre>"},{"location":"lab5/projects/visual-chatbot/mcp/#result","title":"Result","text":"<pre><code>{\"result\":{\"tools\":[{\"name\":\"get-current-time\",\"description\":\"Get the current time for a requested timezone\",\"inputSchema\":{\"type\":\"object\",\"properties\":{\"timezone\":{\"type\":\"string\",\"description\":\"The requested timezone in IANA format\"}},\"required\":[\"timezone\"],\"additionalProperties\":false,\"$schema\":\"http://json-schema.org/draft-07/schema#\"}}]},\"jsonrpc\":\"2.0\",\"id\":1}\n</code></pre>"},{"location":"lab5/projects/visual-chatbot/mcp/#method-2-test-the-time-tool","title":"Method 2: Test the time tool","text":"<p>You can test the \"get-current-time\" tool by sending a JSON-RPC message like this:</p> <pre><code>{\"jsonrpc\": \"2.0\", \"id\": 2, \"method\": \"tools/call\", \"params\": {\"name\": \"get-current-time\", \"arguments\": {\"timezone\": \"America/New_York\"}}}\n</code></pre> <p>You will receive a response with the current time in the specified timezone.</p> <pre><code>{\"result\":{\"content\":[{\"type\":\"text\",\"text\":\"5/31/2025, 10:29:05 PM\"}]},\"jsonrpc\":\"2.0\",\"id\":2}\n</code></pre>"},{"location":"lab5/projects/visual-chatbot/visual-chatbot/","title":"Getting Started","text":""},{"location":"lab5/projects/visual-chatbot/visual-chatbot/#prerequisites","title":"Prerequisites","text":"<ul> <li>Install Docker Desktop 4.41.0 or later</li> <li>Enable Docker Model Runner in Docker Dashboard</li> <li>Enable the TCP host socket for DMR (use the default port of 12434)</li> <li>Download a model</li> </ul> <pre><code>docker model pull ai/llama3.2:1B-Q8_0 \n</code></pre>"},{"location":"lab5/projects/visual-chatbot/visual-chatbot/#step-1-start-the-visual-chatbot-container","title":"Step 1. Start the visual chatbot container","text":"<pre><code>docker run -dp 3002:3000 -v /var/run/docker.sock:/var/run/docker.sock mikesir87/visual-chatbot\n</code></pre> <p>Did you notice that we're running the chatbot on port 3002?</p>"},{"location":"lab5/projects/visual-chatbot/visual-chatbot/#step-2-access-the-chatbot","title":"Step 2. Access the chatbot","text":"<p>Open your web browser and go to http://localhost:3002.</p>"},{"location":"lab5/projects/visual-chatbot/visual-chatbot/#step-3-choose-the-right-model-and-llm-backend","title":"Step 3. Choose the right model and LLM Backend","text":"<p>For this demo, select the following options:</p> <ul> <li>Docker Model Runner</li> <li>Model: <code>ai/llama3.2:1B-Q8_0</code></li> </ul> <p>Click \"Save\" to apply the settings.</p> <p>The system prompt currently indicates the LLM should act as a whimsical guide.  This suggests it will be fun and probably use lots of emojis!</p>"},{"location":"lab5/projects/visual-chatbot/visual-chatbot/#step-4-ask-what-it-can-do","title":"Step 4. Ask what it can do","text":"<p>Enter a prompt asking \"Hello! What can you do for me today?\"</p> <p></p> <p>Note that the app does not stream the responses, so it may take a little while to get the response.  So, try to stall for time a bit (without revealing it's all running locally).</p> <p>Hooray! We have a message!</p>"},{"location":"lab5/projects/visual-chatbot/visual-chatbot/#step-5-click-the-response-to-see-the-message","title":"Step 5. Click the response to see the message","text":"<p>Clicking on a message will provide the details of that message, which came directly from the LLM and will go back in the next API request.</p> <p></p>"},{"location":"lab5/projects/visual-chatbot/visual-chatbot/#step-6-change-the-system-prompt","title":"Step 6. Change the system prompt","text":"<p>Go to <code>Settings -&gt; System prompt</code> and change the prompt to be the <code>\"Grumpy old man\"</code>.</p> <p></p> <p>Before submitting, enable the <code>\"replay messages\"</code>. Then, press <code>save</code>.</p>"},{"location":"lab5/projects/visual-chatbot/visual-chatbot/#step-7-notice-the-very-different-persona","title":"Step 7. Notice the very different persona","text":"<p>Highlight the much shorter and direct response from the LLM now. At the end of the day, it's the same model, but with a very different set of instructions.</p> <p>This starts to introduce prompt engineering and how it's important to set the rules and the persona for the LLM. Again, normally, this is done by the GenAI application and not something an end user can change.</p>"},{"location":"lab5/projects/visual-chatbot/visual-chatbot/#interact-with-the-llm-for-the-real-time-information","title":"Interact with the LLM for the real-time information","text":"<p>Reset all the messages by clicking on the \"Reset messages\" button in the top right corner of the chat window.</p> <p>Let's ask the LLM what time it is in New York.</p> <p>Enter a prompt asking \"What time is it in New York now?\"</p> <p></p>"},{"location":"lab5/projects/visual-chatbot/visual-chatbot/#step-8-adding-time-tool","title":"Step 8. Adding Time Tool","text":"<p>Click \"Add Time tool\" to add a tool that will provide the current time in New York.</p> <p></p>"},{"location":"lab5/projects/visual-chatbot/visual-chatbot/#step-9-ask-the-llm-again","title":"Step 9. Ask the LLM again","text":"<p>Now, ask the LLM again \"What time is it in New York now?\".</p> <p></p>"},{"location":"lab6/getting-started/","title":"Getting Started","text":""},{"location":"lab6/getting-started/#prerequisite","title":"Prerequisite","text":"<ul> <li>Docker Desktop 4.43+</li> <li>Enable Docker Offload (optional)</li> </ul>"},{"location":"lab6/getting-started/#sample-compose-for-agents-demos","title":"Sample Compose for Agents Demos","text":"Demo Agent System Models MCPs project compose A2A Multi-Agent Fact Checker Multi-Agent OpenAI duckduckgo ./a2a compose.yaml Agno agent that summarizes GitHub issues Multi-Agent qwen3(local) github-official ./agno compose.yaml Vercel AI-SDK Chat-UI for mixing MCPs and Model Single Agent llama3.2(local), qwen3(local) wikipedia-mcp, brave, resend(email) ./vercel compose.yaml CrewAI Marketing Strategy Agent Multi-Agent qwen3(local) duckduckgo ./crew-ai compose.yaml ADK Multi-Agent Fact Checker Multi-Agent gemma3-qat(local) duckduckgo ./adk compose.yaml ADK &amp; Cerebras Golang Experts Multi-Agent unsloth/qwen3-gguf:4B-UD-Q4_K_XL &amp; ai/qwen2.5:latest (DMR local), llama-4-scout-17b-16e-instruct (Cerebras remote) ./adk-cerebras compose.yml LangGraph SQL Agent Single Agent qwen3(local) postgres ./langgraph compose.yaml Embabel Travel Agent Multi-Agent qwen3, Claude3.7, llama3.2, jimclark106/all-minilm:23M-F16 brave, github-official, wikipedia-mcp, weather, google-maps, airbnb ./embabel compose.yaml and compose.dmr.yaml Spring AI Brave Search Single Agent none duckduckgo ./spring-ai compose.yaml ADK Sock Store Agent Multi-Agent qwen3 MongoDb, Brave, Curl, ./adk-sock-shop compose.yaml Langchaingo DuckDuckGo Search Single Agent gemma3 duckduckgo ./langchaingo compose.yaml"},{"location":"lab6/overview/","title":"Overview","text":"<p>GenAI applications have evolved beyond simple text generation into sophisticated systems that actively reason, strategize, and execute complex workflows autonomously. We call it \"agentic GenAI\"\u2014a shift from AI systems that primarily generate content to systems that can actively engage in the world, solve problems, and achieve goals autonomously. These agentic GenAI applications don't just process requests\u2014they understand goals, decompose problems, and orchestrate solutions across multiple systems and services.</p> <p>Whether you're building a customer service agent, a research assistant, or a multi-agent workflow system, all agentic GenAI applications share a consistent architectural foundation built on three essential layers:</p> <p></p> <ul> <li>Models serve as the brains of the operation, generating output and determining how to achieve desired outcomes. </li> <li> <p>Tools provide the ability to fetch additional context or perform functions\u2014it's important to note that models can't actually do anything on their own, as tools provide these critical capabilities. </p> </li> <li> <p>Custom app code connects and orchestrates everything, giving the model the list of available tools, performing execution of requested tools, managing state, and building prompts. </p> </li> </ul> <p>While this architecture appears straightforward, deploying these interconnected layers in production creates a cascade of complex challenges that traditional containerization approaches struggle to address.</p>"},{"location":"lab6/overview/#why-deploying-agentic-genai-apps-is-harder-than-it-should-be","title":"Why Deploying agentic GenAI apps is Harder Than It Should Be","text":"<p>Building agentic GenAI applications is exciting\u2014getting them to work together in production is a nightmare.  Most organizations start by running individual agentic components manually: one agent does research, another analyzes data, third handles conversations. Each component runs separately, needs its own setup, and requires constant babysitting. What begins as a simple demo quickly becomes an unmanageable collection of processes that break frequently and are impossible to monitor effectively.</p> <p>When these agentic systems need to work together, the complexity explodes. Imagine trying to coordinate a team where everyone speaks a different language, uses different tools, and has no shared calendar. That's essentially what happens with agentic GenAI applications\u2014they need to pass information between each other, handle failures gracefully, and adapt when things go wrong. Without proper coordination, you end up spending more time managing the infrastructure than benefiting from the intelligence. The resource challenge makes everything worse. Agentic GenAI applications are hungry for computing power, especially GPU resources. While tools like Docker Model Runner help run models locally, most development machines can't handle multiple agentic systems simultaneously. A single modern AI model might need 3.5GB of video memory just to start, and production systems need much more. Teams often find themselves choosing between running applications locally with toy models or jumping straight to expensive cloud infrastructure with no middle ground.</p> <p>Without standards, every pair of agentic systems that need to communicate invents its own communication method. Some use web requests, others use message queues, and some use entirely different protocols. Debugging becomes impossible because there's no single place to see what's happening across your agentic network. Security, monitoring, and troubleshooting all become exponentially harder as you add more agentic components to the mix.</p>"},{"location":"lab6/overview/#introducing-agentic-compose","title":"Introducing Agentic Compose","text":"<p>Agentic Compose solves these production deployment challenges by providing a comprehensive Docker Compose framework specifically designed for orchestrating agentic GenAI applications. This approach transforms chaotic infrastructure management into elegant, declarative patterns that scale from development to enterprise production.</p> <p>Agentic Compose is a comprehensive Docker Compose framework specifically designed for orchestrating agentic GenAI applications in production. It provides declarative infrastructure management for the three essential layers of agentic AI systems\u2014Models, Tools (via MCP Gateway), and App Code\u2014enabling teams to deploy complex multi-agent workflows with the same simplicity as traditional containerized applications. </p> <p>While Docker Model Runner enables developers to run LLMs locally with simple commands and OpenAI-compatible APIs, the reality of GPU-intensive AI workloads creates unique orchestration challenges. Even with local model serving capabilities, most development environments lack the substantial GPU resources required for multi-agent systems (often needing 3.5GB+ VRAM per model, with production systems requiring significantly more). </p> <p>This creates a critical need for seamless workflows that work consistently from local development using Docker Model Runner to cloud-based GPU infrastructure using solutions like Docker Offload for production workloads.</p>"},{"location":"lab6/overview/#whats-new-in-agentic-compose","title":"What\u2019s New in Agentic Compose","text":""},{"location":"lab6/overview/#1-the-old-vs-new-model-definition-and-management","title":"1. The Old Vs New Model Definition and Management","text":""},{"location":"lab6/overview/#the-old-model-definition-and-management","title":"The Old Model Definition and Management","text":"<p>Previously, models were defined as services using a provider mechanism. </p> <pre><code>services:\n  llm:\n    provider:\n      type: model\n      options:\n        model: ai/gemma3\n</code></pre> <p>While this approach worked, it created unnecessary complexity and didn't align with how developers naturally think about models as shared resources. To help the ecosystem evolve and provide a more intuitive experience, we've decided to define models more concretely by giving them a top-level field in the Compose specification.</p>"},{"location":"lab6/overview/#2-the-new-model-definition-and-management","title":"2. The New Model Definition and Management","text":"<p>In Agentic Compose, you can define one or more models for your GenAI app to use through a straightforward, declarative approach. </p> <p>The top-level models field in your Compose file defines the available models, making them easily discoverable and reusable across your entire application stack.</p> <pre><code>services:\n  agent-app:\n    ...\n    models:\n      gemma3:\n        endpoint_var: OPENAI_BASE_URL\n        model_var: OPENAI_MODEL\nmodels:\n  gemma3:\n    model: ai/gemma3\n</code></pre> <p>Each service can specify the model it needs and the environment variables that should be injected to establish the connection. This approach provides clean separation between model configuration and service logic\u2014services simply declare their model requirements, while the infrastructure handles the complexity of making those models available.</p> <p>When you launch your Compose file, the Docker Model Runner automatically figures out how to ensure the model is accessible, regardless of whether you're running in Docker Desktop or deploying to production infrastructure.</p> <p>The Model Runner intelligently handles the underlying complexity: - Docker Desktop users benefit from seamless integration with local GPU resources and model caching - Production environments automatically provision appropriate model containers or connect to external model services - Environment variables are automatically injected based on your model configuration, eliminating manual setup steps</p> <p>This abstraction means developers can focus on building their agentic logic rather than wrestling with model deployment intricacies. Whether you're using a lightweight local model for development or connecting to enterprise-grade model services in production, the same Compose configuration works consistently across all environments.</p> <p>The result is a unified approach where model dependencies are as easy to manage as any other service dependency in your Docker Compose stack.</p>"},{"location":"lab6/overview/#2-mcp-gateway-a-new-standardized-tool-integration","title":"2. MCP Gateway: A New Standardized Tool Integration","text":"<p>To support the definitions of tooling in GenAI applications, we're open-sourcing and making the MCP Gateway available as a standalone container. This represents a major advancement in how tools are managed and integrated across agentic systems. With Agentic Compose, you can start an MCP gateway that will launch containerized MCP servers, handle credentials securely, and provide a standardized interface for tool interactions.</p> <pre><code>services:\n  agent-app:\n    ...\n    environment:\n      - MCPGATEWAY_ENDPOINT=http://mcp-gateway:8811/sse\n    depends_on:\n      - mcp-gateway\n\n  mcp-gateway:\n    image: docker/mcp-gateway:latest\n    use_api_socket: true\n    command:\n      - --transport=sse\n      - --servers=duckduckgo\n      - --tools=search,fetch_content\n</code></pre> <ul> <li>Simplified Tool Configuration</li> </ul> <p>Use the MCP Gateway container directly in your Compose stack by specifying the servers and tools your agents need to use. The configuration is straightforward\u2014you define which MCP servers to launch and which specific tools to make available. The gateway handles all the complexity of server lifecycle management, tool discovery, and secure communication protocols. There are lots of other flags and options available for advanced configurations\u2014check the docs and content kit for more details on customizing your tool setup.</p> <ul> <li>Enhanced Security and Access Control</li> </ul> <p>The new use_api_socket field provides a powerful capability by injecting both the Docker socket and registry credentials into the MCP Gateway. This enables private registry pulls and works seamlessly in enforced login environments, solving common deployment challenges in enterprise settings. The gateway can securely access private tool containers and handle authentication automatically, eliminating manual credential management.</p> <ul> <li>Multi-Agent Tool Isolation</li> </ul> <p>For applications with multiple agents, you can launch multiple MCP Gateway containers and networks to isolate tools appropriately. This ensures agents have access only to the tools they need, following the principle of least privilege. Each gateway can be configured with different tool sets, access permissions, and security policies, enabling fine-grained control over what capabilities each agent can access.</p> <ul> <li>Containerized Tool Ecosystem</li> </ul> <p>The MCP Gateway transforms tool management from a configuration nightmare into a declarative infrastructure pattern. Tools become as easy to deploy and manage as any other containerized service, with benefits including automatic dependency resolution, version management, security isolation, and standardized communication protocols. This approach enables teams to build robust tool ecosystems that scale from development to production while maintaining security and reliability standards.</p>"},{"location":"lab6/overview/#3-app-code","title":"3. App Code","text":"<p>The app code layer in Agentic Compose represents the most familiar part of the stack\u2014these are \"normal\" containerized applications that developers already know how to build, deploy, and manage. There's nothing fundamentally new here from a Compose perspective, as this follows the same patterns and practices used for any containerized application. The power comes from how these applications integrate with the models and tools layers to create intelligent, autonomous systems.</p> <ul> <li>Framework-Driven Development</li> </ul> <p>Many agentic apps will use a framework to manage the complex orchestration of flow, memory, and state that characterizes intelligent applications. These frameworks handle the intricate details of prompt management, conversation history, tool selection, and execution coordination that would otherwise require significant custom development. Rather than building these capabilities from scratch, developers can leverage proven frameworks that provide robust foundations for agentic behavior.</p> <ul> <li>Rich Ecosystem Support</li> </ul> <p>Our samples repository includes examples using several popular frameworks, demonstrating the flexibility and compatibility of Agentic Compose across the ecosystem:</p> <ul> <li>A2A - Agent-to-Agent communication patterns</li> <li>ADK - Agent Development Kit for rapid prototyping</li> <li>Agno - Lightweight agentic framework</li> <li>Crew AI - Multi-agent collaboration systems</li> <li>Embabel - Embedding and semantic search integration</li> <li>Langgraph - Graph-based agent workflows</li> <li>Spring AI - Enterprise Java integration</li> <li> <p>Vercel - Modern web application deployment</p> </li> <li> <p>Seamless Integration</p> </li> </ul> <p>The beauty of this approach is that your existing containerization knowledge directly applies to agentic applications. Your app code simply needs to connect to the models and tools defined in your Compose file through standard environment variables and network connections. The frameworks handle the complexity of agentic behavior, while Compose handles the infrastructure complexity, allowing developers to focus on building the unique business logic and user experiences that define their applications. This familiar development model dramatically reduces the learning curve for teams adopting agentic technologies, making it possible to leverage existing Docker expertise while building cutting-edge intelligent applications.</p> <p>Here\u2019s how a typical Compose file would look like </p> <p>File: compose.yaml</p> <pre><code># Docker Compose configuration for Simple Node.js MCP Agent\nservices:\n  app:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    ports:\n      - \"3000:3000\"\n    environment:\n      - PORT=3000\n      - MCP_GATEWAY_URL=http://mcp-gateway:8811\n    depends_on:\n      - mcp-gateway\n    models:\n      gemma:\n        endpoint_var: MODEL_RUNNER_URL\n        model_var: MODEL_RUNNER_MODEL\n\n  mcp-gateway:\n    # Standard MCP Gateway configuration\n    image: docker/mcp-gateway:latest\n    use_api_socket: true\n    ports:\n      - \"8811:8811\"\n    command:\n      - --transport=sse\n      # Add any MCP servers you want to use\n      - --servers=duckduckgo\n      - --tools=search,fetch_content\n\n# Model configuration\nmodels:\n  gemma:\n    model: ai/gemma3-qat\n</code></pre> <p>File: compose-offload.yaml</p> <pre><code># Override for using Docker Offload with larger models\n# Usage: docker compose -f compose.yaml -f compose.offload.yaml up --build\n\nservices:\n  app:\n    # Override with larger model configuration\n    models:\n      gemma-large:\n        endpoint_var: MODEL_RUNNER_URL\n        model_var: MODEL_RUNNER_MODEL\n\nmodels:\n  gemma-large:\n    # Larger model for enhanced reasoning (requires 16+ GB VRAM)\n    model: ai/gemma3:27B-Q4_K_M\n    context_size: 8192\n</code></pre> <p>File: compose-openai.yaml</p> <pre><code>services:\n  agents:\n    environment:\n      - OPENAI_MODEL_NAME=gpt-4.1-mini\n    secrets:\n      - openai-api-key\n\nsecrets:\n  openai-api-key:\n    file: secret.openai-api-key\n</code></pre> Use Case Command Local Development (Docker Desktop) <code>docker compose up -d --build</code> Docker Offload with GPU Support <code>docker compose -f compose.yaml -f compose.offload.yaml up --build</code> Using OpenAI Integration <code>docker compose -f compose.yaml -f compose.openai.yaml up --build</code>"},{"location":"lab6/projects/a2a-multi-agent-fact-checker/","title":"A2A Multi-Agent Fact Checker","text":""},{"location":"lab6/projects/a2a-multi-agent-fact-checker/#project-overview","title":"Project Overview","text":"<p>The A2A Multi-Agent Fact Checker is a sophisticated demonstration of collaborative multi-agent systems built with Google's Agent2Agent SDK (A2A). This project showcases how specialized AI agents can work together under orchestration to verify facts and claims through a combination of external research and internal reasoning.</p>"},{"location":"lab6/projects/a2a-multi-agent-fact-checker/#what-makes-this-special","title":"What Makes This Special?","text":"<ul> <li>Zero Configuration: Run with a single Docker command</li> <li>Local LLM Inference: Uses Docker Model Runner (no API keys required)</li> <li>Real-time Web Search: Integrates DuckDuckGo via Model Context Protocol (MCP)</li> <li>Orchestrated Workflow: Demonstrates true agent collaboration</li> <li>Modular Architecture: Each agent has distinct roles and capabilities</li> </ul>"},{"location":"lab6/projects/a2a-multi-agent-fact-checker/#architecture-overview","title":"Architecture Overview","text":"Agent Role Agent Tools Used Role Description Auditor \u274c None None Coordinates the entire fact-checking workflow and delivers the final answer. Critic \u2705 DuckDuckGo via MCP DuckDuckGo via MCP Gathers evidence to support or refute the claim. Reviser \u274c None None Refines and finalizes the answer without external input."},{"location":"lab6/projects/a2a-multi-agent-fact-checker/#getting-started","title":"Getting Started","text":"<p>Before running the project, ensure you have:</p> <ul> <li>Docker Desktop 4.43.0+ or Docker Engine installed</li> <li>A laptop or workstation with a GPU (e.g., MacBook) for running models locally</li> <li>Compose 2.38.1+ (if using Docker Engine on Linux)</li> <li>GPU drivers properly installed for your system</li> </ul>"},{"location":"lab6/projects/a2a-multi-agent-fact-checker/#run-the-project","title":"Run the project","text":"<pre><code>cd compose-for-agents/a2a\ndocker compose -f compose.yaml -f compose.offload.yaml up --build\n</code></pre> <p>By now, you should be able to see the containers as shown in the Docker Dashboard.</p> <p></p> <p>Open http://localhost:8080/dev-ui/?app=AgentKit in your browser to access the AgentKit UI, where you can interact with the agents.</p> <p></p>"},{"location":"lab6/projects/a2a-multi-agent-fact-checker/#prompt-1","title":"Prompt 1","text":"<p>Example 1: Scientific Fact Check</p> <p>Input: \"How far is moon from the earth?\"</p> <p>Workflow:</p> <p>Auditor receives the question and plans the verification strategy Critic searches DuckDuckGo for current scientific theories and evidence Reviser analyzes the gathered evidence and forms a reasoned conclusion Auditor presents the final verdict with supporting evidence</p>"},{"location":"lab6/projects/a2a-multi-agent-fact-checker/#prompt-2","title":"Prompt 2","text":"<p>Example 2: Current Events Check</p> <p>Input: \"What is the current status of renewable energy adoption globally?\"</p> <p>Workflow:</p> <p>Auditor identifies the need for recent data Critic searches for latest statistics and reports Reviser synthesizes multiple sources into coherent trends Auditor provides up-to-date analysis with data backing</p>"},{"location":"lab6/projects/agentic-catalog/","title":"Product Catalog Chatbot with AI-Enhanced Management System","text":"<p>An intelligent product catalog management platform powered by Docker Model Runner, AI Agents, and Event-Driven Architecture. This system combines conversational AI, intelligent agents, and real-time processing for comprehensive catalog management.</p>"},{"location":"lab6/projects/agentic-catalog/#system-overview","title":"\ud83c\udfaf System Overview","text":"<p>This is a complete AI-enhanced catalog management system featuring:</p>"},{"location":"lab6/projects/agentic-catalog/#core-ai-components","title":"\ud83e\udd16 Core AI Components","text":"<ul> <li>Chatbot Interface - Natural language product queries and conversations</li> <li>AI Agent Service - Automated vendor evaluation, market research, and customer matching</li> <li>MCP Gateway - Model Context Protocol for AI tool orchestration</li> <li>Model Runner Integration - Local AI model execution with Llama 3.2</li> </ul>"},{"location":"lab6/projects/agentic-catalog/#complete-architecture","title":"\ud83c\udfd7\ufe0f Complete Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Frontend      \u2502  \u2502  Agent Portal   \u2502  \u2502  Chatbot UI     \u2502\n\u2502   Port: 5173    \u2502  \u2502   Port: 3001    \u2502  \u2502   Port: 5174    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        \u2502                     \u2502                     \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                   \u2502                    \u2502\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502   Backend API   \u2502  \u2502 Agent Service   \u2502  \u2502 Chatbot API     \u2502\n        \u2502   Port: 3000    \u2502  \u2502  Port: 7777     \u2502  \u2502  Port: 8082     \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                   \u2502                    \u2502                    \u2502\n                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502                   \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502  MCP Gateway    \u2502  \u2502  Model Runner   \u2502\n                    \u2502  Port: 8811     \u2502  \u2502  (Local AI)     \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n              \u2502               \u2502               \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502   PostgreSQL    \u2502 \u2502     MongoDB     \u2502 \u2502     Kafka       \u2502\n    \u2502   Port: 5432    \u2502 \u2502   Port: 27017   \u2502 \u2502  Port: 9092     \u2502\n    \u2502 (Products DB)   \u2502 \u2502 (Agent History) \u2502 \u2502 (Event Stream)  \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"lab6/projects/agentic-catalog/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"lab6/projects/agentic-catalog/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker Desktop with Model Runner enabled</li> <li>At least 8GB RAM (4GB+ for AI models)</li> <li>Docker Compose v2.0+</li> </ul>"},{"location":"lab6/projects/agentic-catalog/#1-pull-required-models","title":"1. Pull Required Models","text":"<pre><code># Pull the AI model for chatbot and agents\ndocker model pull ai/llama3.2:1B-Q8_0\n</code></pre>"},{"location":"lab6/projects/agentic-catalog/#2-start-all-services","title":"2. Start All Services","text":"<pre><code># Clone the repository\ngit clone https://github.com/ajeetraina/catalog-service-node-chatbot.git\ncd catalog-service-node-chatbot\n\n# Start the complete system\ndocker compose up -d --build\n</code></pre>"},{"location":"lab6/projects/agentic-catalog/#3-access-the-applications","title":"3. Access the Applications","text":"Service URL Description \ud83e\udd16 Chatbot Interface http://localhost:5174 Main chatbot for product queries \ud83c\udfe0 Main Frontend http://localhost:5173 Product catalog management \ud83d\udd27 Agent Portal http://localhost:3001 AI agent management interface \ud83d\udcca Kafka UI http://localhost:8080 Event streaming monitoring \ud83d\uddc4\ufe0f pgAdmin http://localhost:5050 Database administration"},{"location":"lab6/projects/agentic-catalog/#chatbot-capabilities","title":"\ud83d\udcac Chatbot Capabilities","text":"<p>Note: After running <code>sh add-products.sh</code>, your chatbot will have access to 50+ sample products for testing these capabilities.</p>"},{"location":"lab6/projects/agentic-catalog/#natural-language-product-search","title":"Natural Language Product Search","text":"<pre><code>User: \"Show me all electronics under $500\"\nChatbot: \"I found 8 electronics products under $500:\n\n\ud83d\udcf1 iPhone SE - $399.00\n   Compact smartphone with A15 Bionic chip\n\n\ud83c\udfa7 Sony WH-1000XM5 - $399.00  \n   Industry-leading noise canceling headphones\n\n\u231a Apple Watch - $249.00\n   Advanced health and fitness tracking\n\n[Additional products...]\n\nWould you like details about any specific product?\"\n</code></pre>"},{"location":"lab6/projects/agentic-catalog/#intelligent-category-browsing","title":"Intelligent Category Browsing","text":"<pre><code>User: \"What's popular in home electronics?\"\nChatbot: \"Based on our catalog data:\n\n\ud83c\udfe0 Smart Home Category (12 products):\n\u2022 Smart speakers: 4 products ($49-199)\n\u2022 Security cameras: 3 products ($89-299) \n\u2022 Smart thermostats: 2 products ($149-249)\n\u2022 Home automation hubs: 3 products ($99-349)\n\nThe Amazon Echo Dot and Ring Doorbell are our top sellers.\nWhich type of smart home device interests you?\"\n</code></pre>"},{"location":"lab6/projects/agentic-catalog/#catalog-analytics","title":"Catalog Analytics","text":"<pre><code>User: \"Give me a summary of the catalog\"\nChatbot: \"Here's your catalog overview:\n\n\ud83d\udcca **Catalog Statistics:**\n\u2022 Total Products: 127\n\u2022 Active Categories: 8\n\u2022 Price Range: $9.99 - $2,499.00\n\u2022 Average Price: $186.45\n\u2022 New This Week: 5 products\n\n\ud83d\udd25 **Top Categories:**\n1. Electronics (34 products)\n2. Clothing (28 products)  \n3. Home &amp; Garden (21 products)\n\nIs there a specific category you'd like to explore?\"\n</code></pre>"},{"location":"lab6/projects/agentic-catalog/#ai-agent-system","title":"\ud83e\udd16 AI Agent System","text":"<p>The system includes intelligent agents powered by Model Runner:</p>"},{"location":"lab6/projects/agentic-catalog/#vendor-intake-agent","title":"\ud83d\udd0d Vendor Intake Agent","text":"<ul> <li>Purpose: Evaluates new product submissions</li> <li>Scoring: 0-100 AI-powered evaluation</li> <li>Criteria: Product quality, market fit, pricing analysis</li> <li>Integration: Kafka events trigger automatic evaluation</li> </ul>"},{"location":"lab6/projects/agentic-catalog/#market-research-agent","title":"\ud83d\udcc8 Market Research Agent","text":"<ul> <li>Purpose: Automated competitor analysis</li> <li>Features: Price comparison, feature analysis, market positioning</li> <li>Data Sources: Web scraping, API integrations via MCP Gateway</li> <li>Output: Comprehensive market reports</li> </ul>"},{"location":"lab6/projects/agentic-catalog/#customer-match-agent","title":"\ud83c\udfaf Customer Match Agent","text":"<ul> <li>Purpose: Analyzes customer preferences and buying patterns</li> <li>Intelligence: ML-based recommendation engine</li> <li>Personalization: Tailored product suggestions</li> <li>History: Stored in MongoDB for continuous learning</li> </ul>"},{"location":"lab6/projects/agentic-catalog/#catalog-management-agent","title":"\ud83d\udccb Catalog Management Agent","text":"<ul> <li>Purpose: Maintains and optimizes product catalog</li> <li>Automation: Auto-categorization, price updates, inventory sync</li> <li>Quality Control: Duplicate detection, data validation</li> <li>Optimization: SEO improvements, description enhancement</li> </ul>"},{"location":"lab6/projects/agentic-catalog/#model-runner-configuration","title":"\u2699\ufe0f Model Runner Configuration","text":""},{"location":"lab6/projects/agentic-catalog/#supported-models","title":"Supported Models","text":"Model Size Performance Use Case <code>ai/llama3.2:1B-Q4_0</code> 1GB Fast Chatbot, basic agents <code>ai/llama3.2:1B-Q8_0</code> 1.5GB Balanced Recommended <code>ai/llama3.2:3B-Q4_0</code> 2GB High Quality Complex agent tasks"},{"location":"lab6/projects/agentic-catalog/#environment-configuration","title":"Environment Configuration","text":"<pre><code># Model Runner integration in compose.yaml\nmodels:\n  llama_model:\n    model: ai/llama3.2:1B-Q8_0\n\n# Services using Model Runner\nchatbot-backend:\n  models:\n    llama_model:\n      endpoint_var: MODEL_RUNNER_URL\n      model_var: MODEL_RUNNER_MODEL\n\nagent-service:\n  models:\n    llama_model:\n      endpoint_var: MODEL_RUNNER_URL  \n      model_var: MODEL_RUNNER_MODEL\n\nmcp-gateway:\n  models:\n    llama_model:\n      endpoint_var: MODEL_RUNNER_URL\n      model_var: MODEL_RUNNER_MODEL\n</code></pre>"},{"location":"lab6/projects/agentic-catalog/#core-services-deep-dive","title":"\ud83d\udd27 Core Services Deep Dive","text":""},{"location":"lab6/projects/agentic-catalog/#chatbot-backend-chatbot-backend","title":"Chatbot Backend (<code>chatbot-backend</code>)","text":"<ul> <li>Port: 8082</li> <li>Purpose: Natural language processing for product queries</li> <li>AI Integration: Direct Model Runner connection</li> <li>Database: PostgreSQL for product data</li> <li>Features: Intent recognition, smart search, conversation context</li> </ul>"},{"location":"lab6/projects/agentic-catalog/#agent-service-agent-service","title":"Agent Service (<code>agent-service</code>)","text":"<ul> <li>Port: 7777</li> <li>Purpose: AI agents for automation and intelligence</li> <li>Event Processing: Kafka-based event handling</li> <li>Databases: PostgreSQL (catalog) + MongoDB (history)</li> <li>Agents: Vendor evaluation, market research, customer matching</li> </ul>"},{"location":"lab6/projects/agentic-catalog/#mcp-gateway-mcp-gateway","title":"MCP Gateway (<code>mcp-gateway</code>)","text":"<ul> <li>Port: 8811</li> <li>Purpose: AI tool orchestration and integration</li> <li>Protocols: Server-Sent Events (SSE) transport</li> <li>Tools: fetch, brave, resend, curl, mongodb</li> <li>Role: Enables agents to access external data sources</li> </ul>"},{"location":"lab6/projects/agentic-catalog/#backend-api-backend","title":"Backend API (<code>backend</code>)","text":"<ul> <li>Port: 3000  </li> <li>Purpose: Core catalog management API</li> <li>Database: PostgreSQL primary, MongoDB secondary</li> <li>Integration: Agent service communication</li> <li>Features: CRUD operations, vendor management, analytics</li> </ul>"},{"location":"lab6/projects/agentic-catalog/#event-driven-architecture","title":"\ud83d\udcca Event-Driven Architecture","text":""},{"location":"lab6/projects/agentic-catalog/#kafka-integration","title":"Kafka Integration","text":"<pre><code># Kafka configuration for real-time processing\nkafka:\n  image: apache/kafka:latest\n  ports: [\"9092:9092\", \"9093:9093\"]\n\n# Event flow examples:\n# 1. New product \u2192 Agent evaluation \u2192 Catalog update\n# 2. User query \u2192 Agent research \u2192 Enhanced response  \n# 3. Price change \u2192 Market analysis \u2192 Competitive insights\n</code></pre>"},{"location":"lab6/projects/agentic-catalog/#event-types","title":"Event Types","text":"<ul> <li>Product Events: Creation, updates, deletions</li> <li>Agent Events: Evaluation results, research findings</li> <li>User Events: Interactions, preferences, feedback</li> <li>System Events: Health checks, performance metrics</li> </ul>"},{"location":"lab6/projects/agentic-catalog/#database-architecture","title":"\ud83d\uddc4\ufe0f Database Architecture","text":""},{"location":"lab6/projects/agentic-catalog/#postgresql-primary-database","title":"PostgreSQL (Primary Database)","text":"<pre><code>-- Core product catalog\nCREATE TABLE products (\n    id SERIAL PRIMARY KEY,\n    name VARCHAR(255) NOT NULL,\n    description TEXT,\n    category VARCHAR(100),\n    price DECIMAL(10, 2),\n    vendor_id VARCHAR(100),\n    ai_score INTEGER, -- AI evaluation score\n    status VARCHAR(20) DEFAULT 'active',\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\n-- Vendor management\nCREATE TABLE vendors (\n    id VARCHAR(100) PRIMARY KEY,\n    name VARCHAR(255),\n    evaluation_score INTEGER,\n    status VARCHAR(20)\n);\n</code></pre>"},{"location":"lab6/projects/agentic-catalog/#mongodb-agent-history","title":"MongoDB (Agent History)","text":"<pre><code>// Agent interaction history\n{\n  agentType: \"vendor_evaluation\",\n  productId: \"12345\",\n  evaluation: {\n    score: 85,\n    reasoning: \"High quality, competitive pricing\",\n    criteria: [\"quality\", \"price\", \"market_fit\"]\n  },\n  timestamp: \"2025-01-15T10:30:00Z\"\n}\n\n// Customer interaction patterns\n{\n  sessionId: \"abc123\",\n  interactions: [\n    { query: \"electronics under $200\", results: 5 },\n    { action: \"view_product\", productId: \"67890\" }\n  ],\n  preferences: [\"electronics\", \"budget_conscious\"]\n}\n</code></pre>"},{"location":"lab6/projects/agentic-catalog/#api-endpoints","title":"\ud83d\udee0\ufe0f API Endpoints","text":""},{"location":"lab6/projects/agentic-catalog/#chatbot-api-localhost8082","title":"Chatbot API (<code>localhost:8082</code>)","text":"<pre><code># Natural language chat\nPOST /api/chat\n{\n  \"message\": \"Show me laptops for programming\"\n}\n\n# Product search with filters\nGET /api/products/search?q=laptop&amp;category=electronics&amp;maxPrice=1500\n\n# Get conversation context\nGET /api/context/{sessionId}\n</code></pre>"},{"location":"lab6/projects/agentic-catalog/#agent-service-api-localhost7777","title":"Agent Service API (<code>localhost:7777</code>)","text":"<pre><code># Trigger vendor evaluation\nPOST /api/agents/evaluate-vendor\n{\n  \"vendorId\": \"tech_corp_001\",\n  \"products\": [\"product_123\", \"product_456\"]\n}\n\n# Get market research report\nGET /api/agents/market-research/{productId}\n\n# Customer preference analysis\nPOST /api/agents/analyze-customer\n{\n  \"customerId\": \"user_789\",\n  \"interactionHistory\": [...]\n}\n</code></pre>"},{"location":"lab6/projects/agentic-catalog/#main-backend-api-localhost3000","title":"Main Backend API (<code>localhost:3000</code>)","text":"<pre><code># Product management\nGET /api/products\nPOST /api/products\nPUT /api/products/{id}\nDELETE /api/products/{id}\n\n# Vendor management  \nGET /api/vendors\nPOST /api/vendors\nGET /api/vendors/{id}/evaluation\n\n# Analytics\nGET /api/analytics/summary\nGET /api/analytics/trends\n</code></pre>"},{"location":"lab6/projects/agentic-catalog/#development-testing","title":"\ud83d\udd0d Development &amp; Testing","text":""},{"location":"lab6/projects/agentic-catalog/#local-development-setup","title":"Local Development Setup","text":"<pre><code># Start infrastructure only\ndocker compose up postgres mongodb kafka -d\n\n# Run services locally for development\ncd chatbot-backend &amp;&amp; npm run dev  # Port 8082\ncd agent-service &amp;&amp; npm run dev    # Port 7777  \ncd backend &amp;&amp; npm run dev          # Port 3000\n</code></pre>"},{"location":"lab6/projects/agentic-catalog/#health-checks","title":"Health Checks","text":"<pre><code># Check all service health\ncurl http://localhost:3000/health   # Backend\ncurl http://localhost:8082/health   # Chatbot\ncurl http://localhost:7777/health   # Agents\ncurl http://localhost:8811/health   # MCP Gateway\n\n# Check Model Runner status\ndocker model ls\ndocker model info ai/llama3.2:1B-Q8_0\n</code></pre>"},{"location":"lab6/projects/agentic-catalog/#testing-the-chatbot","title":"Testing the Chatbot","text":"<pre><code># Test natural language queries\ncurl -X POST http://localhost:8082/api/chat \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"message\": \"What are your most expensive electronics?\"}'\n\n# Test product search\ncurl \"http://localhost:8082/api/products/search?q=smartphone&amp;maxPrice=800\"\n\n# Test category lookup\ncurl http://localhost:8082/api/categories\n</code></pre>"},{"location":"lab6/projects/agentic-catalog/#monitoring-observability","title":"\ud83d\udcc8 Monitoring &amp; Observability","text":""},{"location":"lab6/projects/agentic-catalog/#service-monitoring","title":"Service Monitoring","text":"<ul> <li>Kafka UI: http://localhost:8080 - Event stream monitoring</li> <li>pgAdmin: http://localhost:5050 - Database monitoring  </li> <li>Docker Logs: <code>docker compose logs [service-name]</code></li> <li>Health Endpoints: Built-in health checks for all services</li> </ul>"},{"location":"lab6/projects/agentic-catalog/#performance-metrics","title":"Performance Metrics","text":"<pre><code># Check resource usage\ndocker stats\n\n# Monitor model performance\ndocker model stats ai/llama3.2:1B-Q8_0\n\n# View service logs\ndocker compose logs -f chatbot-backend\ndocker compose logs -f agent-service\n</code></pre>"},{"location":"lab6/projects/agentic-catalog/#adding-products-data","title":"\ud83d\ude80 Adding Products &amp; Data","text":""},{"location":"lab6/projects/agentic-catalog/#automated-product-import","title":"Automated Product Import","text":"<pre><code># Use the provided import script\n./add-products.sh\n\n# Or use the Node.js automation script\nnpm run import-products\n</code></pre>"},{"location":"lab6/projects/agentic-catalog/#testing-with-sample-data","title":"Testing with Sample Data","text":"<p>The system includes comprehensive sample data: - 50+ Products across multiple categories - Vendor Information with AI evaluations - Mock Market Data for agent testing - Customer Interaction Patterns for recommendation testing</p>"},{"location":"lab6/projects/agentic-catalog/#production-considerations","title":"\ud83d\udd12 Production Considerations","text":""},{"location":"lab6/projects/agentic-catalog/#security","title":"Security","text":"<ul> <li>Environment Variables: Use Docker secrets for production</li> <li>Network Security: Configure proper firewall rules</li> <li>Database Security: Enable PostgreSQL/MongoDB authentication</li> <li>API Security: Implement rate limiting and authentication</li> </ul>"},{"location":"lab6/projects/agentic-catalog/#scaling","title":"Scaling","text":"<ul> <li>Horizontal Scaling: Multiple instances of each service</li> <li>Database Sharding: Partition large datasets</li> <li>Model Optimization: Use quantized models for performance</li> <li>Caching: Redis for frequently accessed data</li> </ul>"},{"location":"lab6/projects/agentic-catalog/#deployment","title":"Deployment","text":"<pre><code># Production environment variables\nenvironment:\n  - NODE_ENV=production\n  - MODEL_RUNNER_URL=${MODEL_RUNNER_URL}\n  - POSTGRES_HOST=${POSTGRES_HOST}\n  - KAFKA_BROKERS=${KAFKA_BROKERS}\n  - MONGODB_URI=${MONGODB_URI}\n</code></pre>"},{"location":"lab6/projects/agentic-catalog/#quick-start-checklist","title":"\ud83c\udfaf Quick Start Checklist","text":"<ul> <li>[ ] Pull AI Model: <code>docker model pull ai/llama3.2:1B-Q8_0</code></li> <li>[ ] Start Services: <code>docker compose up -d --build</code></li> <li>[ ] Import Products: <code>./add-products.sh</code></li> <li>[ ] Test Chatbot: Visit http://localhost:5174</li> <li>[ ] Check Agents: Visit http://localhost:3001  </li> <li>[ ] Monitor System: Visit http://localhost:8080</li> </ul>"},{"location":"lab6/projects/hackathon-recommender/","title":"Hackathon Recommender","text":""},{"location":"lab6/projects/hackathon-recommender/#ai-agents-hackathon-project-recommender","title":"AI Agents Hackathon Project Recommender","text":"<p>Discover your perfect hackathon project with AI-powered GitHub profile analysis</p> <p>An intelligent system that analyzes GitHub profiles and generates personalized hackathon project recommendations using AI agents, Model Context Protocol (MCP), and real-time data integration.</p> <p></p>"},{"location":"lab6/projects/hackathon-recommender/#features","title":"Features","text":"<ul> <li>AI-Powered Analysis: Intelligent GitHub profile analysis using AI agents</li> <li>Real-Time Data: Live integration with GitHub API through MCP servers</li> <li>Personalized Recommendations: Tailored hackathon projects based on skills and interests</li> <li>Secure Architecture: Containerized microservices with proper secrets management</li> <li>Modern Stack: Next.js + Python + Docker + MCP integration</li> <li>Production Ready: Scalable architecture with comprehensive tooling</li> </ul>"},{"location":"lab6/projects/hackathon-recommender/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Next.js UI    \u2502\u2500\u2500\u2500\u25b6\u2502  Python Agents  \u2502\u2500\u2500\u2500\u25b6\u2502   MCP Gateway   \u2502\n\u2502   (Port 3003)   \u2502    \u2502   (Port 7777)   \u2502    \u2502   (Port 8811)   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                       \u2502                       \u2502\n         \u2502                       \u2502                       \u25bc\n         \u2502                       \u2502              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502                       \u2502              \u2502  GitHub Tools   \u2502\n         \u2502                       \u2502              \u2502  DuckDuckGo     \u2502\n         \u2502                       \u2502              \u2502  Fetch Tools    \u2502\n         \u2502                       \u2502              \u2502  (76 tools)     \u2502\n         \u2502                       \u2502              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                       \u25bc\n         \u2502              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502              \u2502   AI Models     \u2502\n         \u2502              \u2502   Qwen3 Small   \u2502\n         \u2502              \u2502   (Local/API)   \u2502\n         \u2502              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502     User        \u2502\n\u2502   Browser       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"lab6/projects/hackathon-recommender/#quick-start","title":"Quick Start","text":""},{"location":"lab6/projects/hackathon-recommender/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker &amp; Docker Compose</li> <li>GitHub Personal Access Token</li> <li>8GB+ RAM (for local AI models)</li> </ul>"},{"location":"lab6/projects/hackathon-recommender/#1-clone-setup","title":"1. Clone &amp; Setup","text":"<pre><code>git clone https://github.com/ajeetraina/ai-agents-hackathon-recommender\ncd ai-agents-hackathon-recommender\n</code></pre>"},{"location":"lab6/projects/hackathon-recommender/#2-configure-secrets","title":"2. Configure Secrets","text":"<p>Create a file .mcp.env and add your GitHub PAT</p> <pre><code>nano .mcp.env\ngithub.personal_access_token=XXX\n</code></pre>"},{"location":"lab6/projects/hackathon-recommender/#3-launch-the-application","title":"3. Launch the Application","text":"<pre><code># Build and start all services\ndocker compose up -d --build\n\n# Monitor the logs\ndocker compose logs -f\n</code></pre>"},{"location":"lab6/projects/hackathon-recommender/#4-access-the-application","title":"4. Access the Application","text":"<ul> <li>Web Interface: http://localhost:3003</li> <li>Agents API: http://localhost:7777</li> <li>Health Check: http://localhost:7777/health</li> </ul>"},{"location":"lab6/projects/hackathon-recommender/#usage","title":"\ud83c\udfaf Usage","text":"<ol> <li>Open the web interface at http://localhost:3003</li> <li>Enter any GitHub username (e.g., \"microsoft\", \"torvalds\", \"ajeetraina\")</li> <li>Click \"Get Recommendations\"</li> <li>Receive personalized hackathon project suggestions!</li> </ol>"},{"location":"lab6/projects/hackathon-recommender/#development","title":"\ud83d\udd27 Development","text":""},{"location":"lab6/projects/hackathon-recommender/#local-development-setup","title":"Local Development Setup","text":"<pre><code># Start in development mode\ndocker compose up --build\n\n# Watch logs\ndocker compose logs -f agents-ui\ndocker compose logs -f agents\n</code></pre>"},{"location":"lab6/projects/hackathon-recommender/#api-endpoints","title":"API Endpoints","text":""},{"location":"lab6/projects/hackathon-recommender/#agents-service-port-7777","title":"Agents Service (Port 7777)","text":"<ul> <li><code>GET /health</code> - Health check</li> <li><code>POST /analyze</code> - Analyze GitHub profile</li> <li><code>GET /agents</code> - List available agents</li> </ul> <pre><code># Example API usage\ncurl -X POST http://localhost:7777/analyze \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"username\": \"microsoft\", \"agent\": \"hackathon_recommender\"}'\n</code></pre>"},{"location":"lab6/projects/devduck/basic-interaction/","title":"Basic Multi-Agent Interaction","text":"<p>Now that your system is properly deployed and configured, it's time to explore how the multi-agent system works in practice. You'll learn to interact with DevDuck and understand how it coordinates between different agents to provide intelligent responses.</p>"},{"location":"lab6/projects/devduck/basic-interaction/#understanding-the-interface","title":"Understanding the Interface","text":""},{"location":"lab6/projects/devduck/basic-interaction/#web-interface-overview","title":"\ud83c\udf10 Web Interface Overview","text":"<p>Let's start by familiarizing yourself with the DevDuck interface:</p> <ol> <li> <p>Open the Interface: Navigate to http://localhost:8000/dev-ui/?app=devduck</p> </li> <li> <p>Interface Components:</p> </li> <li>Chat Area: Main conversation interface</li> <li>Input Field: Where you type your messages</li> <li>Agent Indicator: Shows which agents are active</li> <li>System Status: Connection and health indicators</li> </ol>"},{"location":"lab6/projects/devduck/basic-interaction/#interface-features","title":"\ud83d\udd0d Interface Features","text":"<p>Explore these key features:</p>"},{"location":"lab6/projects/devduck/basic-interaction/#message-types","title":"Message Types","text":"<p>The interface supports various message formats: - Text Messages: Standard conversation - Code Blocks: Formatted code snippets - System Messages: Agent status updates - Error Messages: Troubleshooting information</p>"},{"location":"lab6/projects/devduck/basic-interaction/#agent-status-indicators","title":"Agent Status Indicators","text":"<p>Look for these visual cues: - \ud83d\udfe2 Green: Agent is online and responding - \ud83d\udfe1 Yellow: Agent is processing - \ud83d\udd34 Red: Agent is unavailable or erroring - \ud83d\udd35 Blue: Agent is initializing</p>"},{"location":"lab6/projects/devduck/basic-interaction/#first-interactions","title":"First Interactions","text":""},{"location":"lab6/projects/devduck/basic-interaction/#basic-conversation","title":"\ud83e\udd16 Basic Conversation","text":"<p>Let's start with simple interactions to test the system:</p>"},{"location":"lab6/projects/devduck/basic-interaction/#exercise-1-introduction","title":"Exercise 1: Introduction","text":"<p>Try this conversation:</p> <pre><code>You: Hello DevDuck! Can you introduce yourself?\n</code></pre> <p>Expected Response Pattern: - DevDuck will introduce itself as a multi-agent system - It should mention its capabilities and available agents - The response might route to the Local Agent for a quick reply</p>"},{"location":"lab6/projects/devduck/basic-interaction/#observation","title":"Observation","text":"<p>Notice the tabs available - Trace, Events, State, Artifacts, Sessions, Eval - this gives developers comprehensive insight into agent behavior.  You can track not just what the agent did, but how long each step took, what state changes occurred, and what artifacts were created. </p> <p>The Session ID at the top shows this is running in an isolated session, and the dropdown showing \"devduck\" indicates you can switch between different agent configurations.</p> <p>This interface transforms AI agent development from a \"black box\" experience into transparent, debuggable system. Wonder \"Why did the agent make that decision?\" or \"What's taking so long?\", they can use this exact interface to trace through every step, measure performance bottlenecks, and understand the agent's reasoning process. This is what makes DevDuck not just a demo, but a serious development platform for building production AI agents.</p>"},{"location":"lab6/projects/devduck/basic-interaction/#exercise-2-agent-awareness","title":"Exercise 2: Agent Awareness","text":"<p>Ask about the agents:</p> <pre><code>You: What agents are available in this system?\n</code></pre> <p>Expected Response: DevDuck should explain: - Local Agent for quick processing - Cerebras Agent for complex analysis - Its own role as orchestrator - How it decides which agent to use</p>"},{"location":"lab6/projects/devduck/basic-interaction/#observation_1","title":"Observation","text":"<ul> <li>Notice how the devduck_agent is acting as the orchestrator - it can describe the other agents and route tasks appropriately.</li> <li>This confirms the architecture where the Local Agent  makes routing decisions about when to delegate to the high-performance Cerebras Agent for complex tasks.</li> <li>Tracing the Decision Process: The left panel shows this simple question took ~12.3 seconds to process, with most time spent in agent_run [devduck_agent] and the call_llm.</li> <li>This demonstrates the intelligent routing logic - even answering \"what agents are available\" requires the main agent to think through the system architecture and provide contextual information about each agent's capabilities. -It's fascinating to see how they can trace every decision and understand exactly how the multi-agent system coordinates to provide responses.</li> <li>This is the \"agentic loop\" with full transparency that makes DevDuck so powerful for development!Retry</li> </ul>"},{"location":"lab6/projects/devduck/basic-interaction/#understanding-agent-routing","title":"\ud83e\udde0 Understanding Agent Routing","text":"<p>DevDuck makes intelligent decisions about which agent should handle each request. Let's explore this:</p>"},{"location":"lab6/projects/devduck/basic-interaction/#exercise-3-simple-vs-complex-queries","title":"Exercise 3: Simple vs Complex Queries","text":"<p>Simple Query (likely routed to Local Agent):</p> <pre><code>You: What is Python?\n</code></pre> <p>Complex Query (likely routed to Cerebras Agent):</p> <pre><code>You: Can you analyze this algorithm and suggest optimizations?\n\ndef fibonacci(n):\n    if n &lt;= 1:\n        return n\n    return fibonacci(n-1) + fibonacci(n-2)\n</code></pre> <p>Observation Points: - Notice response time differences - Check which agent handles each request in the logs - Compare the depth and quality of responses</p> <p>Controlling the agents</p> <p>You: Cerebras, can you analyze this algorithm and suggest optimizations?</p> <p>Observation Points:</p> <ul> <li>Try to see if explicit agent targeting changes the routing behavior!</li> <li>By prefixing with \"Cerebras,\" you overrode the automatic routing logic and forced the system to use the high-performance agent.</li> <li>This is exactly what you need to understand - they can control which agent handles their requests.</li> </ul>"},{"location":"lab6/projects/devduck/basic-interaction/#next-steps","title":"Next Steps","text":"<p>In the next section, you'll dive deeper into local agent capabilities and learn how to optimize local processing for specific tasks.</p> <p>Ready to explore local agent specialization? Let's continue! \ud83d\ude80</p>"},{"location":"lab6/projects/devduck/cerebras-interaction/","title":"Cerebras Analysis &amp; Intelligence","text":"<p>The Cerebras Agent represents the \"brain\" of your multi-agent system, leveraging powerful cloud-based AI models for complex analysis, advanced reasoning, and sophisticated problem-solving. In this lab, you'll explore its capabilities and learn to harness its full potential.</p> <p>!!! info \"Learning Focus\"     Master Cerebras cloud AI integration, advanced analysis capabilities, and intelligent request handling for complex scenarios.</p>"},{"location":"lab6/projects/devduck/cerebras-interaction/#understanding-cerebras-technology","title":"Understanding Cerebras Technology","text":""},{"location":"lab6/projects/devduck/cerebras-interaction/#what-makes-cerebras-special","title":"\ud83e\udde0 What Makes Cerebras Special","text":"<p>Cerebras Systems creates the world's largest computer chips specifically designed for AI:</p> <ul> <li>Massive Scale: 850,000 AI cores on a single chip</li> <li>High Speed: 20x faster than traditional GPUs for AI workloads</li> <li>Large Models: Support for models with 70B+ parameters</li> <li>Efficient Processing: Optimized for transformer architectures</li> </ul>"},{"location":"lab6/projects/devduck/cerebras-interaction/#cerebras-cloud-models","title":"\ud83c\udf10 Cerebras Cloud Models","text":"<p>Available models through the API:</p> Model Parameters Best For Llama 3.1 8B 8 billion Quick analysis, code generation Llama 3.1 70B 70 billion Complex reasoning, detailed analysis Llama 3.1 405B 405 billion Advanced research, complex problem solving"},{"location":"lab6/projects/devduck/cerebras-interaction/#quick-test","title":"Quick Test","text":"<pre><code>Prompt 1: Can I talk to Cerebras?\n</code></pre> <pre><code>Prompt 2: Search documentation for Express Routing\n</code></pre> <pre><code>Prompt 3: Create a function that adds two numbers and test it in the sandbox\n</code></pre>"},{"location":"lab6/projects/devduck/cerebras-interaction/#hands-on-cerebras-agent-exploration","title":"Hands-On Cerebras Agent Exploration","text":""},{"location":"lab6/projects/devduck/cerebras-interaction/#exercise-1-complex-analysis-tasks","title":"\ud83c\udfaf Exercise 1: Complex Analysis Tasks","text":"<p>Let's test scenarios that specifically benefit from Cerebras' advanced capabilities:</p>"},{"location":"lab6/projects/devduck/cerebras-interaction/#advanced-code-analysis","title":"Advanced Code Analysis","text":"<p>Complex Algorithm Analysis:</p> <pre><code>You: Analyze this sorting algorithm for time/space complexity and suggest optimizations:\n\ndef complex_sort(arr, key_func=None, reverse=False):\n    if not arr:\n        return arr\n\n    if len(arr) == 1:\n        return arr\n\n    # Multi-dimensional sorting with custom key\n    if key_func:\n        decorated = [(key_func(item), i, item) for i, item in enumerate(arr)]\n    else:\n        decorated = [(item, i, item) for i, item in enumerate(arr)]\n\n    # Recursive merge sort implementation\n    def merge_sort(lst):\n        if len(lst) &lt;= 1:\n            return lst\n\n        mid = len(lst) // 2\n        left = merge_sort(lst[:mid])\n        right = merge_sort(lst[mid:])\n\n        return merge(left, right, reverse)\n\n    def merge(left, right, rev):\n        result = []\n        i = j = 0\n\n        while i &lt; len(left) and j &lt; len(right):\n            if (left[i][0] &lt;= right[j][0]) != rev:\n                result.append(left[i])\n                i += 1\n            else:\n                result.append(right[j])\n                j += 1\n\n        result.extend(left[i:])\n        result.extend(right[j:])\n        return result\n\n    sorted_decorated = merge_sort(decorated)\n    return [item for key, original_index, item in sorted_decorated]\n</code></pre> <p>Expected Cerebras Response: - Detailed complexity analysis (O(n log n) time, O(n) space) - Identification of the Schwartzian transform pattern - Suggestions for optimization and edge case handling - Comparison with built-in sorting methods</p>"},{"location":"lab6/projects/devduck/cerebras-interaction/#architecture-design","title":"Architecture Design","text":"<p>System Architecture Challenge:</p> <pre><code>You: Design a microservices architecture for a real-time collaborative document editing platform like Google Docs. Consider scalability, consistency, conflict resolution, and real-time synchronization. Include specific technologies, data flow, and handling of 100,000+ concurrent users.\n</code></pre> <p>Monitor the Processing:</p> <pre><code># Watch for Cerebras API calls in logs\ndocker compose logs -f devduck-agent | grep -i \"cerebras\\|api\\|routing\"\n</code></pre>"},{"location":"lab6/projects/devduck/cerebras-interaction/#exercise-2-multi-step-reasoning","title":"\ud83d\ude80 Exercise 2: Multi-Step Reasoning","text":""},{"location":"lab6/projects/devduck/cerebras-interaction/#complex-problem-solving","title":"Complex Problem Solving","text":"<p>Business Analysis Scenario:</p> <pre><code>You: A startup has the following metrics:\n- Monthly users: 50,000 (growing 15% monthly)\n- Conversion rate: 2.5%\n- Average revenue per user: $25/month\n- Churn rate: 5% monthly\n- Customer acquisition cost: $30\n\nAnalyze the business model viability, calculate key metrics like LTV:CAC ratio, identify potential issues, and recommend strategies for improvement. Project the business trajectory over 24 months.\n</code></pre> <p>Technical Deep Dive:</p> <pre><code>You: Explain how blockchain consensus mechanisms work, compare Proof of Work vs Proof of Stake vs Delegated Proof of Stake, analyze their trade-offs in terms of security, scalability, and energy efficiency, and suggest which would be best for a new cryptocurrency focused on micropayments.\n</code></pre>"},{"location":"lab6/projects/devduck/cerebras-interaction/#exercise-3-cerebras-performance-analysis","title":"\ud83d\udcca Exercise 3: Cerebras Performance Analysis","text":""},{"location":"lab6/projects/devduck/cerebras-interaction/#response-quality-comparison","title":"Response Quality Comparison","text":"<p>Create a systematic comparison between Local Agent and Cerebras Agent:</p> <pre><code># Create cerebras_comparison.py\nimport requests\nimport time\nimport json\n\ndef compare_agents():\n    \"\"\"Compare Local vs Cerebras Agent responses.\"\"\"\n\n    test_scenarios = [\n        {\n            \"query\": \"Explain recursion in programming\",\n            \"complexity\": \"medium\",\n            \"expected_cerebras\": \"detailed_explanation\"\n        },\n        {\n            \"query\": \"Design a distributed caching system for a global application\",\n            \"complexity\": \"high\", \n            \"expected_cerebras\": \"architectural_analysis\"\n        },\n        {\n            \"query\": \"Analyze the time complexity of this nested loop algorithm\",\n            \"complexity\": \"high\",\n            \"expected_cerebras\": \"mathematical_analysis\"\n        }\n    ]\n\n    results = []\n\n    for scenario in test_scenarios:\n        print(f\"\\n\ud83e\uddea Testing: {scenario['query'][:50]}...\")\n\n        # Test with system (should route to appropriate agent)\n        start_time = time.time()\n\n        response = requests.post(\n            'http://localhost:8000/chat',\n            json={\n                \"message\": scenario['query'],\n                \"conversation_id\": f\"compare-{int(time.time())}\"\n            },\n            timeout=60\n        )\n\n        duration = time.time() - start_time\n\n        if response.status_code == 200:\n            response_data = response.json()\n            response_length = len(response_data.get('response', ''))\n\n            results.append({\n                \"query\": scenario['query'][:50],\n                \"complexity\": scenario['complexity'],\n                \"duration\": duration,\n                \"response_length\": response_length,\n                \"success\": True\n            })\n\n            print(f\"  \u2705 Duration: {duration:.2f}s\")\n            print(f\"  \ud83d\udcdd Response length: {response_length} chars\")\n        else:\n            print(f\"  \u274c Failed: {response.status_code}\")\n            results.append({\n                \"query\": scenario['query'][:50],\n                \"complexity\": scenario['complexity'],\n                \"duration\": duration,\n                \"success\": False\n            })\n\n    # Analysis\n    print(\"\\n\ud83d\udcca Comparison Results:\")\n    for result in results:\n        if result['success']:\n            print(f\"  {result['complexity']:6} | {result['duration']:6.2f}s | {result['response_length']:4d} chars | {result['query']}\")\n\n    return results\n\nif __name__ == '__main__':\n    compare_agents()\n</code></pre> <p>Run the comparison:</p> <pre><code>python cerebras_comparison.py\n</code></pre>"},{"location":"lab6/projects/devduck/cerebras-interaction/#advanced-cerebras-features","title":"Advanced Cerebras Features","text":""},{"location":"lab6/projects/devduck/cerebras-interaction/#model-selection-and-optimization","title":"\ud83c\udf9b\ufe0f Model Selection and Optimization","text":""},{"location":"lab6/projects/devduck/cerebras-interaction/#dynamic-model-selection","title":"Dynamic Model Selection","text":"<p>Configure different models for different scenarios:</p> <pre><code># Create model configuration\ncat &gt; cerebras_config.py &lt;&lt; 'EOF'\nclass CerebrasModelConfig:\n    \"\"\"Configuration for different Cerebras models.\"\"\"\n\n    MODELS = {\n        \"fast\": {\n            \"name\": \"llama3.1-8b\",\n            \"max_tokens\": 1000,\n            \"temperature\": 0.7,\n            \"use_case\": \"Quick analysis, code generation\"\n        },\n        \"balanced\": {\n            \"name\": \"llama3.1-70b\", \n            \"max_tokens\": 2000,\n            \"temperature\": 0.5,\n            \"use_case\": \"Complex reasoning, detailed analysis\"\n        },\n        \"advanced\": {\n            \"name\": \"llama3.1-405b\",\n            \"max_tokens\": 4000,\n            \"temperature\": 0.3,\n            \"use_case\": \"Research, complex problem solving\"\n        }\n    }\n\n    @classmethod\n    def get_model_for_complexity(cls, complexity_score):\n        \"\"\"Select model based on query complexity.\"\"\"\n        if complexity_score &lt; 0.3:\n            return cls.MODELS[\"fast\"]\n        elif complexity_score &lt; 0.7:\n            return cls.MODELS[\"balanced\"]\n        else:\n            return cls.MODELS[\"advanced\"]\n\n    @classmethod\n    def estimate_complexity(cls, query):\n        \"\"\"Estimate query complexity (simplified heuristic).\"\"\"\n        complexity_indicators = [\n            'analyze', 'design', 'architecture', 'optimize',\n            'compare', 'evaluate', 'research', 'complex',\n            'algorithm', 'system', 'scalable', 'distributed'\n        ]\n\n        query_lower = query.lower()\n        matches = sum(1 for indicator in complexity_indicators \n                     if indicator in query_lower)\n\n        base_complexity = len(query) / 1000  # Length factor\n        keyword_complexity = matches * 0.2   # Keyword factor\n\n        return min(base_complexity + keyword_complexity, 1.0)\n\n# Example usage\nquery = \"Design a distributed microservices architecture for real-time data processing\"\ncomplexity = CerebrasModelConfig.estimate_complexity(query)\nmodel = CerebrasModelConfig.get_model_for_complexity(complexity)\n\nprint(f\"Query complexity: {complexity:.2f}\")\nprint(f\"Recommended model: {model['name']}\")\nprint(f\"Use case: {model['use_case']}\")\nEOF\n\npython cerebras_config.py\n</code></pre>"},{"location":"lab6/projects/devduck/cerebras-interaction/#advanced-configuration","title":"\ud83d\udd27 Advanced Configuration","text":""},{"location":"lab6/projects/devduck/cerebras-interaction/#temperature-and-parameter-tuning","title":"Temperature and Parameter Tuning","text":"<pre><code># Create cerebras_tuning.py\nimport requests\nimport json\n\ndef test_temperature_effects():\n    \"\"\"Test how temperature affects Cerebras responses.\"\"\"\n\n    query = \"Explain the concept of machine learning in one paragraph.\"\n    temperatures = [0.1, 0.5, 0.9]\n\n    print(\"\ud83c\udf21\ufe0f Testing Temperature Effects on Cerebras:\")\n\n    for temp in temperatures:\n        print(f\"\\nTesting temperature: {temp}\")\n\n        # Note: This is a conceptual example\n        # Actual temperature control would be implemented in the agent\n        response = requests.post(\n            'http://localhost:8000/chat',\n            json={\n                \"message\": f\"[TEMPERATURE={temp}] {query}\",\n                \"conversation_id\": f\"temp-test-{temp}\"\n            }\n        )\n\n        if response.status_code == 200:\n            resp_data = response.json()\n            resp_text = resp_data.get('response', '')[:200]\n            print(f\"Response preview: {resp_text}...\")\n\n        time.sleep(2)  # Rate limiting\n\ndef test_max_tokens_impact():\n    \"\"\"Test how max_tokens affects response completeness.\"\"\"\n\n    query = \"Provide a comprehensive guide to setting up a CI/CD pipeline with Docker, including best practices and security considerations.\"\n\n    print(\"\\n\ud83d\udccf Testing Max Tokens Impact:\")\n\n    # This would be configured in the agent settings\n    max_tokens_settings = [500, 1500, 3000]\n\n    for max_tokens in max_tokens_settings:\n        print(f\"\\nTesting max_tokens: {max_tokens}\")\n\n        response = requests.post(\n            'http://localhost:8000/chat', \n            json={\n                \"message\": f\"[MAX_TOKENS={max_tokens}] {query}\",\n                \"conversation_id\": f\"tokens-test-{max_tokens}\"\n            }\n        )\n\n        if response.status_code == 200:\n            resp_data = response.json()\n            response_length = len(resp_data.get('response', ''))\n            print(f\"Response length: {response_length} characters\")\n\nif __name__ == '__main__':\n    test_temperature_effects()\n    test_max_tokens_impact()\n</code></pre>"},{"location":"lab6/projects/devduck/cerebras-interaction/#specialized-use-cases","title":"Specialized Use Cases","text":""},{"location":"lab6/projects/devduck/cerebras-interaction/#business-intelligence-and-analysis","title":"\ud83d\udcc8 Business Intelligence and Analysis","text":""},{"location":"lab6/projects/devduck/cerebras-interaction/#exercise-4-market-analysis","title":"Exercise 4: Market Analysis","text":"<p>Complex Business Scenario:</p> <pre><code>You: A SaaS company is considering expanding to the European market. They currently serve 10,000 customers in North America with $2M ARR. Analyze the key factors they should consider for European expansion:\n\n1. Market analysis and sizing\n2. Regulatory considerations (GDPR, data localization)\n3. Technical infrastructure requirements\n4. Competitive landscape\n5. Go-to-market strategy\n6. Financial projections and investment requirements\n7. Risk assessment and mitigation strategies\n\nProvide a comprehensive expansion plan with timeline and milestones.\n</code></pre>"},{"location":"lab6/projects/devduck/cerebras-interaction/#technical-architecture-and-design","title":"\ud83c\udfd7\ufe0f Technical Architecture and Design","text":""},{"location":"lab6/projects/devduck/cerebras-interaction/#exercise-5-system-design-challenge","title":"Exercise 5: System Design Challenge","text":"<p>Scalability Challenge:</p> <pre><code>You: Design a chat system like Discord that can handle:\n- 10 million concurrent users\n- Real-time messaging with &lt;100ms latency\n- File sharing up to 100MB\n- Voice/video calling\n- Message history and search\n- Mobile and web clients\n\nInclude:\n- Complete system architecture diagram\n- Technology stack recommendations\n- Database design and sharding strategy\n- Caching and CDN approach\n- Real-time communication protocols\n- Scalability and reliability considerations\n- Security and privacy measures\n- Deployment and monitoring strategy\n</code></pre>"},{"location":"lab6/projects/devduck/cerebras-interaction/#research-and-analysis","title":"\ud83d\udd2c Research and Analysis","text":""},{"location":"lab6/projects/devduck/cerebras-interaction/#exercise-6-technical-research","title":"Exercise 6: Technical Research","text":"<p>Emerging Technology Analysis:</p> <pre><code>You: Compare quantum computing approaches for solving optimization problems:\n\n1. Gate-based quantum computers (IBM, Google)\n2. Quantum annealing (D-Wave)\n3. Photonic quantum computing (Xanadu)\n4. Trapped ion systems (IonQ)\n\nAnalyze:\n- Current capabilities and limitations\n- Suitable problem types for each approach\n- Commercial viability timeline\n- Integration challenges with classical systems\n- Cost-benefit analysis for enterprise adoption\n- Future development roadmaps\n\nProvide recommendations for a logistics company considering quantum optimization for route planning.\n</code></pre>"},{"location":"lab6/projects/devduck/cerebras-interaction/#monitoring-and-optimization","title":"Monitoring and Optimization","text":""},{"location":"lab6/projects/devduck/cerebras-interaction/#cerebras-performance-monitoring","title":"\ud83d\udcca Cerebras Performance Monitoring","text":""},{"location":"lab6/projects/devduck/cerebras-interaction/#create-monitoring-dashboard","title":"Create Monitoring Dashboard","text":"<pre><code># Create cerebras_monitor.py\nimport requests\nimport time\nimport statistics\nfrom datetime import datetime\n\nclass CerebrasMonitor:\n    def __init__(self):\n        self.api_calls = []\n        self.response_times = []\n        self.token_usage = []\n\n    def log_api_call(self, duration, tokens_used, success=True):\n        \"\"\"Log an API call for monitoring.\"\"\"\n        self.api_calls.append({\n            'timestamp': datetime.now(),\n            'duration': duration,\n            'tokens_used': tokens_used,\n            'success': success\n        })\n\n        if success:\n            self.response_times.append(duration)\n            self.token_usage.append(tokens_used)\n\n    def get_stats(self):\n        \"\"\"Get comprehensive statistics.\"\"\"\n        if not self.api_calls:\n            return {\"message\": \"No API calls recorded\"}\n\n        successful_calls = [call for call in self.api_calls if call['success']]\n\n        return {\n            'total_calls': len(self.api_calls),\n            'successful_calls': len(successful_calls),\n            'success_rate': len(successful_calls) / len(self.api_calls),\n            'avg_response_time': statistics.mean(self.response_times) if self.response_times else 0,\n            'median_response_time': statistics.median(self.response_times) if self.response_times else 0,\n            'avg_tokens_used': statistics.mean(self.token_usage) if self.token_usage else 0,\n            'total_tokens_used': sum(self.token_usage),\n            'calls_per_minute': self._calculate_rate(),\n            'uptime_minutes': self._calculate_uptime()\n        }\n\n    def _calculate_rate(self):\n        if len(self.api_calls) &lt; 2:\n            return 0\n\n        first_call = self.api_calls[0]['timestamp']\n        last_call = self.api_calls[-1]['timestamp']\n        duration_minutes = (last_call - first_call).total_seconds() / 60\n\n        return len(self.api_calls) / max(duration_minutes, 1)\n\n    def _calculate_uptime(self):\n        if not self.api_calls:\n            return 0\n\n        first_call = self.api_calls[0]['timestamp']\n        return (datetime.now() - first_call).total_seconds() / 60\n\n    def print_dashboard(self):\n        \"\"\"Print a monitoring dashboard.\"\"\"\n        stats = self.get_stats()\n\n        if 'message' in stats:\n            print(stats['message'])\n            return\n\n        print(\"\ud83e\udde0 Cerebras Agent Dashboard\")\n        print(\"============================\")\n        print(f\"Total Calls: {stats['total_calls']}\")\n        print(f\"Success Rate: {stats['success_rate']:.1%}\")\n        print(f\"Avg Response Time: {stats['avg_response_time']:.2f}s\")\n        print(f\"Median Response Time: {stats['median_response_time']:.2f}s\")\n        print(f\"Avg Tokens/Call: {stats['avg_tokens_used']:.0f}\")\n        print(f\"Total Tokens Used: {stats['total_tokens_used']}\")\n        print(f\"Rate: {stats['calls_per_minute']:.1f} calls/min\")\n        print(f\"Uptime: {stats['uptime_minutes']:.1f} minutes\")\n\n# Usage example\nmonitor = CerebrasMonitor()\n\n# Simulate monitoring\ntest_queries = [\n    \"Simple explanation of Docker\",\n    \"Complex microservices architecture design\", \n    \"Algorithm analysis and optimization\"\n]\n\nfor query in test_queries:\n    start_time = time.time()\n\n    # Simulate API call\n    try:\n        response = requests.post(\n            'http://localhost:8000/chat',\n            json={\n                \"message\": query,\n                \"conversation_id\": f\"monitor-{int(time.time())}\"\n            },\n            timeout=30\n        )\n\n        duration = time.time() - start_time\n\n        # Estimate token usage (simplified)\n        tokens_used = len(query.split()) + 100  # Rough estimate\n\n        success = response.status_code == 200\n        monitor.log_api_call(duration, tokens_used, success)\n\n        print(f\"\u2705 Processed: {query[:40]}... ({duration:.2f}s)\")\n\n    except Exception as e:\n        duration = time.time() - start_time\n        monitor.log_api_call(duration, 0, False)\n        print(f\"\u274c Failed: {query[:40]}... ({e})\")\n\n    time.sleep(1)\n\nmonitor.print_dashboard()\n</code></pre> <p>Run the monitor:</p> <pre><code>python cerebras_monitor.py\n</code></pre>"},{"location":"lab6/projects/devduck/cerebras-interaction/#cost-optimization","title":"\ud83d\udcb0 Cost Optimization","text":""},{"location":"lab6/projects/devduck/cerebras-interaction/#token-usage-optimization","title":"Token Usage Optimization","text":"<pre><code># Create cost_optimizer.py\nclass CebrasCostOptimizer:\n    # Approximate pricing (check current Cerebras pricing)\n    TOKEN_COSTS = {\n        \"llama3.1-8b\": 0.00002,    # $0.02 per 1K tokens\n        \"llama3.1-70b\": 0.00008,   # $0.08 per 1K tokens  \n        \"llama3.1-405b\": 0.0002,   # $0.20 per 1K tokens\n    }\n\n    def __init__(self):\n        self.usage_history = []\n\n    def estimate_cost(self, model, input_tokens, output_tokens):\n        \"\"\"Estimate cost for a request.\"\"\"\n        total_tokens = input_tokens + output_tokens\n        cost_per_token = self.TOKEN_COSTS.get(model, 0.0001)\n        return total_tokens * cost_per_token\n\n    def suggest_optimization(self, query):\n        \"\"\"Suggest cost optimization strategies.\"\"\"\n        suggestions = []\n\n        # Check query length\n        if len(query.split()) &gt; 500:\n            suggestions.append(\"Consider breaking long queries into smaller parts\")\n\n        # Check complexity\n        if any(word in query.lower() for word in ['comprehensive', 'detailed', 'complete', 'thorough']):\n            suggestions.append(\"Consider using a smaller model for initial analysis\")\n\n        # Check for code analysis\n        if 'analyze' in query.lower() and len(query) &gt; 1000:\n            suggestions.append(\"Pre-process code analysis with Local Agent first\")\n\n        return suggestions\n\n    def daily_cost_estimate(self, daily_queries, avg_tokens_per_query, model=\"llama3.1-70b\"):\n        \"\"\"Estimate daily costs.\"\"\"\n        daily_tokens = daily_queries * avg_tokens_per_query\n        cost_per_token = self.TOKEN_COSTS.get(model, 0.0001)\n\n        daily_cost = daily_tokens * cost_per_token\n        monthly_cost = daily_cost * 30\n        annual_cost = daily_cost * 365\n\n        return {\n            'daily_cost': daily_cost,\n            'monthly_cost': monthly_cost,\n            'annual_cost': annual_cost,\n            'tokens_per_day': daily_tokens\n        }\n\n# Usage\noptimizer = CebrasCostOptimizer()\n\n# Estimate costs for different usage patterns\nscenarios = [\n    {\"name\": \"Light Usage\", \"queries\": 50, \"tokens\": 500},\n    {\"name\": \"Medium Usage\", \"queries\": 200, \"tokens\": 1000},\n    {\"name\": \"Heavy Usage\", \"queries\": 1000, \"tokens\": 2000}\n]\n\nprint(\"\ud83d\udcb0 Cerebras Cost Analysis\")\nprint(\"==========================\")\n\nfor scenario in scenarios:\n    costs = optimizer.daily_cost_estimate(\n        scenario['queries'], \n        scenario['tokens']\n    )\n\n    print(f\"\\n{scenario['name']}:\")\n    print(f\"  Daily: ${costs['daily_cost']:.2f}\")\n    print(f\"  Monthly: ${costs['monthly_cost']:.2f}\")\n    print(f\"  Annual: ${costs['annual_cost']:.2f}\")\n</code></pre>"},{"location":"lab6/projects/devduck/cerebras-interaction/#best-practices-and-tips","title":"Best Practices and Tips","text":""},{"location":"lab6/projects/devduck/cerebras-interaction/#optimization-strategies","title":"\ud83c\udfaf Optimization Strategies","text":"<ol> <li>Query Design:</li> <li>Be specific and clear in your prompts</li> <li>Use structured formats for complex requests</li> <li> <p>Provide context but avoid redundancy</p> </li> <li> <p>Model Selection:</p> </li> <li>Use 8B model for quick analysis</li> <li>Use 70B model for complex reasoning</li> <li> <p>Reserve 405B model for research-level tasks</p> </li> <li> <p>Error Handling:</p> </li> <li>Implement retries with exponential backoff</li> <li>Have fallback to Local Agent for failures</li> <li> <p>Monitor API rate limits</p> </li> <li> <p>Performance Tuning:</p> </li> <li>Cache common responses</li> <li>Batch similar queries when possible</li> <li>Monitor response times and adjust timeouts</li> </ol>"},{"location":"lab6/projects/devduck/cerebras-interaction/#common-pitfalls-to-avoid","title":"\ud83d\udea8 Common Pitfalls to Avoid","text":"<ul> <li>Over-reliance: Don't route every query to Cerebras</li> <li>Context Bloat: Avoid sending unnecessary context</li> <li>Poor Error Handling: Always handle API failures gracefully</li> <li>Ignoring Rate Limits: Respect API quotas and limits</li> <li>Cost Blindness: Monitor token usage and costs</li> </ul>"},{"location":"lab6/projects/devduck/cerebras-interaction/#next-steps","title":"Next Steps","text":"<p>Excellent! You've mastered:</p> <ul> <li>\u2705 Cerebras AI capabilities and model selection</li> <li>\u2705 Complex analysis and reasoning tasks</li> <li>\u2705 Performance monitoring and optimization</li> <li>\u2705 Cost management and token usage</li> <li>\u2705 Best practices for cloud AI integration</li> </ul> <p>In the next section, you'll learn about agent routing and communication patterns - how DevDuck decides which agent to use and how agents can work together on complex multi-step tasks.</p> <p>Ready to explore intelligent agent coordination? Let's dive in! \ud83e\udd16\ud83d\udd04</p>"},{"location":"lab6/projects/devduck/getting-started/","title":"Getting Started with DevDuck Agents","text":"<p>Now that you've understood how Docker Model Runner and MCP Gateway works, let's try building DevDuck Multi-Agent application! This section will guide you through the complete setup process, from cloning the repository to accessing your first multi-agent system</p> <p>DevDuck is a multi-agent system for Node.js programming assistance built with Google Agent Development Kit (ADK). This project features a coordinating agent (DevDuck) that manages two specialized sub-agents (Local Agent and Cerebras) for different programming tasks.</p>"},{"location":"lab6/projects/devduck/getting-started/#architecture","title":"Architecture","text":"<p>The system consists of three main agents orchestrated by Docker Compose, which plays a primordial role in launching and coordinating all agent services:</p>"},{"location":"lab6/projects/devduck/getting-started/#docker-compose-orchestration","title":"\ud83d\udc19 Docker Compose Orchestration","text":"<ul> <li>Central Role: Docker Compose serves as the foundation for the entire multi-agent system</li> <li>Service Orchestration: Manages the lifecycle of all three agents (DevDuck, Local Agent, and Cerebras)</li> <li>Configuration Management: Defines agent prompts, model configurations, and service dependencies   directly in the compose file</li> <li>Network Coordination: Establishes secure inter-agent communication channels</li> <li>Environment Management: Handles API keys, model parameters, and runtime configurations</li> </ul>"},{"location":"lab6/projects/devduck/getting-started/#agent-components","title":"Agent Components","text":""},{"location":"lab6/projects/devduck/getting-started/#devduck-main-agent","title":"\ud83e\udd86 DevDuck (Main Agent)","text":"<ul> <li>Role: Main development assistant and project coordinator</li> <li>Model: Jan-Nano (<code>hf.co/menlo/jan-nano-gguf:q4_k_m</code>)</li> <li>Capabilities: Routes requests to appropriate sub-agents based on user needs</li> </ul>"},{"location":"lab6/projects/devduck/getting-started/#local-agent-agent","title":"\ud83d\udc68\u200d\ud83d\udcbb Local Agent Agent","text":"<ul> <li>Role: General development tasks and project coordination</li> <li>Model:  Lucy (<code>hf.co/menlo/lucy-gguf:q8_0</code>)</li> <li>Specialization: Node.js programming expert for understanding code, explaining concepts, and generating code snippets</li> </ul>"},{"location":"lab6/projects/devduck/getting-started/#cerebras-agent","title":"\ud83e\udde0 Cerebras Agent","text":"<ul> <li>Role: Advanced computational tasks and complex problem-solving</li> <li>Model: qwen-3-235b-a22b-instruct-2507</li> <li>Provider: Cerebras API</li> <li>Specialization: Node.js programming expert for complex problem-solving scenarios</li> </ul>"},{"location":"lab6/projects/devduck/getting-started/#step-1-repository-setup","title":"Step 1: Repository Setup","text":"<p>First, let's clone the repository and explore its structure:</p>"},{"location":"lab6/projects/devduck/getting-started/#clone-the-repository","title":"Clone the Repository","text":"<pre><code># Clone the Docker Cerebras demo repository\ngit clone https://github.com/dockersamples/docker-cerebras-demo.git\n\n# Navigate to the project directory\ncd docker-cerebras-demo\n\n# List the contents to familiarize yourself\nls -la\n</code></pre>"},{"location":"lab6/projects/devduck/getting-started/#component-breakdown","title":"Component Breakdown","text":""},{"location":"lab6/projects/devduck/getting-started/#devduck-orchestrator","title":"\ud83c\udfbc DevDuck Orchestrator","text":"<ul> <li>Role: Central coordinator and decision maker</li> <li>Responsibilities: </li> <li>Route user requests to appropriate agents</li> <li>Manage conversation context and history</li> <li>Handle multi-agent workflows</li> <li>Provide unified response interface</li> <li>Technology: Python, FastAPI</li> <li>Port: 8000</li> </ul>"},{"location":"lab6/projects/devduck/getting-started/#local-agent","title":"\ud83e\udd16 Local Agent","text":"<ul> <li>Role: Fast, local processing for common tasks</li> <li>Responsibilities:</li> <li>Handle simple queries and code completion</li> <li>Provide quick responses for development tasks</li> <li>Cache frequently used information</li> <li>Fallback processing when cloud services are unavailable</li> <li>Technology: Python with local ML models</li> <li>Models: Lightweight language models (1-7B parameters)</li> </ul>"},{"location":"lab6/projects/devduck/getting-started/#cerebras-agent_1","title":"\ud83e\udde0 Cerebras Agent","text":"<ul> <li>Role: Advanced AI processing for complex tasks</li> <li>Responsibilities:</li> <li>Complex code analysis and architecture suggestions</li> <li>Advanced problem-solving and reasoning</li> <li>Large-scale text processing and generation</li> <li>Research and documentation tasks</li> <li>Technology: Python client for Cerebras API</li> <li>Models: High-performance cloud models (70B+ parameters)</li> </ul>"},{"location":"lab6/projects/devduck/getting-started/#mcp-gateway","title":"\ud83c\udf10 MCP Gateway","text":"<ul> <li>Role: Enhanced protocol support and tool integration</li> <li>Responsibilities:</li> <li>Extend agent capabilities with tools and plugins</li> <li>Handle file system operations</li> <li>Provide additional context and memory</li> <li>Enable advanced agent interactions</li> <li>Technology: Docker's MCP Gateway service</li> </ul>"},{"location":"lab6/projects/devduck/getting-started/#communication-flow","title":"Communication Flow","text":""},{"location":"lab6/projects/devduck/getting-started/#request-processing-flow","title":"\ud83d\udce1 Request Processing Flow","text":"<ol> <li>User Input: User sends request via web interface</li> <li>Orchestration: DevDuck analyzes request and determines routing</li> <li>Agent Selection: Based on complexity and requirements:</li> <li>Simple queries \u2192 Local Agent</li> <li>Complex analysis \u2192 Cerebras Agent  </li> <li>Multi-step tasks \u2192 Both agents in sequence</li> <li>Processing: Selected agent(s) process the request</li> <li>Response: DevDuck aggregates and returns unified response</li> </ol>"},{"location":"lab6/projects/devduck/getting-started/#environment-configuration","title":"Environment Configuration","text":""},{"location":"lab6/projects/devduck/getting-started/#required-environment-variables","title":"Required Environment Variables","text":"<p>You'll need to configure these environment variables:</p> <pre><code># Cerebras Configuration\nCEREBRAS_API_KEY=your_cerebras_api_key\nCEREBRAS_BASE_URL=https://api.cerebras.ai/v1\nCEREBRAS_CHAT_MODEL=qwen-3-235b-a22b-instruct-2507\n</code></pre>"},{"location":"lab6/projects/devduck/getting-started/#examine-key-files","title":"\ud83d\udd0d Examine Key Files","text":"<p>Let's look at the main configuration files:</p> <pre><code># View the Docker Compose configuration\ncat compose.yml\n\n# Examine the main agent entry point\ncat agents/main.py\n\n# Check the agent requirements\ncat agents/requirements.txt\n</code></pre>"},{"location":"lab6/projects/devduck/getting-started/#docker-image-preparation","title":"Docker Image Preparation","text":"<p>Before deploying, let's understand what Docker will build:</p>"},{"location":"lab6/projects/devduck/getting-started/#examine-the-dockerfile","title":"\ud83d\udc33 Examine the Dockerfile","text":"<pre><code># View the agent Dockerfile\ncat agents/Dockerfile\n</code></pre> <p>Understanding the Build Process: - Base image with Python runtime - Agent dependencies installation - Application code copying - Container startup configuration</p>"},{"location":"lab6/projects/devduck/getting-started/#step-2-system-deployment","title":"Step 2. System Deployment","text":"<p>Now let's deploy the complete multi-agent system:</p>"},{"location":"lab6/projects/devduck/getting-started/#initial-deployment","title":"\ud83d\ude80 Initial Deployment","text":"<pre><code># Deploy all services\ndocker compose up\n</code></pre> <p>What to expect during first deployment:</p> <ol> <li>Image Building (2-3 minutes)</li> <li>Base image download</li> <li>Python dependencies installation</li> <li> <p>Application code integration</p> </li> <li> <p>Service Initialization (1-2 minutes)</p> </li> <li>Container startup</li> <li>Model downloads (if using local models)</li> <li> <p>Service health checks</p> </li> <li> <p>Network Setup</p> </li> <li>Internal network creation</li> <li>Port mapping configuration</li> <li>Inter-service communication setup</li> </ol>"},{"location":"lab6/projects/devduck/getting-started/#development-mode-deployment","title":"\ud83d\udd04 Development Mode Deployment","text":"<p>For active development with code changes:</p> <pre><code># Run in detached mode (background)\ndocker compose up -d\n\n# Build and restart when code changes\ndocker compose up --build\n</code></pre>"},{"location":"lab6/projects/devduck/getting-started/#step-3-deployment-verification","title":"Step 3: Deployment Verification","text":""},{"location":"lab6/projects/devduck/getting-started/#check-service-status","title":"\u2705 Check Service Status","text":"<pre><code># Verify all containers are running\ndocker compose ps\n</code></pre> <p>Expected Output:</p> <pre><code>NAME                                   IMAGE                               COMMAND                  SERVICE         CREATED         STATUS         PORTS\ndocker-cerebras-demo-devduck-agent-1   docker-cerebras-demo-devduck-agent  \"sh -c 'uvicorn main\u2026\"   devduck-agent   2 minutes ago   Up 2 minutes   0.0.0.0:8000-&gt;8000/tcp\ndocker-cerebras-demo-mcp-gateway-1     docker/mcp-gateway:latest           \"/docker-mcp gateway\u2026\"   mcp-gateway     2 minutes ago   Up 2 minutes   \n</code></pre>"},{"location":"lab6/projects/devduck/getting-started/#health-check","title":"\ud83e\ude7a Health Check","text":"<p>Perform a comprehensive health check:</p> <pre><code># Check container resource usage\ndocker stats --no-stream\n\n# Verify agent endpoints\ncurl -s http://localhost:8000/health | jq .\n</code></pre>"},{"location":"lab6/projects/devduck/getting-started/#step-4-first-access","title":"Step 4: First Access","text":""},{"location":"lab6/projects/devduck/getting-started/#access-the-web-interface","title":"\ud83c\udf10 Access the Web Interface","text":"<p>Open your web browser and navigate to:</p> <p>Primary URL: http://localhost:8000/dev-ui/?app=devduck</p> <p>Alternative URLs (if the primary doesn't work): - http://0.0.0.0:8000/dev-ui/?app=devduck - http://127.0.0.1:8000/dev-ui/?app=devduck</p>"},{"location":"lab6/projects/devduck/getting-started/#step-5-system-validation","title":"Step 5: System Validation","text":""},{"location":"lab6/projects/devduck/getting-started/#comprehensive-testing","title":"\ud83d\udd0d Comprehensive Testing","text":"<p>Run through this validation checklist:</p>"},{"location":"lab6/projects/devduck/getting-started/#basic-functionality","title":"Basic Functionality","text":"<ul> <li>[ ] Web interface loads without errors</li> <li>[ ] Can send and receive messages</li> <li>[ ] DevDuck orchestrator responds appropriately</li> <li>[ ] No error messages in container logs</li> </ul>"},{"location":"lab6/projects/devduck/getting-started/#success-confirmation","title":"Success Confirmation","text":"<p>If everything is working correctly, you should see:</p> <p>\u2705 Web Interface: DevDuck interface loads at http://localhost:8000/dev-ui/?app=devduck \u2705 Agent Response: Messages sent receive appropriate responses \u2705 Container Status: All containers show \"Up\" status \u2705 Resource Usage: System resources are stable \u2705 Logs: No critical errors in application logs  </p>"},{"location":"lab6/projects/devduck/getting-started/#next-steps","title":"Next Steps","text":"<p>Excellent! You now have a fully functional multi-agent system running. In the next section, you'll dive deeper into environment configuration and learn how to customize your deployment for different scenarios.</p> <p>Ready to explore the environment setup and deployment options? Let's continue! \ud83d\ude80</p>"},{"location":"lab6/projects/devduck/local-agent/","title":"Local Agent Tasks","text":"<p>The Local Agent is your system's fast-response specialist, designed for quick processing tasks, code completion, and immediate assistance. In this lab, you'll explore its capabilities, learn optimization techniques, and understand when to leverage local processing versus cloud-based AI.</p> <p>!!! info \"Learning Focus\"     Master local agent capabilities, model management, and performance optimization for immediate response scenarios.</p>"},{"location":"lab6/projects/devduck/local-agent/#understanding-the-local-agent","title":"Understanding the Local Agent","text":""},{"location":"lab6/projects/devduck/local-agent/#local-agent-architecture","title":"\ud83c\udfe0 Local Agent Architecture","text":"<p>The Local Agent operates within your Docker container using:</p> <ul> <li>Local Models: Cached language models (1-7B parameters)</li> <li>Fast Processing: Sub-second response times for common tasks</li> <li>Offline Capability: Works without internet connectivity</li> <li>Resource Efficient: Optimized for container environments</li> </ul>"},{"location":"lab6/projects/devduck/local-agent/#key-capabilities","title":"Key Capabilities","text":""},{"location":"lab6/projects/devduck/local-agent/#strengths","title":"\u2705 Strengths","text":"<ul> <li>Code completion and simple explanations</li> <li>Quick syntax help and debugging</li> <li>Basic conversation and Q&amp;A</li> <li>Template generation and boilerplate code</li> <li>Fast pattern matching and simple analysis</li> </ul>"},{"location":"lab6/projects/devduck/local-agent/#limitations","title":"\u26a0\ufe0f Limitations","text":"<ul> <li>Limited context window (compared to cloud models)</li> <li>Reduced knowledge depth</li> <li>Less sophisticated reasoning</li> <li>Smaller training data scope</li> </ul>"},{"location":"lab6/projects/devduck/local-agent/#quick-test","title":"Quick Test","text":"<pre><code>Prompt 1: How many agents are available in this system?\n</code></pre> <pre><code>Prompt 2: Can I talk to Local Agent?\n</code></pre> <pre><code>Prompt 3: Show me code for todo list app\n</code></pre>"},{"location":"lab6/projects/devduck/local-agent/#hands-on-local-agent-exploration","title":"Hands-On Local Agent Exploration","text":""},{"location":"lab6/projects/devduck/local-agent/#exercise-1-direct-local-agent-testing","title":"\ud83e\uddea Exercise 1: Direct Local Agent Testing","text":"<p>Let's interact directly with the Local Agent to understand its capabilities:</p>"},{"location":"lab6/projects/devduck/local-agent/#force-local-agent-routing","title":"Force Local Agent Routing","text":"<p>Method 1: Using Query Patterns</p> <p>The Local Agent typically handles these types of queries:</p> <pre><code>You: Quick JavaScript syntax question - how do I create an arrow function?\n</code></pre> <pre><code>You: Simple Python loop example please\n</code></pre> <pre><code>You: Basic HTML template for a webpage\n</code></pre>"},{"location":"lab6/projects/devduck/local-agent/#method-2-simulate-offline-mode","title":"Method 2: Simulate Offline Mode","text":"<p>Temporarily disable internet to force local processing:</p> <pre><code># Block external API access (temporary)\ndocker compose exec devduck-agent sh -c \"echo '127.0.0.1 api.cerebras.ai' &gt;&gt; /etc/hosts\"\n\n# Now test with complex queries\n# They should fallback to Local Agent\n</code></pre> <p>Test Query:</p> <pre><code>You: Explain object-oriented programming concepts\n</code></pre> <p>Restore connectivity:</p> <pre><code>docker compose exec devduck-agent sh -c \"sed -i '/api.cerebras.ai/d' /etc/hosts\"\n</code></pre>"},{"location":"lab6/projects/devduck/local-agent/#exercise-2-local-agent-performance-analysis","title":"\ud83d\udd0d Exercise 2: Local Agent Performance Analysis","text":""},{"location":"lab6/projects/devduck/local-agent/#response-time-comparison","title":"Response Time Comparison","text":"<pre><code># Create local_agent_benchmark.py\nimport requests\nimport time\nimport statistics\n\ndef benchmark_local_agent():\n    \"\"\"Benchmark Local Agent response times.\"\"\"\n\n    # Queries likely to route to Local Agent\n    local_queries = [\n        \"Hello\",\n        \"What is a variable in Python?\",\n        \"Show me a basic for loop\",\n        \"Simple HTML template\",\n        \"How to declare a JavaScript function?\"\n    ]\n\n    times = []\n\n    print(\"\ud83c\udfe0 Benchmarking Local Agent...\")\n\n    for query in local_queries:\n        print(f\"Testing: {query[:30]}...\")\n\n        start_time = time.time()\n\n        response = requests.post(\n            'http://localhost:8000/chat',\n            json={\n                \"message\": query,\n                \"conversation_id\": f\"benchmark-{int(time.time())}\",\n                \"force_local\": True  # If supported\n            },\n            timeout=30\n        )\n\n        duration = time.time() - start_time\n        times.append(duration)\n\n        print(f\"  Response time: {duration:.3f}s\")\n        time.sleep(0.5)\n\n    # Statistics\n    avg_time = statistics.mean(times)\n    median_time = statistics.median(times)\n    min_time = min(times)\n    max_time = max(times)\n\n    print(f\"\\n\ud83d\udcca Local Agent Performance:\")\n    print(f\"  Average: {avg_time:.3f}s\")\n    print(f\"  Median:  {median_time:.3f}s\")\n    print(f\"  Min:     {min_time:.3f}s\")\n    print(f\"  Max:     {max_time:.3f}s\")\n\n    return times\n\nif __name__ == '__main__':\n    benchmark_local_agent()\n</code></pre> <p>Run the benchmark:</p> <pre><code>python local_agent_benchmark.py\n</code></pre>"},{"location":"lab6/projects/devduck/local-agent/#exercise-3-local-agent-specialization","title":"\ud83c\udfaf Exercise 3: Local Agent Specialization","text":""},{"location":"lab6/projects/devduck/local-agent/#code-generation-tasks","title":"Code Generation Tasks","text":"<p>Test the Local Agent's code generation capabilities:</p> <p>Template Generation:</p> <pre><code>You: Generate a basic Express.js server template\n</code></pre> <p>Simple Functions:</p> <pre><code>You: Create a function to validate an email address in JavaScript\n</code></pre> <p>Quick Fixes:</p> <pre><code>You: Fix this Python code: \nprint(\"Hello World\"\n</code></pre>"},{"location":"lab6/projects/devduck/local-agent/#documentation-and-explanations","title":"Documentation and Explanations","text":"<p>Concept Explanations:</p> <pre><code>You: Explain what is a REST API in simple terms\n</code></pre> <p>Quick References:</p> <pre><code>You: List common Git commands\n</code></pre>"},{"location":"lab6/projects/devduck/local-agent/#local-model-management","title":"Local Model Management","text":""},{"location":"lab6/projects/devduck/local-agent/#model-configuration","title":"\ud83d\udce6 Model Configuration","text":""},{"location":"lab6/projects/devduck/local-agent/#viewing-current-model","title":"Viewing Current Model","text":"<pre><code># Check current local model configuration\ndocker compose exec devduck-agent env | grep LOCAL_MODEL\n\n# View model cache directory\ndocker compose exec devduck-agent ls -la /app/models/\n\n# Check model size and usage\ndocker compose exec devduck-agent du -sh /app/models/*\n</code></pre>"},{"location":"lab6/projects/devduck/local-agent/#model-switching","title":"Model Switching","text":"<p>You can experiment with different local models:</p> <pre><code># Stop the current system\ndocker compose down\n\n# Edit your .env file with different model\ncat &gt; .env.local-model &lt;&lt; 'EOF'\n# Lightweight model (faster, less capable)\nLOCAL_MODEL_NAME=microsoft/DialoGPT-small\n\n# Medium model (balanced)\nLOCAL_MODEL_NAME=microsoft/DialoGPT-medium\n\n# Larger model (slower, more capable)\nLOCAL_MODEL_NAME=microsoft/DialoGPT-large\n\n# Alternative: GPT-2 based models\nLOCAL_MODEL_NAME=gpt2\nEOF\n\n# Update your main .env file\ncp .env.local-model .env\n\n# Restart with new model\ndocker compose up --build\n</code></pre>"},{"location":"lab6/projects/devduck/local-agent/#performance-tuning","title":"\ud83d\udd27 Performance Tuning","text":""},{"location":"lab6/projects/devduck/local-agent/#memory-optimization","title":"Memory Optimization","text":"<pre><code># Add to compose.yml for local agent optimization\nservices:\n  devduck-agent:\n    environment:\n      # Model optimization settings\n      - TORCH_HOME=/app/models\n      - HF_HOME=/app/models\n      - TRANSFORMERS_CACHE=/app/models\n      - LOCAL_MODEL_DEVICE=auto\n      - LOCAL_MODEL_PRECISION=fp16  # Use half precision\n      - LOCAL_MODEL_MAX_LENGTH=512  # Limit context length\n    deploy:\n      resources:\n        limits:\n          memory: 6G  # Adjust based on model size\n        reservations:\n          memory: 2G\n</code></pre>"},{"location":"lab6/projects/devduck/local-agent/#cpu-optimization","title":"CPU Optimization","text":"<pre><code># Create optimization script\ncat &gt; optimize_local_agent.py &lt;&lt; 'EOF'\n#!/usr/bin/env python3\nimport os\nimport torch\n\ndef optimize_local_performance():\n    \"\"\"Apply local agent performance optimizations.\"\"\"\n\n    # Set optimal thread count for PyTorch\n    torch.set_num_threads(min(4, os.cpu_count()))\n\n    # Enable optimized attention (if available)\n    if hasattr(torch.backends, 'cuda'):\n        torch.backends.cuda.matmul.allow_tf32 = True\n\n    # Set memory growth (if using GPU)\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        print(\"\u2705 CUDA optimizations applied\")\n\n    print(\"\u2705 CPU optimizations applied\")\n    print(f\"\ud83d\udcca Using {torch.get_num_threads()} threads\")\n\nif __name__ == '__main__':\n    optimize_local_performance()\nEOF\n\n# Run optimization\ndocker compose exec devduck-agent python optimize_local_agent.py\n</code></pre>"},{"location":"lab6/projects/devduck/local-agent/#advanced-local-agent-use-cases","title":"Advanced Local Agent Use Cases","text":""},{"location":"lab6/projects/devduck/local-agent/#real-time-code-assistance","title":"\ud83d\ude80 Real-time Code Assistance","text":""},{"location":"lab6/projects/devduck/local-agent/#exercise-4-interactive-development-helper","title":"Exercise 4: Interactive Development Helper","text":"<p>Create scenarios where the Local Agent acts as a real-time coding assistant:</p> <p>Scenario 1: Syntax Checking</p> <pre><code>You: Is this JavaScript code syntactically correct?\n\nfunction calculateTotal(items) {\n  let total = 0;\n  for (item of items) {\n    total += item.price\n  }\n  return total;\n}\n</code></pre> <p>Scenario 2: Quick Refactoring</p> <pre><code>You: Convert this to arrow function syntax:\n\nfunction greet(name) {\n  return \"Hello, \" + name;\n}\n</code></pre> <p>Scenario 3: Error Explanation</p> <pre><code>You: What's wrong with this Python code?\n\ndef process_list(items):\n    for i in range(len(items))\n        print(items[i])\n</code></pre>"},{"location":"lab6/projects/devduck/local-agent/#template-and-boilerplate-generation","title":"\ud83c\udfaf Template and Boilerplate Generation","text":""},{"location":"lab6/projects/devduck/local-agent/#exercise-5-rapid-prototyping","title":"Exercise 5: Rapid Prototyping","text":"<p>API Endpoint Template:</p> <pre><code>You: Create a basic Express.js API endpoint for user registration\n</code></pre> <p>Database Schema:</p> <pre><code>You: Generate a MongoDB schema for a blog post\n</code></pre> <p>Configuration Files:</p> <pre><code>You: Create a package.json for a Node.js REST API project\n</code></pre>"},{"location":"lab6/projects/devduck/local-agent/#documentation-and-learning-aid","title":"\ud83d\udcda Documentation and Learning Aid","text":""},{"location":"lab6/projects/devduck/local-agent/#exercise-6-learning-assistant","title":"Exercise 6: Learning Assistant","text":"<p>Concept Clarification:</p> <pre><code>You: Explain the difference between let, const, and var in JavaScript\n</code></pre> <p>Quick References:</p> <pre><code>You: Show me common array methods in JavaScript with examples\n</code></pre> <p>Best Practices:</p> <pre><code>You: What are Python naming conventions for variables and functions?\n</code></pre>"},{"location":"lab6/projects/devduck/local-agent/#local-agent-customization","title":"Local Agent Customization","text":""},{"location":"lab6/projects/devduck/local-agent/#custom-prompts-and-templates","title":"\ud83d\udee0\ufe0f Custom Prompts and Templates","text":""},{"location":"lab6/projects/devduck/local-agent/#create-specialized-prompts","title":"Create Specialized Prompts","text":"<pre><code># Create local_agent_templates.py\nclass LocalAgentTemplates:\n    \"\"\"Custom templates for Local Agent specialization.\"\"\"\n\n    CODE_REVIEW_PROMPT = \"\"\"\n    You are a code review assistant. For the given code:\n    1. Check for syntax errors\n    2. Suggest improvements\n    3. Identify potential bugs\n    4. Keep responses concise and actionable\n    \"\"\"\n\n    QUICK_HELP_PROMPT = \"\"\"\n    Provide quick, practical help. Include:\n    1. Direct answer to the question\n    2. Simple code example if applicable\n    3. One-line explanation\n    No lengthy explanations needed.\n    \"\"\"\n\n    TEMPLATE_GENERATOR_PROMPT = \"\"\"\n    Generate clean, minimal code templates.\n    Include:\n    1. Essential structure only\n    2. Clear placeholder comments\n    3. Basic error handling\n    4. Standard naming conventions\n    \"\"\"\n\n# Usage example\ndef use_specialized_prompt(query, template_type=\"quick_help\"):\n    templates = LocalAgentTemplates()\n    prompt = getattr(templates, f\"{template_type.upper()}_PROMPT\")\n\n    return f\"{prompt}\\n\\nQuery: {query}\"\n</code></pre>"},{"location":"lab6/projects/devduck/local-agent/#performance-monitoring","title":"\ud83d\udcca Performance Monitoring","text":""},{"location":"lab6/projects/devduck/local-agent/#create-local-agent-dashboard","title":"Create Local Agent Dashboard","text":"<pre><code># Create local_agent_monitor.py\nimport psutil\nimport time\nimport json\n\nclass LocalAgentMonitor:\n    def __init__(self):\n        self.start_time = time.time()\n        self.request_count = 0\n        self.response_times = []\n\n    def log_request(self, duration):\n        self.request_count += 1\n        self.response_times.append(duration)\n\n    def get_stats(self):\n        uptime = time.time() - self.start_time\n\n        return {\n            \"uptime_seconds\": uptime,\n            \"total_requests\": self.request_count,\n            \"avg_response_time\": sum(self.response_times) / len(self.response_times) if self.response_times else 0,\n            \"requests_per_minute\": (self.request_count / uptime) * 60 if uptime &gt; 0 else 0,\n            \"memory_usage_mb\": psutil.Process().memory_info().rss / 1024 / 1024,\n            \"cpu_percent\": psutil.cpu_percent()\n        }\n\n    def print_stats(self):\n        stats = self.get_stats()\n        print(\"\ud83c\udfe0 Local Agent Statistics:\")\n        print(f\"   Uptime: {stats['uptime_seconds']:.0f}s\")\n        print(f\"   Requests: {stats['total_requests']}\")\n        print(f\"   Avg Response: {stats['avg_response_time']:.3f}s\")\n        print(f\"   Rate: {stats['requests_per_minute']:.1f} req/min\")\n        print(f\"   Memory: {stats['memory_usage_mb']:.1f} MB\")\n        print(f\"   CPU: {stats['cpu_percent']:.1f}%\")\n\n# Usage\nmonitor = LocalAgentMonitor()\n\n# Simulate monitoring\nfor _ in range(10):\n    start = time.time()\n    # Simulate request processing\n    time.sleep(0.1)\n    monitor.log_request(time.time() - start)\n\nmonitor.print_stats()\n</code></pre>"},{"location":"lab6/projects/devduck/local-agent/#optimization-strategies","title":"Optimization Strategies","text":""},{"location":"lab6/projects/devduck/local-agent/#configuration-tuning","title":"\ud83c\udf9b\ufe0f Configuration Tuning","text":""},{"location":"lab6/projects/devduck/local-agent/#model-selection-guidelines","title":"Model Selection Guidelines","text":"Use Case Recommended Model Response Time Memory Usage Quick Help DialoGPT-small &lt;0.5s 1-2 GB Code Completion DialoGPT-medium 1-2s 2-4 GB Detailed Explanations DialoGPT-large 2-5s 4-6 GB Specialized Tasks Custom fine-tuned Variable Variable"},{"location":"lab6/projects/devduck/local-agent/#performance-optimization-checklist","title":"Performance Optimization Checklist","text":"<pre><code># Create optimization checklist script\ncat &gt; local_agent_optimization.sh &lt;&lt; 'EOF'\n#!/bin/bash\n\necho \"\ud83d\udd27 Local Agent Optimization Checklist\"\necho \"=====================================\"\n\n# Check 1: Memory availability\nMEM_TOTAL=$(free -m | awk 'NR==2{print $2}')\nMEM_AVAILABLE=$(free -m | awk 'NR==2{print $7}')\necho \"Memory: ${MEM_AVAILABLE}MB / ${MEM_TOTAL}MB available\"\n\nif [ $MEM_AVAILABLE -lt 4000 ]; then\n    echo \"\u26a0\ufe0f  Consider using smaller model (DialoGPT-small)\"\nelse\n    echo \"\u2705 Memory sufficient for medium/large models\"\nfi\n\n# Check 2: CPU cores\nCPU_CORES=$(nproc)\necho \"CPU Cores: $CPU_CORES\"\n\nif [ $CPU_CORES -lt 4 ]; then\n    echo \"\u26a0\ufe0f  Limited CPU - optimize thread count\"\nelse\n    echo \"\u2705 CPU suitable for multi-threading\"\nfi\n\n# Check 3: Disk space for models\nDISK_SPACE=$(df -BG /app/models 2&gt;/dev/null | awk 'NR==2{print $4}' | sed 's/G//')\necho \"Disk Space: ${DISK_SPACE}GB available for models\"\n\nif [ \"$DISK_SPACE\" -lt 5 ]; then\n    echo \"\u26a0\ufe0f  Limited disk space - clean old models\"\nelse\n    echo \"\u2705 Sufficient disk space\"\nfi\n\n# Check 4: Model loading time\necho \"Testing model loading time...\"\nSTART_TIME=$(date +%s)\ndocker compose exec devduck-agent python -c \"import torch; print('Model check complete')\" 2&gt;/dev/null\nEND_TIME=$(date +%s)\nLOAD_TIME=$((END_TIME - START_TIME))\n\necho \"Model load test: ${LOAD_TIME}s\"\n\nif [ $LOAD_TIME -gt 30 ]; then\n    echo \"\u26a0\ufe0f  Slow model loading - consider smaller model\"\nelse\n    echo \"\u2705 Model loading performance acceptable\"\nfi\n\nEOF\n\nchmod +x local_agent_optimization.sh\n./local_agent_optimization.sh\n</code></pre>"},{"location":"lab6/projects/devduck/local-agent/#caching-strategies","title":"\ud83d\ude80 Caching Strategies","text":""},{"location":"lab6/projects/devduck/local-agent/#implement-response-caching","title":"Implement Response Caching","text":"<pre><code># Create local_cache_manager.py\nimport hashlib\nimport json\nimport time\nfrom typing import Optional, Dict, Any\n\nclass LocalResponseCache:\n    def __init__(self, max_size: int = 1000, ttl: int = 300):\n        self.cache: Dict[str, Dict[str, Any]] = {}\n        self.max_size = max_size\n        self.ttl = ttl\n\n    def _generate_key(self, query: str) -&gt; str:\n        \"\"\"Generate cache key from query.\"\"\"\n        return hashlib.md5(query.lower().strip().encode()).hexdigest()\n\n    def get(self, query: str) -&gt; Optional[str]:\n        \"\"\"Get cached response if available and valid.\"\"\"\n        key = self._generate_key(query)\n\n        if key not in self.cache:\n            return None\n\n        entry = self.cache[key]\n\n        # Check TTL\n        if time.time() - entry['timestamp'] &gt; self.ttl:\n            del self.cache[key]\n            return None\n\n        entry['hits'] += 1\n        return entry['response']\n\n    def set(self, query: str, response: str) -&gt; None:\n        \"\"\"Cache a response.\"\"\"\n        # Evict oldest entries if cache is full\n        if len(self.cache) &gt;= self.max_size:\n            oldest_key = min(self.cache.keys(), \n                           key=lambda k: self.cache[k]['timestamp'])\n            del self.cache[oldest_key]\n\n        key = self._generate_key(query)\n        self.cache[key] = {\n            'response': response,\n            'timestamp': time.time(),\n            'hits': 0\n        }\n\n    def stats(self) -&gt; Dict[str, Any]:\n        \"\"\"Get cache statistics.\"\"\"\n        total_hits = sum(entry['hits'] for entry in self.cache.values())\n\n        return {\n            'size': len(self.cache),\n            'max_size': self.max_size,\n            'total_hits': total_hits,\n            'hit_rate': total_hits / max(1, len(self.cache)),\n            'ttl': self.ttl\n        }\n\n# Example usage\ncache = LocalResponseCache()\n\n# Simulate caching\ntest_queries = [\n    \"What is Python?\",\n    \"Hello\",\n    \"Basic for loop example\"\n]\n\nfor query in test_queries:\n    # Check cache first\n    cached_response = cache.get(query)\n\n    if cached_response:\n        print(f\"\ud83d\udcbe Cache hit for: {query[:30]}...\")\n    else:\n        print(f\"\ud83d\udd0d Cache miss for: {query[:30]}...\")\n        # Simulate response generation\n        response = f\"Generated response for: {query}\"\n        cache.set(query, response)\n\nprint(f\"\\n\ud83d\udcca Cache stats: {cache.stats()}\")\n</code></pre>"},{"location":"lab6/projects/devduck/local-agent/#troubleshooting-local-agent-issues","title":"Troubleshooting Local Agent Issues","text":""},{"location":"lab6/projects/devduck/local-agent/#common-problems","title":"\ud83d\udc1b Common Problems","text":""},{"location":"lab6/projects/devduck/local-agent/#issue-1-model-loading-failures","title":"Issue 1: Model Loading Failures","text":"<p>Symptoms: - Container crashes during startup - \"Model not found\" errors - Memory allocation errors</p> <p>Diagnosis:</p> <pre><code># Check model download status\ndocker compose logs devduck-agent | grep -i \"model\"\n\n# Check disk space\ndocker compose exec devduck-agent df -h /app/models\n\n# Check memory usage during startup\ndocker stats devduck-agent\n</code></pre> <p>Solutions:</p> <pre><code># Use smaller model\nexport LOCAL_MODEL_NAME=microsoft/DialoGPT-small\n\n# Increase memory limit\n# Edit compose.yml:\n# deploy:\n#   resources:\n#     limits:\n#       memory: 8G\n\n# Pre-download model\ndocker compose exec devduck-agent python -c \"from transformers import AutoTokenizer, AutoModelForCausalLM; AutoTokenizer.from_pretrained('microsoft/DialoGPT-small'); AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-small')\"\n</code></pre>"},{"location":"lab6/projects/devduck/local-agent/#issue-2-slow-response-times","title":"Issue 2: Slow Response Times","text":"<p>Diagnosis:</p> <pre><code># Profile response times\ncurl -w \"@curl-format.txt\" -s -o /dev/null -X POST \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"message\":\"Hello\",\"conversation_id\":\"test\"}' \\\n  http://localhost:8000/chat\n\n# Create curl-format.txt\ncat &gt; curl-format.txt &lt;&lt; 'EOF'\ntime_namelookup:  %{time_namelookup}s\ntime_connect:     %{time_connect}s\ntime_appconnect:  %{time_appconnect}s\ntime_pretransfer: %{time_pretransfer}s\ntime_redirect:    %{time_redirect}s\ntime_starttransfer: %{time_starttransfer}s\ntime_total:       %{time_total}s\nEOF\n</code></pre> <p>Solutions: - Switch to smaller, faster model - Implement response caching - Optimize container resources - Use GPU acceleration if available</p>"},{"location":"lab6/projects/devduck/local-agent/#next-steps","title":"Next Steps","text":"<p>Excellent progress! You've mastered:</p> <ul> <li>\u2705 Local Agent capabilities and limitations</li> <li>\u2705 Model management and optimization</li> <li>\u2705 Performance monitoring and tuning</li> <li>\u2705 Caching strategies for improved response times</li> <li>\u2705 Troubleshooting common issues</li> </ul> <p>In the next section, you'll explore the Cerebras Agent and learn how to leverage cloud-based AI for complex analysis and advanced reasoning tasks.</p> <p>Ready to dive into cloud AI capabilities? Let's explore Cerebras! \ud83e\udde0\ud83d\ude80</p> <p>!!! success \"Local Agent Mastery\"     You now understand how to optimize and utilize the Local Agent for fast, efficient processing. The combination of local and cloud agents gives you the best of both worlds - speed and sophistication!</p>"},{"location":"lab6/projects/devduck/overview/","title":"Overview","text":""},{"location":"lab6/projects/devduck/overview/#introduction","title":"Introduction","text":"<p>\ud83d\udc4b Welcome to the Docker DevDuck Multi-Agent Workshop! During this workshop, you'll learn how to build and deploy a multi-agent system using Docker, Google Agent Development Kit (ADK), and Cerebras AI.</p>"},{"location":"lab6/projects/devduck/overview/#the-devduck-system-architecture","title":"The DevDuck System Architecture","text":"<p>In this workshop, you'll build a system called \"DevDuck\" - a developer assistance platform that uses multiple specialized agents:</p> <p></p>"},{"location":"lab6/projects/devduck/overview/#agent-roles","title":"Agent Roles","text":"<ol> <li>DevDuck Orchestrator: The main coordinator that routes requests to appropriate agents</li> <li>Local Agent: Handles quick processing tasks using local models</li> <li>Cerebras Agent: Leverages powerful cloud-based AI for complex analysis</li> <li>MCP Gateway: Manages Model Context Protocol for enhanced capabilities</li> </ol>"},{"location":"lab6/projects/devduck/overview/#how-it-works","title":"How it works?","text":"<ul> <li>This application shows how secure AI coding agents are orchestrated through a multi-layered system.</li> <li>At the top, user requests flow through the User Interface into the DevDuck Orchestrator, which serves as the central decision-making hub.</li> <li> <p>The orchestrator manages three critical pathways:</p> <ul> <li>routing to a Local Agent (powered by local Qwen models for quick decisions),</li> <li>delegating complex tasks to the Cerebras Agent (utilizing high-performance cloud APIs), and</li> <li>coordinating tool access through the MCP Gateway.</li> </ul> </li> <li> <p>The bidirectional arrows between agents and the MCP Gateway show that this isn't just a simple pipeline - agents actively communicate with each other for routing decisions and continuously interact with tools to enhance their capabilities.</p> </li> <li>The MCP Gateway acts as a secure intermediary that manages two specialized MCP servers, each serving distinct purposes.</li> <li>The Context7 MCP Server handles documentation retrieval, connecting to external sources like framework documentation (React, Vue, Angular), API specifications (REST, GraphQL), and library documentation (npm, PyPI packages). This ensures agents have access to current, authoritative information when generating code.</li> <li>Meanwhile, the Node Sandbox MCP Server manages secure code execution by creating Isolated Docker Containers with disabled networking, providing a safe environment where generated code can run without security risks.</li> </ul> <p>The architecture's brilliance lies in its security-by-design approach: while Context7 can access the internet to fetch documentation, the sandbox containers are completely network-isolated, preventing any potential data exfiltration or malicious activity from generated code. This creates a perfect balance where AI agents can access up-to-date knowledge to write better code, while ensuring that code execution happens in a bulletproof secure environment - essentially giving you the benefits of current documentation and safe execution simultaneously.</p> <p></p>"},{"location":"lab6/projects/devduck/overview/#real-world-applications","title":"Real-World Applications","text":"<p>This workshop demonstrates practical use cases for multi-agent systems:</p>"},{"location":"lab6/projects/devduck/overview/#software-development-assistance","title":"\ud83d\udcbb Software Development Assistance","text":"<ul> <li>Code generation and explanation</li> <li>Bug detection and troubleshooting</li> <li>Architecture recommendations</li> <li>Testing strategy development</li> </ul>"},{"location":"lab6/projects/devduck/overview/#intelligent-task-routing","title":"\ud83c\udfaf Intelligent Task Routing","text":"<ul> <li>Determine which agent is best suited for each request</li> <li>Load balancing across multiple agents</li> <li>Fallback mechanisms for agent failures</li> </ul>"},{"location":"lab6/projects/devduck/overview/#scalable-ai-solutions","title":"\ud83d\udcc8 Scalable AI Solutions","text":"<ul> <li>Combine local and cloud AI capabilities</li> <li>Cost-effective resource utilization</li> <li>Performance optimization strategies</li> </ul>"},{"location":"lab6/projects/devduck/overview/#workshop-learning-objectives","title":"Workshop Learning Objectives","text":"<p>By the end of this workshop, you'll be able to:</p> <ul> <li>\u2705 Design multi-agent architectures using Docker containers</li> <li>\u2705 Implement agent communication patterns using HTTP APIs and message queues</li> <li>\u2705 Integrate multiple AI models (local and cloud-based) in a cohesive system</li> <li>\u2705 Deploy and manage multi-agent systems in production-like environments</li> <li>\u2705 Debug and troubleshoot common multi-agent system issues</li> <li>\u2705 Apply best practices for containerized AI applications</li> </ul>"},{"location":"lab6/projects/devduck/overview/#technology-stack","title":"Technology Stack","text":"<p>This workshop uses cutting-edge technologies:</p> <ul> <li>Docker &amp; Docker Compose: Container orchestration</li> <li>Python &amp; FastAPI: Agent implementation and web interfaces</li> <li>Cerebras AI: High-performance cloud AI models</li> <li>Google ADK: Agent development framework</li> <li>Model Context Protocol (MCP): Enhanced agent capabilities</li> </ul>"},{"location":"lab6/projects/devduck/overview/#getting-ready","title":"Getting Ready","text":"<p>Before proceeding to the next section, ensure you have:</p> <ul> <li>[ ] Docker Desktop installed and running</li> <li>[ ] Basic familiarity with command-line interfaces</li> <li>[ ] Text editor or IDE for viewing code</li> <li>[ ] Internet connection for downloading models and accessing APIs</li> </ul>"},{"location":"lab6/projects/devduck/overview/#next-steps","title":"Next Steps","text":"<p>Ready to dive in? The next section covers the prerequisites and system requirements needed to get started. You'll also get a detailed overview of the system architecture and how all the components work together.</p> <p>Let's build something amazing! \ud83d\ude80</p>"},{"location":"lab6/projects/devduck/prereq/","title":"Prerequisites &amp; System Overview","text":"<p>Before diving into the hands-on exercises, let's ensure your environment is properly set up and understand the system architecture we'll be working with.</p>"},{"location":"lab6/projects/devduck/prereq/#software-prerequisites","title":"Software Prerequisites","text":""},{"location":"lab6/projects/devduck/prereq/#required-software","title":"\u2705 Required Software","text":"<ol> <li>Docker Desktop (Latest version - 4.46.0)</li> <li>Windows: Docker Desktop for Windows</li> <li>macOS: Docker Desktop for Mac  </li> <li> <p>Linux: Docker Engine + Docker Compose</p> </li> <li> <p>Git (for cloning repositories)</p> </li> <li> <p>Web Browser (Chrome, Firefox, Safari, Edge)</p> </li> </ol>"},{"location":"lab6/projects/devduck/prereq/#verification-commands","title":"\ud83d\udd27 Verification Commands","text":"<p>Run these commands to verify your setup:</p> <pre><code># Check Docker version\ndocker --version\n\n# Check Docker Compose version\ndocker compose version\n\n# Verify Docker is running\ndocker ps\n\n# Check available system resources\ndocker system info\n</code></pre>"},{"location":"lab6/projects/devduck/prereq/#enable-docker-model-runner","title":"Enable Docker Model Runner","text":"<p>Open Docker Desktop Settings &gt; AI &gt; \u201cDocker Model Runner\u201d is enabled.</p>"},{"location":"lab6/projects/devduck/prereq/#download-ai-models","title":"Download AI models","text":"<pre><code>docker model pull ai/llama3.2:1B-Q8_0\ndocker model pull ai/mistral:7B-Q4_0\ndocker model pull hf.co/menlo/lucy-gguf:q8_0\ndocker model pull hf.co/menlo/jan-nano-gguf:q4_k_m\n\n</code></pre>"},{"location":"lab6/projects/devduck/prereq/#api-requirements","title":"API Requirements","text":""},{"location":"lab6/projects/devduck/prereq/#cerebras-api-setup","title":"Cerebras API Setup","text":"<ol> <li>Create Account: Visit https://cloud.cerebras.ai/</li> <li>Get API Key: Navigate to API section and generate a key</li> <li>Verify Access: Test API connectivity (we'll do this together)</li> </ol> <p>Note: Cerebras offers a generous free tier for testing and learning. You won't need to provide payment information for this workshop.</p>"},{"location":"lab6/projects/devduck/prereq/#pre-flight-checklist","title":"Pre-flight Checklist","text":"<p>Before proceeding to the next section, ensure you have:</p> <ul> <li>[ ] Docker Desktop installed and running</li> <li>[ ] Cerebras API account created and key obtained</li> <li>[ ] At least 16GB RAM available</li> <li>[ ] Stable internet connection</li> <li>[ ] Basic understanding of Docker concepts</li> <li>[ ] Terminal/command prompt access</li> </ul>"},{"location":"lab6/projects/devduck/prereq/#next-steps","title":"Next Steps","text":"<p>Great! Now that you have verified your prerequisites, you're ready to start building. In the next section, you'll clone the repository and begin the deployment process.</p> <p>The exciting hands-on work begins now! \ud83d\ude80</p>"},{"location":"lab7/getting-started/","title":"Getting Started","text":"<ul> <li>Install Docker Desktop 4.43.0+</li> <li>Open the terminal and run the following command:</li> </ul> <pre><code>docker offload\nUsage:  docker offload COMMAND\n\nDocker Offload\n\nCommands:\n  accounts       Prints available accounts\n  diagnose       Print diagnostic information\n  start          Start a Docker Offload session\n  status         Show the status of the Docker Offload connection\n  stop           Stop a Docker Offload session\n  version        Prints the version\n\nRun 'docker offload COMMAND --help' for more information on a command.\n</code></pre>"},{"location":"lab7/getting-started/#start-a-docker-offload-session","title":"Start a Docker Offload session","text":"<pre><code>docker offload start\n</code></pre> <p>Please choose your Hub account and proceed further.</p>"},{"location":"lab7/getting-started/#enable-or-disable-gpu-support-based-on-your-preference","title":"Enable or disable GPU support based on your preference.","text":"<p>!Tip: If you choose to enable GPU support, Docker Offload will run in an instance with an NVIDIA L4 GPU, which is useful for machine learning or compute-intensive workloads. </p> <p>It starts Docker Offload session through the <code>docker</code> account.</p> <p></p> <p>Verify if a new <code>docker-cloud</code> context is created or not.</p> <pre><code>docker context ls\nNAME             DESCRIPTION                                      DOCKER ENDPOINT                                             ERROR\ndefault          Current DOCKER_HOST based configuration          unix:///var/run/docker.sock                                 \ndesktop-linux    Docker Desktop                                   unix:///Users/ajeetsraina/.docker/run/docker.sock           \ndocker-cloud *   docker cloud context created by version v0.4.2   unix:///Users/ajeetsraina/.docker/cloud/docker-cloud.sock   \ntcd              Testcontainers Desktop                           tcp://127.0.0.1:49496   \n</code></pre> <p>By now, you should be able to able to print available accounts through CLI.</p> <pre><code>docker offload accounts\n{\n  \"user\": {\n    \"id\": \"15ee357d-XXXX-4d39-87d9-dc3b697b3392\",\n    \"fullName\": \"XXX\",\n    \"gravatarUrl\": \"\",\n    \"username\": \"XXX\",\n    \"state\": \"READY\"\n  },\n  \"orgs\": [\n    {\n      \"id\": \"57b45934-74c0-11e4-XXX-0242ac11001b\",\n      \"fullName\": \"Docker, Inc.\",\n      \"gravatarUrl\": \"https://www.gravatar.com/avatar/XXXXXa68e?s=80&amp;r=g&amp;d=mm\",\n      \"orgname\": \"docker\",\n      \"state\": \"READY\"\n    },\n    {\n      \"id\": \"96a075XXXXXX1a78b\",\n      \"fullName\": \"\",\n      \"gravatarUrl\": \"\",\n      \"orgname\": \"XXXX\",\n      \"state\": \"READY\"\n    }\n  ]\n}\n\n</code></pre>"},{"location":"lab7/getting-started/#check-the-status-of-docker-offload","title":"Check the status of Docker Offload.","text":"<pre><code>docker offload status\n</code></pre>"},{"location":"lab7/getting-started/#check-the-version","title":"Check the version","text":"<pre><code>docker offload version\nDocker Offload v0.4.2 build at 2025-06-30\n</code></pre>"},{"location":"lab7/getting-started/#stop-the-docker-offload","title":"Stop the Docker Offload","text":"<p>This command removes <code>docker-cloud</code> instance from your system.</p>"},{"location":"lab7/getting-started/#using-docker-offload-using-docker-dashboard","title":"Using Docker Offload using Docker Dashboard","text":"<ul> <li>Navigate to Settings &gt; Beta Features to enable Docker Offload.</li> </ul> <p> - Once enabled, you will need to hit the toggle button to start Docker Offload.</p> <p></p> <ul> <li>You can check the status by running the following command:</li> </ul> <pre><code>docker offload status\n</code></pre> <ul> <li>You\u2019ll notice that Docker Offload appears in multiple places    Under Models    On the left sidebar of the Docker Dashboard</li> </ul> <p></p> <ul> <li>Verify if you\u2019re using Cloud instance through Docker Offload.</li> </ul> <pre><code>docker info | grep -E \"(Server Version|Operating System)\"\n Server Version: 28.0.2\n Operating System: Ubuntu 22.04.5 LTS\najeetsraina  ~  \u2665 16:54\n</code></pre> <ul> <li>You can verify the type of GPU that your remote instance is leveraging.</li> </ul> <pre><code>docker run --rm --gpus all nvidia/cuda:12.4.0-runtime-ubuntu22.04 nvidia-smi\n</code></pre> <p>Results:</p> <pre><code>==========\n== CUDA ==\n==========\n\nCUDA Version 12.4.0\n\nContainer image Copyright (c) 2016-2023, NVIDIA CORPORATION &amp; AFFILIATES. All rights reserved.\n\nThis container image and its contents are governed by the NVIDIA Deep Learning Container License.\nBy pulling and using the container, you accept the terms and conditions of this license:\nhttps://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\n\nA copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.\n\nFri Jul  4 11:26:11 2025       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.247.01             Driver Version: 535.247.01   CUDA Version: 12.4     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  NVIDIA L4                      Off | 00000000:31:00.0 Off |                    0 |\n| N/A   44C    P0              27W /  72W |  20200MiB / 23034MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n\n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n+---------------------------------------------------------------------------------------+\n\n\n</code></pre> <p>It shows NVIDIA L4 GPU with 23GB of memory. You can find further details: - GPU: NVIDIA L4 (great for AI/ML workloads) - Memory: 23GB total, ~20GB already allocated - Driver: 535.247.01 with CUDA 12.4 support - Current Usage: 0% (idle)</p>"},{"location":"lab7/overview/","title":"Overview","text":"<p>Docker Offload is a fully managed service that lets you offload building and running containers to the cloud using the Docker tools you already know. It provides cloud infrastructure for fast, consistent builds and compute-heavy workloads like running LLMs or machine learning pipelines.</p>"},{"location":"lab7/overview/#key-capabilities","title":"Key Capabilities:","text":"<p>Here are the key capabilities: - One-click access to GPU: Access NVIDIA L4 GPUs for AI/ML workloads and data processing - Cloud Builds: Execute docker build commands on powerful cloud infrastructure - Container Execution: Run containers with cloud compute and GPU acceleration - Seamless Integration: Same Docker commands, cloud execution</p>"},{"location":"lab7/overview/#whos-this-for","title":"Who's This For?","text":"<ul> <li>Docker Offload is ideal for developers and teams who need:</li> <li>High-velocity development workflows that require cloud scale without complexity</li> <li>Resource-intensive builds that exceed local machine capabilities</li> <li>GPU acceleration for AI/ML development and testing</li> <li>Consistent performance across different development environments</li> <li>Legacy hardware support including virtual desktops and machines without nested virtualization</li> </ul>"},{"location":"lab7/overview/#how-it-works","title":"How It Works?","text":""},{"location":"lab7/overview/#architecture-overview","title":"Architecture Overview","text":"<p>When you use Docker Offload, Docker Desktop creates a secure SSH tunnel to a Docker daemon running in the cloud. Your containers are created, managed, and executed entirely in the remote environment while providing a seamless local experience. The Process 1. Connection: Docker Desktop connects securely to dedicated cloud resources 2. Execution: Docker Offload pulls required images and starts containers in the cloud 3. Interaction: The connection remains active while containers run, supporting features like bind mounts and port forwarding 4. Cleanup: When containers stop or after 30 minutes of inactivity, the environment automatically shuts down and cleans up</p>"},{"location":"lab7/overview/#session-management","title":"Session Management","text":"<p>Docker Offload provisions ephemeral cloud environments for each session:</p> <ul> <li>Environment remains active during Docker Desktop interaction or container usage</li> <li>Automatic shutdown after ~30 minutes of inactivity</li> <li>Complete cleanup of containers, images, and volumes when sessions end</li> <li>No persistent state between sessions ensures security and cost efficiency</li> </ul>"},{"location":"lab7/overview/#cloud-builds-with-docker-offload","title":"Cloud Builds with Docker Offload","text":""},{"location":"lab7/overview/#enhanced-build-performance","title":"Enhanced Build Performance","text":"<p>When you use Docker Offload for builds, the docker buildx build command executes on remote BuildKit instances in the cloud instead of locally. Your workflow remains identical\u2014only the execution environment changes.</p>"},{"location":"lab7/overview/#build-infrastructure","title":"Build Infrastructure","text":"<ul> <li>Isolated Environment: Each cloud builder runs on dedicated Amazon EC2 instances with EBS volumes</li> <li>Shared Cache: Accelerate builds across machines and team members with intelligent caching</li> <li>Multi-Platform Support: Native support for multiple architectures (linux/amd64, linux/arm64)</li> <li>Secure Transfer: Build results are encrypted in transit and sent to your specified destination</li> <li>Automatic Management: Docker handles all infrastructure provisioning and maintenance</li> </ul>"},{"location":"lab7/overview/#performance-benefits","title":"Performance Benefits","text":"<ul> <li>Faster Builds: Powerful cloud instances outperform typical development machines</li> <li>Consistent Results: Standardized build environment eliminates \"works on my machine\" issues</li> <li>Resource Efficiency: No local CPU/memory consumption during builds</li> <li>Team Collaboration: Shared cache improves build times across the entire team</li> </ul>"},{"location":"lab7/overview/#gpu-accelerated-workloads","title":"GPU-Accelerated Workloads","text":"<p>Docker Offload supports GPU-enabled instances with NVIDIA L4 GPUs, enabling:</p> <ul> <li>AI Inferencing: Run large language models and machine learning pipelines</li> <li>Media Processing: Handle video encoding, image processing, and graphics workloads</li> <li>General-Purpose GPU Computing: Accelerate any CUDA-compatible applications</li> <li>Hardware-Accelerated CI: Run GPU-dependent tests and validation</li> </ul>"},{"location":"prereq/prereq/","title":"Prerequisites","text":""},{"location":"prereq/prereq/#1-docker-desktop","title":"1. Docker Desktop","text":"<p>Download and Install Docker Desktop 4.44.0+ on your system. </p> <ul> <li>Apple Chip</li> <li>Intel Chip</li> <li>Windows with NVIDIA GPUs</li> <li>Linux</li> </ul>"},{"location":"prereq/prereq/#2-download-your-preferred-ides-optional","title":"2. Download your preferred IDEs (optional)","text":"<ul> <li>IntelliJ IDEA</li> <li>VS Code</li> </ul>"},{"location":"prereq/prereq/#enabling-wsl-2-based-engine-on-docker-desktop-for-windows","title":"Enabling WSL 2 based engine on Docker Desktop for Windows","text":"<p>In case you're using Windows 11, you will need to enable WSL 2 by opening Docker Desktop &gt; Settings &gt; Resources &gt; WSL Integration</p> <p></p>"},{"location":"prereq/prereq/#3-install-nodejs","title":"3. Install Nodejs","text":"<p>To demonstrate the container-first development workflow, you will require Nodejs installed on your system.</p> <p>Note: You must download and install the Node pre-built installer on your local system to get the npm install command to work seamlessly. Click here to download</p>"},{"location":"prereq/prereq/#4-access-to-the-repositories","title":"4. Access to the repositories","text":"<ul> <li>https://github.com/dockersamples/catalog-service-node</li> <li>https://github.com/dockersamples/genai-model-runner-metrics</li> <li>https://github.com/ajeetraina/catalog-service-node-chatbot</li> <li>https://github.com/ajeetraina/catalog-service-ai-enhanced</li> <li>https://github.com/dockersamples/visual-chatbot</li> <li>https://github.com/ajeetraina/pen-shop-demo</li> </ul>"},{"location":"prereq/prereq/#6-enable-docker-model-runner-for-docker-ai-workshop","title":"6. Enable Docker Model Runner (For Docker AI workshop)","text":"<pre><code>docker desktop enable model-runner\n</code></pre>"},{"location":"prereq/prereq/#7-download-the-models-for-docker-ai-workshop","title":"7. Download the models (For Docker AI workshop)","text":"<p>Ensure that you have sufficient space to download these models on your Macbook.</p> <pre><code>docker model pull ai/llama3.2:1B-Q8_0\ndocker model pull hf.co/menlo/jan-nano-gguf:q4_k_m\ndocker model pull hf.co/menlo/lucy-gguf:q8_0\n</code></pre>"},{"location":"prereq/prereq/#8-download-the-following-images-from-the-docker-hub-for-docker-ai-workshop","title":"8. Download the following images from the Docker Hub (For Docker AI workshop)","text":"<pre><code>docker pull grafana/grafana:10.1.0\ndocker pull jaegertracing/all-in-one:1.46\ndocker pull prom/prometheus:v2.45.0\n</code></pre>"},{"location":"prereq/prereq/#9-install-and-configure-your-preferred-mcp-client-for-docker-ai-workshop","title":"9. Install and configure your preferred MCP Client (For Docker AI workshop)","text":"<pre><code>Ask Gordon  (optional)\nClaude Desktop (optional)\nClaude Code\nGemini CLI\nVisual Studio Code\n</code></pre>"},{"location":"product-catalog/build/","title":"Build","text":""},{"location":"product-catalog/build/#using-docker-build-cloud","title":"Using Docker Build Cloud","text":"<p>Method: 1 : Running it locally</p> <pre><code>docker buildx create --driver cloud dockerdevrel/demo-builder\ndocker buildx build --builder cloud-dockerdevrel-demo-builder .\n</code></pre> <p></p> <p>Method:2 =- Running it using GitHub Workflow</p> <p>Open dockersamples/ repo and se the workflow.</p>"},{"location":"product-catalog/develop/","title":"Develop","text":""},{"location":"product-catalog/develop/#clone-the-repo","title":"Clone the repo","text":"<pre><code>git clone https://github.com/dockersamples/catalog-service-node\n</code></pre>"},{"location":"product-catalog/develop/#initial-setup","title":"Initial Setup","text":"<pre><code>cd demo/sdlc-e2e\n./setup.sh\n</code></pre> <p>The setup.sh script performs several important setup tasks to prepare the development environment for the Docker workshop. Let me explain what it does in detail: The script performs the following tasks:</p> <ul> <li>Creates a demo branch:</li> </ul> <p>It determines the repository root using git rev-parse --show-toplevel It creates a new git branch with a unique name combining \"demo\", the current date, and your username (e.g., demo-20250304-ajeet) This ensures each participant has their own isolated branch to work with</p> <ul> <li>Cleans the environment:</li> </ul> <p>Runs git clean -f to remove untracked files Deletes any existing branches named 'temp' or with the same demo branch name Creates a temporary branch, deletes the main branch locally, then recreates it This ensures everyone starts with a clean state</p> <ul> <li>Updates to latest code:</li> </ul> <p>Pulls the latest changes from the remote repository</p> <ul> <li>Applies the demo patch:</li> </ul> <p>Applies the demo.patch file using git apply --whitespace=fix This patch includes specific modifications to the codebase for the workshop In particular, it removes the upc: product.upc, line from src/services/ProductService.js (line 52) It also modifies the Dockerfile to use an older Node.js version and changes some package versions</p> <ul> <li>Creates a commit:</li> </ul> <p>Commits the changes with the message \"Demo prep\"</p> <ul> <li>Installs dependencies:</li> </ul> <p>Runs npm install to install all required Node.js dependencies</p> <ul> <li>Downloads container images:</li> </ul> <p>Runs docker compose pull to download all the required Docker images in advance This saves time during the workshop</p> <ul> <li>Prepares for Docker Build Cloud:</li> </ul> <p>Removes postgres:17.2 container image (if it exists) Creates and configures a cloud buildx builder for Docker Build Cloud This enables faster builds using Docker's cloud-based build infrastructure</p> <ul> <li>Configures Docker Scout:</li> </ul> <p>Sets up Docker Scout with the dockerdevrel organization This enables security scanning capabilities for the workshop</p> <p>In summary, the setup.sh script prepares a consistent, clean environment with all necessary dependencies, tools, and configurations so that workshop participants can immediately start working on the exercises without spending time on setup. It also makes deliberate modifications to the codebase (like removing the UPC field from Kafka messages) that will be \"fixed\" during the workshop exercises.</p>"},{"location":"product-catalog/develop/#run-the-compose","title":"Run the Compose","text":"<pre><code>docker compose up -d\n</code></pre>"},{"location":"product-catalog/develop/#setting-up-the-demo","title":"Setting up the Demo","text":"<p>Bring up the API service </p> <pre><code>npm install\nnpm run dev\n</code></pre>"},{"location":"product-catalog/develop/#accessing-the-web-client","title":"Accessing the Web Client","text":"<ul> <li>Open the web client (http://localhost:5173) and create a few products.</li> </ul>"},{"location":"product-catalog/develop/#accessing-the-database-visualizer","title":"Accessing the database visualizer","text":"<ul> <li> <p>Open http://localhost:5050 and validate the products exist in the database.  \"Good! We see the UPCs are persisted in the database\"</p> </li> <li> <p>Use \"postgres\" as password to enter into the visualizer.</p> </li> <li> <p>Use the following Postgres CLI to check if the products are added or not.</p> </li> </ul> <pre><code># psql -U postgres\npsql (17.1 (Debian 17.1-1.pgdg120+1))\nType \"help\" for help\npostgres=# \\c catalog\nYou are now connected to database \"catalog\" as user \"postgres\".\ncatalog=# SELECT * FROM products;\n  1 | New Product | 100000000001 | 100.00 | f\n  2 | New Product | 100000000002 | 100.00 | f\n  3 | New Product | 100000000003 | 100.00 | f\n</code></pre>"},{"location":"product-catalog/develop/#access-the-kafka-visualizer","title":"Access the Kafka Visualizer","text":"<p>Before we access visualizer, let's apply the patch:</p> <p>Open the Kafka visualizer http://localhost:8080 and look at the published messages.  \"Ah! We see the messages don't have the UPC\"</p> <p></p>"},{"location":"product-catalog/develop/#lets-fix-it","title":"Let's fix it...","text":""},{"location":"product-catalog/develop/#configuring","title":"Configuring","text":"<p>In VS Code, open the <code>src/services/ProductService.js</code> file and add the following to the publishEvent on line ~52:</p> <pre><code>upc: product.upc,\n</code></pre> <p>Save the file and create a new product using the web UI. </p> <p></p> <p>Validate the message has the expected contents.</p>"},{"location":"product-catalog/overview/","title":"Catalog Service - Node","text":"<p>This repo is a demo project that demonstrates all of Docker's services in a single project. Specifically, it includes the following:</p> <ul> <li>A containerized development environment (in a few varieties of setup)</li> <li>Integration testing with Testcontainers</li> <li>Building in GitHub Actions with Docker Build Cloud</li> </ul> <p>This project is also setup to be used for various demos. </p>"},{"location":"product-catalog/prereq/","title":"Prereq","text":""},{"location":"product-catalog/prereq/#1-docker-desktop","title":"1. Docker Desktop","text":"<p>Download and Install Docker Desktop on your system. Make sure you are using Docker Desktop v4.27.2 and above.</p> <ul> <li>Apple Chip</li> <li>Intel Chip</li> <li>Windows</li> <li>Linux</li> </ul>"},{"location":"product-catalog/prereq/#enabling-wsl-2-based-engine-on-docker-desktop-for-windows","title":"Enabling WSL 2 based engine on Docker Desktop for Windows","text":"<p>In case you're using Windows 11, you will need to enable WSL 2 by opening Docker Desktop &gt; Settings &gt; Resources &gt; WSL Integration</p> <p></p>"},{"location":"product-catalog/prereq/#2-install-nodejs","title":"2. Install Nodejs","text":"<p>To demonstrate the Product Catalog sample app, you will require Node 22+ version installed on your system.</p> <p>Note: You must download and install the Node pre-built installer on your local system to get the npm install command to work seamlessly. Click here to download</p>"},{"location":"product-catalog/prereq/#3-access-to-the-repositories","title":"3. Access to the repositories","text":"<ul> <li>https://github.com/dockersamples/catalog-service-node </li> </ul>"},{"location":"product-catalog/prereq/#4-access-to-the-list-of-packages","title":"4. Access to the list of Packages","text":"<p>If you're behind the firewall, these are the list of packages required for this workshop:</p>"},{"location":"product-catalog/prereq/#docker-init-demo","title":"Docker Init Demo","text":"<pre><code>alpine-baselayout-3.6.5-r0\nalpine-baselayout-data-3.6.5-r0\nalpine-keys-2.4-r1\napk-tools-2.14.4-r0\nbusybox-1.36.1-r29\nbusybox-binsh-1.36.1-r29\nca-certificates-bundle-20240705-r0\nlibcrypto3-3.3.2-r0\nlibgcc-13.2.1_git20240309-r0\nlibssl3-3.3.2-r0\nlibstdc++-13.2.1_git20240309-r0\nmusl-1.2.5-r0\nmusl-utils-1.2.5-r0\nscanelf-1.3.7-r2\nssl_client-1.36.1-r29\nzlib-1.3.1-r1\n</code></pre>"},{"location":"product-catalog/prereq/#install-testcontainers-desktop-app","title":"Install Testcontainers Desktop App","text":"<p>Click here to download Testcontainers Desktop app and install it on your machine.</p>"},{"location":"product-catalog/secure/","title":"Secure","text":""},{"location":"product-catalog/secure/#using-docker-scout","title":"Using Docker Scout","text":"<p>For a Scout demo, the following patch will adjust the Dockerfile to use an older base image and install an older version of Express, allowing you to demo out-of-date base images and vunlerable dependencies.</p> <pre><code>git apply --whitespace=fix demo/scout.patch\n</code></pre> <p>Let's build the Docker image.</p> <pre><code>docker build -t scout:v1.0 .\n</code></pre> <p>Search \"Express\" package and it will instruct that you can fix it by using express 4.17.3+.</p> <p>Open up <code>package.json</code> file and change express version from 4.17.3 to 4.19.2.</p> <p>Next, run <code>npm install</code> before running the following command</p> <pre><code>docker build -t scout:v2.0 .\n</code></pre> <p>You'll notice that all the H and C vulnerabilities are all gone.</p>"},{"location":"product-catalog/tech-stack/","title":"Tech stack","text":""},{"location":"product-catalog/tech-stack/#application-architecture","title":"Application architecture","text":"<p>This sample app provides an API that utilizes the following setup:</p> <ul> <li>Data is stored in a PostgreSQL database</li> <li>Product images are stored in a AWS S3 bucket</li> <li>Inventory data comes from an external inventory service</li> <li>Updates to products are published to a Kafka cluster</li> </ul> <p></p> <p>During development, containers provide the following services:</p> <ul> <li>PostgreSQL and Kafka runs directly in a container</li> <li>LocalStack is used to run S3 locally</li> <li>WireMock is used to mock the external inventory service</li> <li>pgAdmin and kafbat are added to visualize the PostgreSQL database and Kafka cluster</li> </ul> <p></p>"},{"location":"product-catalog/test/","title":"Testing Your Containerized Application","text":"<p>In this section, you'll learn how to implement and run automated tests for your containerized application using Testcontainers. This approach ensures that your tests run in an environment that closely matches production, leading to more reliable test results.</p>"},{"location":"product-catalog/test/#understanding-testcontainers","title":"Understanding Testcontainers","text":"<p>Testcontainers is a library that provides lightweight, throwaway instances of common databases, message brokers, or anything else that can run in a Docker container. It's perfect for integration testing because:</p> <ul> <li>It creates isolated environments for each test</li> <li>It spins up actual services rather than mocks (when needed)</li> <li>It cleans up automatically after tests complete</li> <li>It's language-agnostic (though we'll use the JavaScript implementation)</li> </ul>"},{"location":"product-catalog/test/#prerequisites","title":"Prerequisites","text":"<p>Before running the tests, ensure you have:</p> <ul> <li>Docker Desktop installed and running</li> <li>Testcontainers Desktop installed (optional but recommended)</li> <li>Completed the Development phase</li> </ul>"},{"location":"product-catalog/test/#setting-up-testcontainers-desktop-optional","title":"Setting Up Testcontainers Desktop (Optional)","text":"<ol> <li>Download and install Testcontainers Desktop</li> <li>Open the application</li> <li>Ensure it can connect to your Docker instance</li> </ol> <p>Testcontainers Desktop provides a visual interface for monitoring containers created during testing.</p>"},{"location":"product-catalog/test/#understanding-the-test-structure","title":"Understanding the Test Structure","text":"<p>Our application uses two types of tests:</p> <ol> <li>Unit Tests: Test individual functions without external dependencies</li> <li>Integration Tests: Test complete features with actual dependencies (using Testcontainers)</li> </ol> <p>The test files are located in: - <code>test/src/unit/</code> - Unit tests - <code>test/src/integration/</code> - Integration tests</p> <p>Key integration test files: - <code>containerSupport.js</code>: Manages container lifecycle for tests - <code>kafkaSupport.js</code>: Provides Kafka testing utilities - <code>productCreation.integration.test.js</code>: Tests product creation functionality</p>"},{"location":"product-catalog/test/#running-the-tests","title":"Running the Tests","text":""},{"location":"product-catalog/test/#unit-tests","title":"Unit Tests","text":"<p>To run the unit tests:</p> <pre><code>npm run unit-test\n</code></pre> <p>These tests verify the behavior of individual functions without external dependencies.</p>"},{"location":"product-catalog/test/#integration-tests","title":"Integration Tests","text":"<p>To run the integration tests:</p> <pre><code>npm run integration-test\n</code></pre> <p>When you run integration tests, Testcontainers will: 1. Spin up required containers (PostgreSQL, Kafka, LocalStack) 2. Run the tests against these containers 3. Tear down the containers when tests complete</p> <p>You can observe these containers in Docker Desktop or Testcontainers Desktop:</p> <p></p>"},{"location":"product-catalog/test/#understanding-the-integration-test-code","title":"Understanding the Integration Test Code","text":"<p>Let's examine the key components of the integration test:</p>"},{"location":"product-catalog/test/#container-setup","title":"Container Setup","text":"<pre><code>// From containerSupport.js\nasync function setup() {\n  // Start PostgreSQL container\n  postgres = await new GenericContainer(\"postgres:15\")\n    .withExposedPorts(5432)\n    .withEnvironment({ POSTGRES_PASSWORD: \"postgres\" })\n    .start();\n\n  // Start Kafka container\n  kafka = await new GenericContainer(\"confluentinc/cp-kafka:7.4.0\")\n    .withExposedPorts(9092)\n    .withEnvironment({\n      KAFKA_ADVERTISED_LISTENERS: \"PLAINTEXT://localhost:9092\",\n      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n    })\n    .start();\n\n  // Start LocalStack (for S3)\n  localstack = await new GenericContainer(\"localstack/localstack:2.2\")\n    .withExposedPorts(4566)\n    .start();\n\n  // Configure environment variables for tests\n  process.env.DATABASE_URL = `postgres://postgres:postgres@localhost:${postgres.getMappedPort(5432)}/postgres`;\n  process.env.KAFKA_BROKER = `localhost:${kafka.getMappedPort(9092)}`;\n  process.env.S3_ENDPOINT = `http://localhost:${localstack.getMappedPort(4566)}`;\n}\n</code></pre> <p>This code sets up isolated containers for each service our application depends on.</p>"},{"location":"product-catalog/test/#test-cases","title":"Test Cases","text":"<pre><code>// From productCreation.integration.test.js\ndescribe(\"Product creation\", () =&gt; {\n  it(\"should publish and return a product when creating a product\", async () =&gt; {\n    const product = {\n      name: \"Test Product\",\n      upc: \"123456789012\",\n      price: 9.99\n    };\n\n    const createdProduct = await productService.createProduct(product);\n\n    expect(createdProduct.id).toBeDefined();\n    expect(createdProduct.name).toBe(product.name);\n    expect(createdProduct.upc).toBe(product.upc);\n    expect(createdProduct.price).toBe(product.price);\n\n    const retrievedProduct = await productService.getProduct(createdProduct.id);\n    expect(retrievedProduct).toEqual(createdProduct);\n  });\n\n  it(\"should publish a Kafka message when creating a product\", async () =&gt; {\n    const product = {\n      name: \"Kafka Test Product\",\n      upc: \"123456789013\",\n      price: 19.99\n    };\n\n    await productService.createProduct(product);\n\n    const message = await kafkaConsumer.waitForMessage(\"products\", 5000);\n    expect(message).toBeDefined();\n    expect(message.action).toBe(\"product_created\");\n    expect(message.name).toBe(product.name);\n    expect(message.upc).toBe(product.upc);\n    expect(message.price).toBe(product.price);\n  });\n\n  // Additional tests...\n});\n</code></pre> <p>These tests verify that: 1. Products can be created and retrieved 2. Kafka messages are published correctly 3. File uploads work as expected 4. Business rules (like preventing duplicate UPCs) are enforced</p>"},{"location":"product-catalog/test/#benefits-of-testcontainers-for-integration-testing","title":"Benefits of Testcontainers for Integration Testing","text":"<ol> <li>Realistic Testing: Tests run against actual services, not mocks</li> <li>Isolation: Each test run gets fresh containers</li> <li>Portability: Tests run the same way on any machine with Docker</li> <li>Parallelism: Tests can run in parallel with isolated containers</li> <li>CI/CD Compatibility: Works well in CI/CD pipelines</li> </ol>"},{"location":"product-catalog/test/#common-testing-patterns-with-containers","title":"Common Testing Patterns with Containers","text":"<ol> <li>Database Testing: Use a containerized database with a known schema and test data</li> <li>Message Queue Testing: Verify message publishing and consuming with real message brokers</li> <li>API Testing: Test API endpoints against containerized dependencies</li> <li>End-to-End Testing: Use containers for all services to test complete workflows</li> </ol>"},{"location":"product-catalog/test/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter issues with Testcontainers:</p> <ol> <li>Port Conflicts: Ensure no other services are using the same ports</li> <li>Docker Connection: Verify Docker is running and accessible</li> <li>Resource Limits: Check if Docker has sufficient resources (CPU, memory)</li> <li>Network Issues: Ensure containers can communicate with each other</li> </ol>"},{"location":"product-catalog/test/#next-steps","title":"Next Steps","text":"<p>Now that you've learned how to test your application with Testcontainers, you can:</p> <ol> <li>Add more tests to improve coverage</li> <li>Integrate tests into your CI/CD pipeline</li> <li>Move on to the Build phase to learn how to build and package your application for deployment</li> </ol>"}]}