{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Docker Workshop","text":""},{"location":"#github-sources","title":"GitHub Sources","text":"<p>The source code for this workshop is available here</p>"},{"location":"#authors","title":"Authors","text":"<ul> <li>Ajeet Singh Raina - DevRel @Docker</li> </ul>"},{"location":"#benefits-of-this-docker-workshop","title":"Benefits of this Docker Workshop","text":"<ul> <li>Learn about Inner-loop versus Outer-loop development workflow</li> <li>Learn about Docker development workflow</li> <li>Get familiar with Container-first Vs Container-supported development workflow</li> <li>Learn about Docker Init</li> <li>Learn about Compose Watch</li> <li>Learn how to containerise a Todo-List application using AWS S3 and Mongo</li> </ul>"},{"location":"lab1/best-practices/","title":"Image Building Best Practices","text":""},{"location":"lab1/best-practices/#1-use-explicit-base-image-reference-instead-of-the-latest","title":"1. Use explicit base image reference instead of the latest","text":"<p>Developers are often led to believe that specifying the latest tag will provide them with the most recent image in the repository but it has some side effects.</p> <p>Image tags are mutable, meaning a publisher can update a tag to point to a new image. For example, if you specify FROM node:latest in your Dockerfile, it might resolve to the latest patch version for 18.11. However, if you rebuild the image 3 months later, the same tag might point to a different version, such as 18.13. This could result in breaking changes, and it means you also don't have an audit trail of the exact image versions that you're using.. </p> <p></p>"},{"location":"lab1/best-practices/#2-prefer-leaner-docker-images","title":"2. Prefer leaner Docker Images","text":"<p>Using leaner Docker images can help reduce the size of the final image, which can lead to faster build times, smaller storage footprint, and quicker deployment times.</p> <p>For example, try to use Slimmer Images. Select smaller images for your FROM instructions in your Dockerfile. For example, the `node:16.17.0-slim image is a minimal Docker image that provides all of the OS utilities you would expect from a Linux container. There's also the special scratch image, which contains nothing at all and is useful for creating images of statically linked binaries (source).</p> <p></p>"},{"location":"lab1/best-practices/#3-use-multi-stage-builds","title":"3. Use Multi-stage builds","text":"<p>Multi-stage builds let you reduce the size of your final image, by creating a cleaner separation between the building of your image and the final output. Split your Dockerfile instructions into distinct stages to make sure that the resulting output only contains the files that's needed to run the application.</p> <p>Using multiple stages can also let you build more efficiently by executing build steps in parallel.</p> <p></p>"},{"location":"lab1/best-practices/#4-quickly-identify-and-fix-vulnerabilities-during-the-build-time-using-docker-scout","title":"4. Quickly identify and fix vulnerabilities during the Build time using Docker Scout","text":"<p>Container images consist of layers and software packages, which are susceptible to vulnerabilities. These vulnerabilities can compromise the security of containers and applications.</p> <p>Docker Scout is a solution for proactively enhancing your software supply chain security. By analyzing your images, Docker Scout compiles an inventory of components, also known as a Software Bill of Materials (SBOM). The SBOM is matched against a continuously updated vulnerability database to pinpoint security weaknesses.</p> <p>Docker Scout image analysis is available by default for Docker Hub repositories. You can also integrate third-party registries and other services</p> <p></p>"},{"location":"lab1/best-practices/#5-add-healthcheck-in-dockerfile-and-docker-compose","title":"5. Add Healthcheck in Dockerfile and Docker Compose","text":"<p>You can add a healthcheck in both Dockerfile and Docker Compose file. In a Dockerfile, you can use the HEALTHCHECK instruction. Here's an example:</p> <pre><code>HEALTHCHECK --interval=5m --timeout=3s \\\n  CMD curl -f http://localhost/ || exit 1\n</code></pre> <p>In this example, Docker will check every five minutes if a web-server is able to serve the site's main page within three seconds. If the command (curl -f http://localhost/ || exit 1) returns a non-zero code, the container is considered unhealthy (source).</p> <p>In a Docker Compose file, you can use the healthcheck attribute under a service. Here's an example:</p> <pre><code>services:\n  web:\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost\"]\n      interval: 1m30s\n      timeout: 10s\n      retries: 3\n</code></pre> <p>In this example, Docker Compose will run the command curl -f http://localhost to check the health of the web service. It will do this every 1 minute and 30 seconds, and if the command doesn't return within 10 seconds or if it fails 3 times in a row, the service is considered unhealthy (source).</p>"},{"location":"lab1/best-practices/#6-use-dockerignore","title":"6. Use .dockerignore","text":"<p>The .dockerignore file is used to exclude files and directories from the build context when building a Docker image. This can help to improve build speed and avoid sending unwanted files to the Docker builder. The syntax of the .dockerignore file is similar to the .gitignore file, with each line representing a pattern that matches files and directories to be excluded.</p> <p>Here's an example of a .dockerignore file:</p> <pre><code># .dockerignore\nnode_modules\nbar\n</code></pre> <p>In this example, the node_modules directory and any file or directory named bar are excluded from the build context.</p> <p>When you run a build command, Docker looks for a .dockerignore file in the root directory of the context. If this file exists, the files and directories that match patterns in the file are removed from the build context before it's sent to the builder. If you have multiple Dockerfiles, you can use different .dockerignore files for each Dockerfile by using a special naming convention. You should place your .dockerignore file in the same directory as the Dockerfile, and prefix the .dockerignore file with the name of the Dockerfile. For example, for a Dockerfile named myapp.Dockerfile, you would create a .dockerignore file named myapp.Dockerfile.dockerignore.</p> <pre><code>.\n\u251c\u2500\u2500 index.ts\n\u251c\u2500\u2500 src/\n\u251c\u2500\u2500 docker\n\u2502   \u251c\u2500\u2500 build.Dockerfile\n\u2502   \u251c\u2500\u2500 build.Dockerfile.dockerignore\n\u2502   \u251c\u2500\u2500 lint.Dockerfile\n\u2502   \u251c\u2500\u2500 lint.Dockerfile.dockerignore\n\u2502   \u251c\u2500\u2500 test.Dockerfile\n\u2502   \u2514\u2500\u2500 test.Dockerfile.dockerignore\n\u251c\u2500\u2500 package.json\n\u2514\u2500\u2500 package-lock.json\n</code></pre> <p>In this example, each Dockerfile has its own corresponding .dockerignore file. If both a Dockerfile-specific .dockerignore file and a .dockerignore file at the root of the build context exist, the Dockerfile-specific .dockerignore file takes precedence.</p>"},{"location":"lab1/best-practices/#7-run-as-non-root-user-for-security-purpose","title":"7. Run as non-root user for security purpose","text":"<p>Running Docker as a non-root user is a good practice to mitigate potential vulnerabilities in the Docker daemon and the container runtime. Docker provides a feature called \"Rootless mode\" that allows running the Docker daemon and containers as a non-root user.</p> <p></p>"},{"location":"lab1/best-practices/#8-favour-multi-architecture-docker-images","title":"8. Favour Multi-Architecture Docker Images","text":"<p>Using multi-architecture Docker images is beneficial as it allows your Docker images to run on different hardware architectures without any modifications. This means that whether you are using an ARM-based system or an x86 machine, Docker automatically detects and selects the appropriate variant for your host's operating system and architecture.</p> <p>There are three strategies to build multi-platform images depending on your use case: - Using emulation, via QEMU support in the Linux kernel. - Building on a single builder backed by multiple nodes of different architectures. - Using a stage in your Dockerfile to cross-compile to different architectures.</p> <p>To build multi-platform images, you can use the --platform flag with the docker build command to define the target platforms for the build output, such as linux/amd64 and linux/arm64.  For example:</p> <pre><code>$ docker build --platform linux/amd64,linux/arm64 .\n</code></pre> <p>By default, Docker can build for only one platform at a time. To build for multiple platforms concurrently, you can enable the containerd image store or create a custom builder. For example, to enable the containerd image store in Docker Desktop, go to Settings and select Use containerd for pulling and storing images in the General tab. If you're not using Docker Desktop, enable the containerd image store by adding the following feature configuration to your /etc/docker/daemon.json configuration file.</p> <pre><code>{\n  \"features\": {\n    \"containerd-snapshotter\": true\n  }\n}\n</code></pre> <p>Then, restart the daemon after updating the configuration file.</p> <pre><code>$ systemctl restart docker\n</code></pre> <p></p>"},{"location":"lab1/compose-watch/","title":"Overview of Compose Watch","text":"<p>Compose File Watch is a feature introduced in Docker Compose version 2.22.0. It allows for automatic updates and previews of your running Compose services as you edit and save your code. This can enable a hands-off development workflow once Compose is running, as services automatically update themselves when you save your work.</p> <pre><code>services:\n  web:\n    build: .\n    command: npm start\n    develop:\n      watch:   \n        - actions: sync\n          path: ./web\n          target: /src/web\n          ignore: \n            - node_modules/\n        - action: rebuild\n          path: package.json\n</code></pre> <p>The <code>watch</code> attribute in the Compose file defines a list of rules that control these automatic service updates based on local file changes. Each rule requires a path pattern and an action to take when a modification is detected. The action can be set to rebuild, sync, or sync+restart.</p> <p>Here's a brief explanation of these actions:</p> <ul> <li>rebuild: Compose rebuilds the service image based on the build section and recreates the service with the updated image.</li> <li>sync: Compose keeps the existing service container(s) running, but synchronizes source files with container content according to the target attribute.</li> <li>sync+restart: Compose synchronizes source files with container content according to the target attribute, and then restarts the container.</li> </ul> <p>You can also define a list of patterns for paths to be ignored using the ignore attribute. Any updated file that matches a pattern, or belongs to a folder that matches a pattern, won't trigger services to be re-created. To use Compose Watch, you need to add the watch instructions to your compose.yaml file and then run your application with the docker compose watch command.</p> <p></p>"},{"location":"lab1/compose-watch/#getting-started","title":"Getting Started","text":""},{"location":"lab1/compose-watch/#clone-the-repository","title":"Clone the repository","text":"<pre><code>git clone https://github.com/docker/getting-started-todo-app/\ncd getting-started-todo-app\n</code></pre>"},{"location":"lab1/compose-watch/#switch-to-compose-watch-branch","title":"Switch to compose-watch branch","text":"<pre><code>git checkout compose-watch\n</code></pre>"},{"location":"lab1/compose-watch/#bringing-up-the-app","title":"Bringing up the app","text":"<pre><code>docker compose watch\n</code></pre>"},{"location":"lab1/compose-watch/#make-some-changes","title":"Make some changes","text":"<p>Open a new terminal and modify the following package under frontend/package.json from </p> <pre><code>\"optionalDependencies\": {\n    \"fsevents\": \"^2.1.2\"\n</code></pre> <p>to</p> <pre><code>\"optionalDependencies\": {\n    \"fsevents\": \"^2.1.3\"\n</code></pre> <p>You will find that the frontend service gets rebuild automatcially for you</p> <pre><code>[+] Running 6/6\n \u2714 Network getting-started-todo-app_express-mongo     Created                                                          0.1s \n \u2714 Network getting-started-todo-app_react-express     Created                                                          0.0s \n \u2714 Container mongo                                    Started                                                          1.1s \n \u2714 Container getting-started-todo-app-mongoexpress-1  Started                                                          1.1s \n \u2714 Container backend                                  Started                                                          1.3s \n \u2714 Container frontend                                 Started                                                          1.8s \nWatch enabled\nRebuilding service \"frontend\" after changes were detected...\n[+] Building 52.5s (12/12) FINISHED                                                             docker-container:my-builder\n =&gt; [frontend internal] load build definition from Dockerfile                                                          0.0s\n =&gt; =&gt; transferring dockerfile: 532B                                                                                   0.0s\n =&gt; [frontend internal] load metadata for docker.io/library/node:lts-buster                                            1.1s\n =&gt; [frontend internal] load .dockerignore                                                                             0.0s\n =&gt; =&gt; transferring context: 67B                                                                                       0.0s\n =&gt; [frontend 1/6] FROM docker.io/library/node:lts-buster@sha256:479103df06b40b90f189461b6f824a62906683e26a32c77d7c3e  0.0s\n =&gt; =&gt; resolve docker.io/library/node:lts-buster@sha256:479103df06b40b90f189461b6f824a62906683e26a32c77d7c3e2d855a0e3  0.0s\n =&gt; [frontend internal] load build context                                                                             0.0s\n =&gt; =&gt; transferring context: 1.94kB                                                                                    0.0s\n =&gt; CACHED [frontend 2/6] WORKDIR /usr/src/app                                                                         0.0s\n =&gt; [frontend 3/6] COPY package.json /usr/src/app                                                                      0.0s\n =&gt; [frontend 4/6] COPY package-lock.json /usr/src/app                                                                 0.0s\n =&gt; [frontend 5/6] RUN npm ci                                                                                         24.4s\n =&gt; [frontend 6/6] COPY . /usr/src/app                                                                                 0.3s \n =&gt; [frontend] exporting to docker image format                                                                       26.5s \n =&gt; =&gt; exporting layers                                                                                                8.4s \n =&gt; =&gt; exporting manifest sha256:c3639997f048e5adea7487bafdafaf4ea3f946e071fd273aefb262b72f2c87a8                      0.0s \n =&gt; =&gt; exporting config sha256:1eb3a22fdb9d89316c9bdb1eb1d9f138e0f9545e95af4d812c9db8499309b491                        0.0s\n =&gt; =&gt; sending tarball                                                                                                18.1s\n =&gt; [frontend] importing to docker                                                                                    10.0s\n =&gt; =&gt; loading layer 7544a5e696a4 567B / 567B                                                                         10.0s\n =&gt; =&gt; loading layer 6e1b4449163b 32.77kB / 128.96kB                                                                   9.9s\n =&gt; =&gt; loading layer 874fffdf421f 115.87MB / 121.34MB                                                                  9.9s\n =&gt; =&gt; loading layer b8d4de9612f2 27.14kB / 27.14kB                                                                    0.0s\nservice \"frontend\" successfully built\n</code></pre>"},{"location":"lab1/docker-init/","title":"Introduction to Docker init","text":"<p>Introduced for the first time in Docker Desktop 4.18, the new docker init CLI generates Docker assets for projects, making it easier to create Docker images and containers. When you run the docker init command in your project directory, it will guide you through the creation of the necessary files for your project with sensible defaults. These files include:</p> <pre><code>.dockerignore\nDockerfile\ndocker-compose.yaml\n</code></pre> <p>The docker init command also allows you to choose the application platform that your project uses and the relative directory of your main package. </p>"},{"location":"lab1/docker-init/#whos-this-for","title":"Who\u2019s this for?","text":"<p>This feature is targeted at developers who want to quickly create and manage Docker assets without having to manually configure everything. </p>"},{"location":"lab1/docker-init/#benefits-of-docker-init","title":"Benefits of Docker Init","text":"<p>The advantages of using the docker init command include:</p> <ul> <li>Simplified Docker asset creation: The command streamlines the creation of necessary Docker files, reducing the chances of errors and ensuring that best practices are followed.</li> <li>Saves time and effort: With the default settings and guided prompts, users can quickly create Docker assets without the need for extensive knowledge of Docker or its syntax.</li> <li>Better project organization: The generated files provide a standardized and organized structure for the project, making it easier for developers to maintain and update the project over time.</li> <li>Enhanced portability: By using Docker assets, projects become more portable across different environments, making it easier to move the project from development to production.</li> </ul> <p></p>"},{"location":"lab1/docker-init/#getting-started","title":"Getting Started","text":""},{"location":"lab1/docker-init/#clone-the-repository","title":"Clone the repository","text":"<pre><code>git clone https://github.com/dockersamples/docker-init-demos\ncd docker-init-demos/node\n</code></pre>"},{"location":"lab1/docker-init/#run-the-following-command","title":"Run the following command:","text":"<pre><code> docker init\n</code></pre> <p>This utility will walk you through creating the following files with sensible defaults for your project:   - .dockerignore   - Dockerfile   - docker-compose.yaml</p>"},{"location":"lab1/docker-init/#install-the-dependencies","title":"Install the Dependencies","text":"<pre><code>npm install\n</code></pre>"},{"location":"lab1/docker-init/#running-the-container-service","title":"Running the container service","text":"<pre><code> docker compose up -d --build\n</code></pre>"},{"location":"lab1/docker-init/#accessing-the-node-app","title":"Accessing the Node app","text":"<pre><code> curl localhost:8080      .\n/\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\\___/ ===\n{                       /  ===-\n\\______ O           __/\n \\    \\         __/\n  \\____\\_______/\n\n\nHello from Docker\n</code></pre>"},{"location":"lab1/docker-workflow/","title":"Docker Developer workflow","text":"<ul> <li> <p>Build: Typically a developer run <code>docker build</code> command to build an image from a Dockerfile, which is a text document that contains instructions for how to build the image.</p> </li> <li> <p>Share: Once a developer builds a Docker image, they can share it with others by pushing it to a container registry. You can use the <code>docker push</code> command to do this.</p> </li> <li> <p>Run: The <code>docker run</code> command lets you create and start a running container based on a Docker image.</p> </li> </ul> <p></p>"},{"location":"lab1/docker-workflow/#docker-inner-loop-development-workflow","title":"Docker Inner-Loop Development workflow","text":"<ul> <li>Development Environment: As you deep-dive into the inner-loop development workflow, developers have their own choice anduse any operating system (OS) they prefer, like Windows, Mac, or Linux. They can also leverage their favorite Integrated Development Environments (IDEs) for coding. This flexibility empowers developers to work in a familiar and comfortable setting.</li> </ul> <ul> <li> <p>Building and storing the application - Once the code is written, the developer uses GitHub Actions. GitHub Actions is a built-in automation tool in GitHub that allows you to automate tasks within your development workflow. In this context, the developer uses GitHub Actions to trigger the building of a Docker image. A Dockerfile, which is a set of instructions that specifies how to build the image, is used in this process.  After the image is built, it's uploaded to a container registry. A container registry acts as a library or repository that stores Docker images.</p> </li> <li> <p>Deployment - Finally, the image is deployed to the cloud. This means the image is uploaded to a cloud platform where it can be run on virtual machines. There are several cloud platforms available, including Google Cloud Platform, Amazon Web Services, and Microsoft Azure. The specific steps for deployment will vary depending on the chosen cloud platform. But generally, it involves using the cloud platform tools to run the Docker image and create a container instance. This container instance then executes the application.</p> </li> </ul>"},{"location":"lab1/overview/","title":"Inner Vs Outer Loop Development workflow","text":"<p>The inner loop is the iterative process of writing, building, and debugging code that a single developer performs before sharing the code, either publicly or with their team. It is typically characterized by frequent changes to the code as the developer learns more about the problem they are trying to solve.</p> <p>The outer loop is everything else that happens leading up to release. This includes code merge, automated code review, test execution, deployment, controlled (canary) release, and observation of results. It is typically characterized by less frequent changes to the code, as the focus is on ensuring that the code is stable and ready for production..</p> <p>The inner loop is often associated with the development phase of the SDLC, while the outer loop is associated with the testing, deployment, and release phases. However, the two loops are not mutually exclusive, and they can overlap in some cases.</p> <p></p>"},{"location":"lab2/aws-s3-setup/","title":"Configuring AWS with IAM and S3","text":""},{"location":"lab2/aws-s3-setup/#1-login-to-aws-and-create-a-user","title":"1. Login to AWS and create a user","text":""},{"location":"lab2/aws-s3-setup/#2-add-a-user-called-developer","title":"2. Add a user called developer","text":""},{"location":"lab2/aws-s3-setup/#3-creat-a-group","title":"3. Creat a group","text":""},{"location":"lab2/aws-s3-setup/#4-add-developer-to-admin1-group","title":"4. Add developer to admin1 group","text":""},{"location":"lab2/aws-s3-setup/#5-review-and-save","title":"5. Review and save","text":""},{"location":"lab2/aws-s3-setup/#6-enable-access-keys-and-security-key-for-this-user","title":"6. Enable Access Keys and Security key for this user","text":""},{"location":"lab2/aws-s3-setup/#7-create-a-s3-bucket","title":"7. Create a S3 bucket","text":""},{"location":"lab2/aws-s3-setup/#8-grant-this-user-with-s3-bucket-access","title":"8. Grant this user with S3 bucket access","text":""},{"location":"lab2/getting-started/","title":"Getting Started","text":""},{"location":"lab2/getting-started/#a-basic-todo-list-app","title":"A Basic Todo List App","text":"<p>This is a starting point for todo-list app powered with React, Node, Mongo and Mongo Express.</p>"},{"location":"lab2/getting-started/#tech-stack","title":"Tech Stack","text":"<ul> <li>Frontend: React</li> <li>Backend: Node.js</li> <li>Database: Mongo DB</li> <li>Database Admin Interface: MongoExpress</li> </ul>"},{"location":"lab2/getting-started/#clone-the-repository","title":"Clone the repository","text":"<pre><code>git clone https://github.com/dockersamples/getting-started-todo-app\ncd getting-started-todo-app\n</code></pre>"},{"location":"lab2/getting-started/#switch-to-basic-branch","title":"Switch to <code>basic</code> branch","text":"<pre><code>git checkout basic\n</code></pre>"},{"location":"lab2/getting-started/#bringing-up-the-service-containers","title":"Bringing up the service containers","text":"<pre><code>docker compose up -d\n</code></pre> <p>After the application starts, navigate to <code>http://localhost:3000</code> in your web browser.</p>"},{"location":"lab2/getting-started/#logging-into-mongo-express","title":"Logging into Mongo Express","text":"<p>Enter <code>admin/pass</code> as credential to login into Mongo Express and view the database and collections/items.</p>"},{"location":"lab2/overview/","title":"Overview","text":"<p>Container-first development takes the concept of containerization a step further. It involves using containers for every aspect of software development, including the application runtime itself. This means developers can ditch traditional local installations and leverage containers for everything needed to run the application.</p>"},{"location":"lab2/overview/#what-it-means","title":"What it means?","text":"<p>In container-first development, developers work within a containerized environment. They:</p> <ul> <li>Clone the project repository.</li> <li>Run a command (often docker-compose up or docker-compose watch) to start the development environment. This command usually pulls pre-built container images containing the application runtime (e.g., Node.js, Python interpreter) and any dependencies.</li> </ul> <p>That's it! The development environment is up and running entirely within containers. No need to install the application runtime or specific libraries on the developer's machine.</p>"},{"location":"lab2/overview/#benefits-for-developers","title":"Benefits for Developers:","text":"<ul> <li>Extreme Portability: Developers only need a container engine and an IDE to work on the project. This ensures identical development environments regardless of the underlying operating system or pre-installed software.</li> <li>Faster Setup: No time wasted installing the application runtime or fiddling with local configurations. Developers can start coding as soon as the containerized environment is up.</li> <li>Improved Isolation: Each project runs within its own isolated container, preventing conflicts between projects or dependencies from interfering with other applications on the developer's machine.</li> <li>Simplified Collaboration: Team members can easily share and reproduce development environments using the same container images.</li> </ul>"},{"location":"lab2/overview/#choosing-container-first","title":"Choosing Container-First:","text":"<ul> <li>Container-first development is ideal for teams seeking:</li> <li>Maximum portability across development environments.</li> <li>Fast and streamlined development setup.</li> <li>Strong isolation between projects to avoid conflicts.</li> </ul> <p>However, it requires a steeper learning curve for containerization technologies and might have higher resource demands.</p> <p>Container-first development offers a powerful approach for building applications entirely within containerized environments. It streamlines development workflows, ensures consistent development environments, and promotes collaboration. But, it's important to weigh the benefits against the increased complexity and potential resource usage before adopting this approach for your development team.</p>"},{"location":"lab2/services/","title":"Bringing up the services","text":""},{"location":"lab2/services/#1-clone-the-repository","title":"1. Clone the repository:","text":"<pre><code>git clone https://github.com/dockersamples/getting-started-todo-app\ncd getting-started-todo-app\n</code></pre>"},{"location":"lab2/services/#2-switch-to-container-first-branch","title":"2. Switch to container-first branch","text":"<pre><code>git checkout container-first-aws-mongo\n</code></pre>"},{"location":"lab2/services/#3-add-the-environment-variables","title":"3. Add the Environment Variables","text":"<p>Copy .env.sample to .env file and Ensure that you have the right environmental variable added as shown:</p> <pre><code>MONGODB_URI=mongodb://mongodb:27017/todo-app\nJWT_SECRET=603b31XXXXXXX90d3b8cb62f0a585fd70a5ee0b4d\nAWS_ACCESS_KEY_ID=AKIAXXXXXDDDX\nAWS_SECRET_ACCESS_KEY=hSYXtvXXXXXXXO/k39FGt3u078pYWsh\nAWS_REGION=us-east-1\nS3_BUCKET_NAME=localbuckett\n</code></pre> <p>You can leverage this link to generate JWT token.</p>"},{"location":"lab2/services/#4-bring-up-the-services","title":"4. Bring up the services:","text":"<pre><code>docker compose up -d\n</code></pre>"},{"location":"lab2/services/#5-access-the-app","title":"5. Access the app","text":"<p>Open http://localhost:3000 to access the todo-list app. Try adding a task and uploading the image.</p>"},{"location":"lab2/services/#verify-mongo","title":"Verify Mongo","text":"<p>You can verify if task gets added by selecting the container and clicking on \"Exec\" option on the Docker dashboard. Now you should be able to run the following command to verify the tasks.</p> <pre><code># mongosh\nCurrent Mongosh Log ID: 66879e864955d6e7b2f3f54d\nConnecting to:          mongodb://127.0.0.1:27017/?directConnection=true&amp;serverSelectionTimeoutMS=2000&amp;appName=mongosh+2.2.10\nUsing MongoDB:          7.0.12\nUsing Mongosh:          2.2.10\n\nFor mongosh info see: https://docs.mongodb.com/mongodb-shell/\n\n\nTo help improve our products, anonymous usage data is collected and sent to MongoDB periodically (https://www.mongodb.com/legal/privacy-policy).\nYou can opt-out by running the disableTelemetry() command.\n\n------\n   The server generated these startup warnings when booting\n   2024-07-05T07:18:03.008+00:00: Using the XFS filesystem is strongly recommended with the WiredTiger storage engine. See http://dochub.mongodb.org/core/prodnotes-filesystem\n   2024-07-05T07:18:03.737+00:00: Access control is not enabled for the database. Read and write access to data and configuration is unrestricted\n   2024-07-05T07:18:03.738+00:00: /sys/kernel/mm/transparent_hugepage/enabled is 'always'. We suggest setting it to 'never' in this binary version\n   2024-07-05T07:18:03.738+00:00: vm.max_map_count is too low\n------\n\ntest&gt; show dbs\nadmin     40.00 KiB\nconfig    12.00 KiB\nlocal     40.00 KiB\ntodo-app  68.00 KiB\ntest&gt; use todo-app\nswitched to db todo-app\ntodo-app&gt; show collections\ntodos\nusers\ntodo-app&gt; db.todos.showDocuments()\nTypeError: db.todos.showDocuments is not a function\ntodo-app&gt; db.todos.countDocuments()\n1\ntodo-app&gt; db.todos.countDocuments()\n2\ntodo-app&gt; db.todos.countDocuments()\n3\ntodo-app&gt;\n</code></pre>"},{"location":"lab2/services/#verify-the-images-added-to-aws-s3","title":"Verify the images added to AWS S3","text":"<p>Open AWS Dashboard &gt; S3 service to see the list of images uploaded.</p> <p></p>"},{"location":"lab2/tech-stack/","title":"Tech stack","text":"<ul> <li>Frontend: React, Material UI.</li> <li>Backend: Node.js, Express</li> <li>Database: Mongo(Atlas or running locally for storing tasks)</li> <li>Object Storage: AWS S3(for storing images)</li> </ul>"},{"location":"lab3/overview/","title":"Overview","text":"<p>Container-supported development is the idea of using containers to support and enhance development without touching the main application runtime itself. </p>"},{"location":"lab3/overview/#what-it-means","title":"What it means?","text":"<p>The developer will run their application using a runtime installed natively on their machine (such as a JVM, Node engine, or Python interpreter). But, the external dependencies will run in containers. </p>"},{"location":"lab3/overview/#what-benefits-does-it-provide-to-the-developers","title":"What benefits does it provide to the developers?","text":"<p>Even though the developers didn\u2019t go \u201call-in\u201d, Docker still provided significant value by running dependent services out of containers, making it quick and easy to get started and ensure version consistency across the entire team.</p> <p>With Docker, teams can do things that might otherwise have been impossible. They can run local instances of cloud services, run real services in their tests, and more. There is no \u201cone right path\u201d for teams to leverage Docker. You see teams using wrapper scripts that run docker run commands, others using IDE plugins to launch declarative Compose stacks, or programmatic interactions using Testcontainers.In many of these cases, teams can leverage off-the-shelf (or very slightly customized versions of) images from our DOI/DVP catalog.</p>"},{"location":"lab3/overview/#choosing-the-container-supported-approach","title":"Choosing the container-supported approach:","text":"<ul> <li> <p>Separation of Concerns: Developers focus on the core application logic using their familiar runtime (JVM, Node, Python etc.), while external dependencies are isolated in containers.</p> </li> <li> <p>Improved Efficiency:</p> </li> <li> <p>Easier setup: Containers simplify dependency management, reducing time spent configuring environments. Version consistency: All developers use the same container image, ensuring consistent dependencies across the team.</p> </li> <li> <p>Enhanced Capabilities: Docker allows running local simulations of cloud services and real services within tests, providing a more realistic development environment.</p> </li> <li> <p>Flexibility in Implementation: There's no single approach. Teams can use:</p> </li> <li> <p>Wrapper scripts for simple container execution.</p> </li> <li>IDE plugins for launching development environments defined in Docker Compose files.</li> <li> <p>Programming interactions with libraries like Testcontainers.</p> </li> <li> <p>Leveraging Shared Resources: Teams can benefit from pre-built container images from public repositories like Docker Official Images (DOI) and Docker Verified Publishers (DVP).</p> </li> </ul> <p>Overall, container-supported development offers a way to streamline the development process by managing dependencies and enhancing development environments without completely switching to a containerized application runtime.</p>"},{"location":"lab3/services/","title":"Bringing up the services","text":""},{"location":"lab3/services/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker Desktop</li> <li>JWT Secret</li> <li>awscli installed</li> </ul>"},{"location":"lab3/services/#getting-started","title":"Getting Started","text":""},{"location":"lab3/services/#1-clone-the-repository","title":"1. Clone the repository:","text":"<pre><code>git clone https://github.com/dockersamples/getting-started-todo-app\ncd getting-started-todo-app\n</code></pre>"},{"location":"lab3/services/#2-switch-to-the-right-branch","title":"2. Switch to the right branch","text":"<p>Switch to container-supported branch before you run the following command:</p> <pre><code>git checkout container-supported\n</code></pre>"},{"location":"lab3/services/#3-bring-up-the-services","title":"3. Bring up the services","text":"<pre><code> docker compose up -d\n</code></pre>"},{"location":"lab3/services/#4-create-s3-bucket-manually","title":"4. Create S3 bucket manually","text":"<pre><code>awslocal s3 mb s3://sample-bucket\nmake_bucket: sample-bucket\n</code></pre>"},{"location":"lab3/services/#5-check-the-localstack-logs","title":"5. Check the LocalStack logs","text":"<pre><code>2024-07-03 19:27:32 \n2024-07-03 19:27:32 LocalStack version: 3.5.1.dev\n2024-07-03 19:27:32 LocalStack build date: 2024-06-24\n2024-07-03 19:27:32 LocalStack build git hash: 9a3d238ac\n2024-07-03 19:27:32 \n2024-07-03 19:27:32 Ready.\n2024-07-03 19:28:13 2024-07-03T13:58:13.804  INFO --- [et.reactor-0] localstack.request.aws     : AWS s3.CreateBucket =&gt; 200\n</code></pre>"},{"location":"lab3/services/#6-access-the-app-and-try-uploading-the-image","title":"6. Access the app and try uploading the image","text":"<p>Open http://localhost:3000 and try to create a new task as well as upload the image.</p>"},{"location":"lab3/services/#7-listing-the-items-in-the-s3-bucket","title":"7. Listing the items in the S3 bucket","text":"<pre><code> awslocal s3api list-objects --bucket sample-bucket\n{\n    \"Contents\": [\n        {\n            \"Key\": \"1720015203095-Screenshot 2024-07-03 at 9.24.34\u00e2\u00afAM.png\",\n            \"LastModified\": \"2024-07-03T14:00:03.000Z\",\n            \"ETag\": \"\\\"cd4396baa401efb22797472599faff87\\\"\",\n            \"Size\": 735617,\n            \"StorageClass\": \"STANDARD\",\n            \"Owner\": {\n                \"DisplayName\": \"webfile\",\n                \"ID\": \"75aa57f09aa0c8caeab4f8c24e99d10f8e7faeebf76c078efc7c6caea54ba06a\"\n            }\n        }\n    ],\n    \"RequestCharged\": null\n</code></pre>"},{"location":"lab3/services/#8-verify-if-item-gets-into-mongo-database","title":"8. Verify if item gets into Mongo database","text":"<pre><code>Connecting to:          mongodb://127.0.0.1:27017/?directConnection=true&amp;serverSelectionTimeoutMS=2000&amp;appName=mongosh+2.2.10\nUsing MongoDB:          7.0.12\nUsing Mongosh:          2.2.10\n\nFor mongosh info see: https://docs.mongodb.com/mongodb-shell/\n\n------\n   The server generated these startup warnings when booting\n   2024-07-03T13:57:31.418+00:00: Using the XFS filesystem is strongly recommended with the WiredTiger storage engine. See http://dochub.mongodb.org/core/prodnotes-filesystem\n   2024-07-03T13:57:32.732+00:00: Access control is not enabled for the database. Read and write access to data and configuration is unrestricted\n   2024-07-03T13:57:32.733+00:00: /sys/kernel/mm/transparent_hugepage/enabled is 'always'. We suggest setting it to 'never' in this binary version\n   2024-07-03T13:57:32.733+00:00: vm.max_map_count is too low\n------\n\ntest&gt; show dbs\nadmin      40.00 KiB\nconfig     72.00 KiB\nlocal      80.00 KiB\ntodo-app  180.00 KiB\ntest&gt; use todo-app\nswitched to db todo-app\ntodo-app&gt; db.todos.countDocuments()\n5\ntodo-app&gt; db.todos.countDocuments()\n6\ntodo-app&gt;\n</code></pre>"},{"location":"lab3/tech-stack/","title":"Tech stack","text":"<ul> <li>Frontend: React, Material UI.</li> <li>Backend: Node.js, Express</li> <li>Database: Mongo(running locally for storing tasks)</li> <li>Object Storage: Localstack (for emulating S3 and storing images locally for testing purpose)</li> </ul>"},{"location":"prereq/prereq/","title":"Prerequisites","text":""},{"location":"prereq/prereq/#1-docker-desktop","title":"1. Docker Desktop","text":"<p>Download and Install Docker Desktop on your system. Make sure you are using Docker Desktop v4.27.2 and above.</p> <ul> <li>Apple Chip</li> <li>Intel Chip</li> <li>Windows</li> <li>Linux</li> </ul>"},{"location":"prereq/prereq/#enabling-wsl-2-based-engine-on-docker-desktop-for-windows","title":"Enabling WSL 2 based engine on Docker Desktop for Windows","text":"<p>In case you're using Windows 11, you will need to enable WSL 2 by opening Docker Desktop &gt; Settings &gt; Resources &gt; WSL Integration</p> <p></p>"}]}